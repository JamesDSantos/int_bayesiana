[
  {
    "objectID": "regressao.html",
    "href": "regressao.html",
    "title": "12  O modelo de regressão linear normal",
    "section": "",
    "text": "12.1 Introdução\nDenominamos regressão o problema de determinar a média de \\(Y\\) dado o vetor \\(\\boldsymbol{x}'=(x_1,\\ldots,x_q)\\). Em particular se\n\\[E(Y|\\boldsymbol{x},\\boldsymbol{\\beta})=\\boldsymbol{x}'\\boldsymbol{\\beta}=\\sum_{j=1}^q x_j\\beta_j,\\] dizemos que a regressão é linear (em \\(\\boldsymbol{\\beta}\\)) e o objetivo passa a ser fazer inferências sobre \\(\\boldsymbol{\\beta}\\).\nNo problema de regressão, as seguintes nomenclaturas são usuais:\nOs termos input e output são usuais em aprendizagem de máquina, enquanto que variável independente e dependente são usuais na área da saúde (vale ressaltar que não há relação com a independência sob o ponto de vista da probabilidade, pois claramente \\(Y\\) e \\(\\boldsymbol{X}\\) são dependentes).\nSejam \\(Y_1,\\ldots,Y_n\\) uma amostra de variáveis aleatórias independentes. Sejam \\(\\boldsymbol{x}_1,\\ldots,\\boldsymbol{x}_n\\) vetores de dimensão \\(q\\). Então, o modelo de regressão linear normal é dado por\n\\[y|\\boldsymbol{x},\\boldsymbol{\\beta}\\sim\\hbox{Normal}(\\boldsymbol{x}'\\boldsymbol{\\beta},\\sigma^2).\\]\nComo \\(Y_1,\\ldots,Y_n\\) são independentes, temos que \\(\\boldsymbol{Y}'=(Y_1,\\ldots,Y_n)\\) tem distribuição normal multivariada com média\n\\[E(\\boldsymbol{Y}|\\boldsymbol{x}_1,\\ldots,\\boldsymbol{x}_n)=\\left(\\begin{array}{c}E(Y_1|\\boldsymbol{x}_1)\\\\ \\vdots \\\\ E(Y_n|\\boldsymbol{x}_n)\\end{array}\\right)=\\left(\\begin{array}{c}\\boldsymbol{x}_1'\\boldsymbol{\\beta}\\\\ \\vdots \\\\ \\boldsymbol{x}_n'\\boldsymbol{\\beta}\\end{array}\\right)=\\underbrace{\\left(\\begin{array}{c}\\boldsymbol{x}_1'\\\\ \\vdots \\\\ \\boldsymbol{x}_n'\\end{array}\\right)}_{\\boldsymbol{X}}\\boldsymbol{\\beta}=\\boldsymbol{X}\\boldsymbol{\\beta}\\]\ne como \\(Var(Y_i|\\boldsymbol{X})=Var(Y_i|\\boldsymbol{x}_i)=\\sigma^2\\) e, para \\(i\\neq j,\\) \\(Cov(Y_i,Y_j|\\boldsymbol{X})=0\\), teremos que \\[Var(\\boldsymbol{Y}|\\boldsymbol{X})=\\sigma^2\\text{I}_n\\] Portanto \\(\\boldsymbol{Y}|\\boldsymbol{X},\\boldsymbol{\\beta},\\sigma^2\\sim\\hbox{Normal}(\\boldsymbol{X}\\boldsymbol{\\beta},\\sigma^2\\text{I}_q)\\). Sua função de verossimilhança é\n\\[L(\\boldsymbol{\\beta},\\sigma^2)\\propto \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{\\frac{n}{2}}e^{-\\frac{1}{2\\sigma^2}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})'(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})}.\\] Note que\n\\[\\begin{align}\n(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})'(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})&= \\boldsymbol{y}'\\boldsymbol{y}+\\boldsymbol{\\beta}'(\\boldsymbol{X}'\\boldsymbol{X})\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'\\boldsymbol{X}'\\boldsymbol{y}\\\\&=\n\\boldsymbol{y}'\\boldsymbol{y}+\\boldsymbol{\\beta}'(\\boldsymbol{X}'\\boldsymbol{X})\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'(\\boldsymbol{X}'\\boldsymbol{X})\\underbrace{(\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{y}}_{\\hat{\\boldsymbol{\\beta}}}\n\\\\&=\n\\boldsymbol{y}'\\boldsymbol{y}+\\boldsymbol{\\beta}'(\\boldsymbol{X}'\\boldsymbol{X})\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}\\pm \\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}\\\\&=(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})+\\boldsymbol{y}'\\boldsymbol{y}-\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}\\end{align}\n\\] Observe que \\[\\begin{align}\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}&=((\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{y})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{y}\\\\\n&=((\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{y})'\\boldsymbol{X}'\\boldsymbol{y}\\\\&=\n\\boldsymbol{y}\\boldsymbol{X}(\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{y}\\end{align}\\] logo\n\\[\\begin{align}\n(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})'(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})&= (\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})+\\underbrace{\\boldsymbol{y}'(\\text{I}_n-\\boldsymbol{X}(\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}')\\boldsymbol{y}}_{SQR}\\end{align}\n\\] onde \\(SQR\\) é a sigla para soma de quadrados de resíduos. Esse termo tem esse nome porque \\[SQR=(\\boldsymbol{y}-\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{y}-\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}})\\] e \\[\\boldsymbol{r}=\\boldsymbol{y}-\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}\\] é denominado vetor de resíduos. Portanto, a função de verossimilhança pode ser reescrita como\n\\[L(\\boldsymbol{\\beta},\\sigma^2)\\propto \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{\\frac{n}{2}}e^{-\\frac{1}{2\\sigma^2}(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})}e^{-\\frac{1}{2\\sigma^2}SQR}.\\] ## Priori conjugada e suas limitações\nSeja \\(\\phi=1/\\sigma^2\\). Então, a priori normal-gama, também escrita como \\[\\begin{align}\\boldsymbol{\\beta}|\\phi,\\boldsymbol{X}&\\sim\\hbox{Normal}(\\boldsymbol{\\beta}_0,\\phi^{-1} C_0^{-1})\\\\ \\phi&\\sim\\hbox{Gama}\\left(\\frac{n_0}{2},\\frac{s_0}{2}\\right)\\end{align}\\] é conjugada para o modelo linear. A posteriori é dada por\n\\[\\begin{align}f(\\boldsymbol{\\beta},\\phi|\\text{dados})&\\propto \\phi^\\frac{n}{2}e^{-\\frac{\\phi}{2}(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})-\\frac{\\phi}{2}SQR}\\times\\\\&|\\phi\\boldsymbol{C}_0|^{1/2}e^{-\\frac{\\phi}{2}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)'\\boldsymbol{C}_0(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)]}\\phi^{\\frac{n_0}{2}-1}e^{-\\frac{s_0}{2}\\phi}\\\\&=\\phi^{\\frac{q}{2}}e^{-\\frac{\\phi}{2}[(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})+(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)'\\boldsymbol{C}_0(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)]}\\phi^{\\frac{n+n_0}{2}-1}e^{-\\frac{\\phi}{2}(SQR+s_0)}\\end{align}\\] Como \\[\\begin{align}\n&(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})+(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)'\\boldsymbol{C}_0(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)\\\\&=\\boldsymbol{\\beta}'[\\underbrace{(\\boldsymbol{X}'\\boldsymbol{X})+\\boldsymbol{C}_0}_{\\boldsymbol{C}_1}]\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'( (\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{C}_0\\boldsymbol{\\beta}_0)+\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{\\beta}_0'\\boldsymbol{C}_0\\boldsymbol{\\beta}_0\\\\\n&=\\boldsymbol{\\beta}'\\boldsymbol{C}_1\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'\\boldsymbol{C}_1[\\underbrace{\\boldsymbol{C}_1^{-1}( (\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{C}_0\\boldsymbol{\\beta}_0)}_{\\tilde{\\boldsymbol{\\beta}}}]+\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{\\beta}_0'\\boldsymbol{C}_0\\boldsymbol{\\beta}_0\\\\\n&=\\boldsymbol{\\beta}'\\boldsymbol{C}_1\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'\\boldsymbol{C}_1\\tilde{\\boldsymbol{\\beta}}\\pm\\tilde{\\boldsymbol{\\beta}}'\\boldsymbol{C}_1\\tilde{\\boldsymbol{\\beta}}+\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{\\beta}_0'\\boldsymbol{C}_0\\boldsymbol{\\beta}_0\\\\\n&=(\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{\\beta}})'\\boldsymbol{C}_1(\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{\\beta}})+\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{\\beta}_0'\\boldsymbol{C}_0\\boldsymbol{\\beta}_0-\\tilde{\\boldsymbol{\\beta}}'\\boldsymbol{C}_1\\tilde{\\boldsymbol{\\beta}}\n\\end{align}\\] Além disso, pode-se mostrar que \\[\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{\\beta}_0'\\boldsymbol{C}_0\\boldsymbol{\\beta}_0-\\tilde{\\boldsymbol{\\beta}}'\\boldsymbol{C}_1\\tilde{\\boldsymbol{\\beta}}=(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)'(\\boldsymbol{C}_0+(\\boldsymbol{X}'\\boldsymbol{X})^{-1})^{-1}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)\\] \\[\\begin{align}f(\\boldsymbol{\\beta},\\phi|\\text{dados})&\\propto\\phi^{\\frac{q}{2}}e^{-\\frac{\\phi}{2}(\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{\\beta}})'\\boldsymbol{C}_1(\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{\\beta}})}\\phi^{\\frac{n+n_0}{2}-1}e^{-\\frac{\\phi}{2}(SQR+s_0+(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)'(\\boldsymbol{C}_0+(\\boldsymbol{X}'\\boldsymbol{X})^{-1})^{-1}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0))}\\end{align}\\] ou seja \\[\\begin{align}\\boldsymbol{\\beta}|\\phi,\\hbox{dados}&\\sim\\hbox{Normal}(\\tilde{\\boldsymbol{\\beta}},\\phi^{-1}\\boldsymbol{C}_1^{-1})\\\\\n\\phi|\\text{dados}&\\sim\\hbox{Gama}\\left(\\frac{n_0+n}{2},\\frac{s_1}{2}\\right)\n\\end{align}\\] onde \\[s_1=SQR+s_0+(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)'(\\boldsymbol{C}_0+(\\boldsymbol{X}'\\boldsymbol{X})^{-1})^{-1}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)\\]\nO estimador de Bayes para \\(\\boldsymbol{\\beta}\\) é\n\\[\\tilde{\\boldsymbol{\\beta}}=\\boldsymbol{C}_1^{-1}[ (\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{C}_0\\boldsymbol{\\beta}_0]\\]\nPara elicitar corretamente a priori conjugada é necessário elicitar a matriz \\(\\boldsymbol{C}_0\\), que é um desafio. A solução mais simples é considerar que \\(\\boldsymbol{C_0}=\\lambda \\text{I}_q\\) e valores pequenos de \\(\\lambda\\) levam a uma informação a priori mais difusa. Essa solução é utilizada na inferência frequentista, conforme podemos ver abaixo.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>O modelo de regressão linear normal</span>"
    ]
  },
  {
    "objectID": "regressao.html#introdução",
    "href": "regressao.html#introdução",
    "title": "12  O modelo de regressão linear normal",
    "section": "",
    "text": "\\(Y\\): variável resposta, output, resultado, variável dependente\n\\(\\boldsymbol{x}\\): variáveis regressoras, input, variáveis independentes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA relação com a Regressão Ridge O estimador frequentista existe apenas quando \\(\\boldsymbol{X}'\\boldsymbol{X}\\) tem inversa. Essa inversa pode não existir quando \\(q&gt;n\\) ou quando as colunas de \\(\\boldsymbol{X}\\) são altamente correlacionada (esse fenômeno é denominado multicolinearidade). Uma solução é utilizar o estimador de regressão de Ridge, dado por\n\\[\\hat{\\boldsymbol{\\beta}}_{R}(\\lambda)=[\\lambda\\text{I}_q+ \\boldsymbol{X}'\\boldsymbol{X}]^{-1}\\boldsymbol{X}'\\boldsymbol{Y},\\] onde \\(\\lambda\\) é denominado shrinkage (algo como parâmetro de contração, embora também possa ser denominado parâmetro de regularização). A escolha de \\(\\lambda\\) é um problema em aberto, pois não há garantias de resultados ótimos.\nSob o ponto de vista bayesiano, fazendo \\(\\boldsymbol{\\beta}_0=\\text{0}_q\\) e \\(\\boldsymbol{C}_0=\\lambda\\text{I}_q\\), temos que\n\\[\\begin{align}\\tilde{\\boldsymbol{\\beta}}&=(\\lambda\\text{I}_q +\\boldsymbol{X}'\\boldsymbol{X})^{-1} (\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}\\\\&=\n(\\lambda\\text{I}_q +\\boldsymbol{X}'\\boldsymbol{X})^{-1} (\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{Y}\\\\&=\n[\\lambda\\text{I}_q+ \\boldsymbol{X}'\\boldsymbol{X}]^{-1}\\boldsymbol{X}'\\boldsymbol{Y}=\\hat{\\beta}(\\lambda),\\end{align}\\] ou seja, o estimador de Bayes é o estimador da regressão Ridge. Neste caso, o aumento de \\(\\lambda\\) implica no aumento da crença a priori de que \\(\\boldsymbol{\\beta}\\) é zero.\n\n\nExemplo. mtcars\nO banco de dados mtcars, disponível no R, apresenta estatísticas de design e performance de 32 automóveis. Considere as seguintes variáveis:\n\nmpg: milhas/galão\ncyl: número de cilindros\ndisp: deslocamento (polegadas cúbicas)\nhp: potência bruta\nwt: peso (em libras)\n\nConsiderando mpg como variável resposta, vamos analisar sua relação com as demais.\n\nbanco  &lt;- mtcars[,c(\"mpg\",\"cyl\", \"disp\",\"hp\",\"wt\")]\npairs(banco)\n\n\n\n\n\n\n\n\nAlgumas relações, como disp - deslocamento - e hp - potência bruta, não parecem lineares. Podemos linearizar a relação aplicando a transformação logarítmica\n\nbanco2  &lt;- banco\nbanco2$disp &lt;- log ( banco$disp )\nbanco2$hp   &lt;- log ( banco$hp )\npairs(banco2)\n\n\n\n\n\n\n\n\nVamos ajustar um modelo de regressão linear com \\(\\boldsymbol{\\beta}_0=\\text{0}_5\\), \\(n_0=0,01,s_0=0,01\\), \\(\\boldsymbol{C}_0=\\lambda\\text{I}_5\\) para diferentes valores de \\(\\lambda\\). Abaixo, apresentamos as estimativas de Bayes para \\(\\boldsymbol{\\beta}\\) com diferentes valores de \\(\\lambda\\). Para verificar como as estimativas são influenciadas por \\(\\lambda\\), vamos representar as estimativas de máxima verossimilhança nas linhas tracejadas. Observe que as estimativas falham em se aproximar das estimativas de máxima verossmilhança mesmo para \\(\\lambda\\) pequeno\n\nX &lt;- as.matrix(cbind(1,banco2[,-1]))\ny =  banco2[,1]\n\nbeta_til &lt;- function(lambda) solve( diag(lambda,5) + t(X)%*%X )%*%t(X)%*%y\nbeta_emv &lt;- beta_til(0)\n\nm = 20\nlambda = seq(.001,m,.001)\nresp = NULL\nfor(l in lambda){\n  resp &lt;- rbind(resp, beta_til(l)[,1])\n}\n\nplot.new()\nplot.window(ylim=c(-5,10), xlim=c(0,m))\nlines(lambda, resp[,2],col=1,lwd=2)\nabline(h=beta_emv[2,1],col=1, lty = 2,lwd=2)\nlines(lambda, resp[,3],col=2,lwd=2)\nabline(h=beta_emv[3,1],col=2, lty = 2,lwd=2)\nlines(lambda, resp[,4],col=3,lwd=2)\nabline(h=beta_emv[4,1],col=3, lty = 2,lwd=2)\nlines(lambda, resp[,5],col=4,lwd=2)\nabline(h=beta_emv[5,1],col=4, lty = 2,lwd=2)\npoints(cbind(lambda[1],resp[1,]), pch = 16, col=1:4, cex= 1.2)\naxis(1)\naxis(2)\nlegend('topright',names(banco2)[-1],col=1:4,bty='n',lty=1)\ntitle(xlab=expression(lambda), ylab='Estimativas dos coeficientes de regressão')",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>O modelo de regressão linear normal</span>"
    ]
  },
  {
    "objectID": "regressao.html#a-priori-g-de-zellner",
    "href": "regressao.html#a-priori-g-de-zellner",
    "title": "12  O modelo de regressão linear normal",
    "section": "12.2 A priori \\(G\\) de Zellner",
    "text": "12.2 A priori \\(G\\) de Zellner\nA maior dificuldade na priori conjugada está na dificuldade em introduzir a estrutura de correlação a priori para \\(\\boldsymbol{\\beta}\\). A priori \\(G\\) de Zellner resolve essa questão. Para tanto, observe que \\[\\hat{\\boldsymbol{\\beta}}\\sim\\hbox{Normal}(\\boldsymbol{\\beta},\\phi^{-1}(\\boldsymbol{X}'\\boldsymbol{X})^{-1})\\] A ideia central é utilizar a matriz \\((\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\) para construção da estrutura de covariâncias a priori para \\(\\boldsymbol{\\beta}\\).\nA priori de \\(G\\) de Zellner é dada por \\[\\begin{align}\\boldsymbol{\\beta}|\\phi&\\sim\\hbox{Normal}(\\boldsymbol{\\beta}_0,\\phi^{-1}\\lambda^{-1}(\\boldsymbol{X}'\\boldsymbol{X})^{-1})\\\\f(\\phi)&\\propto \\frac{1}{\\phi}\\end{align}\\] Deste modo ,a posteriori é dada por\n\\[\\begin{align}f(\\boldsymbol{\\beta},\\phi|\\text{dados})&\\propto \\phi^\\frac{n}{2}e^{-\\frac{\\phi}{2}(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})-\\frac{\\phi}{2}SQR}\\times\\\\&|\\phi\\boldsymbol{X}'\\boldsymbol{X}|^{1/2}e^{-\\frac{\\phi}{2}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)'\\lambda(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)]}\\frac{1}{\\phi}\\\\&=\\phi^{\\frac{q}{2}}e^{-\\frac{\\phi}{2}[(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})+\\lambda(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)'\\boldsymbol{X}'\\boldsymbol{X}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)]}\\phi^{\\frac{n}{2}-1}e^{\\frac{\\phi}{2}SQR}\\end{align}\\] Pode mostrar que o termo na primeira exponencial acima é dado por \\[(\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{\\beta}})'(1+\\lambda)\\boldsymbol{X}'\\boldsymbol{X}(\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{\\beta}})+(\\boldsymbol{\\beta}_0-\\tilde{\\boldsymbol{\\beta}})'\\frac{\\lambda}{1+\\lambda}\\boldsymbol{X}'\\boldsymbol{X}(\\boldsymbol{\\beta}_0-\\tilde{\\boldsymbol{\\beta}})\\] onde \\[\\tilde{\\boldsymbol{\\beta}}=\\frac{\\lambda}{1+\\lambda}\\boldsymbol{\\beta}_0+\\frac{1}{1+\\lambda}\\hat{\\boldsymbol{\\beta}}.\\] Então, \\[\\begin{align}\n\\boldsymbol{\\beta}|\\phi,\\hbox{dados}&\\sim\\hbox{Normal}(\\tilde{\\boldsymbol{\\beta}}, \\phi^{-1}(1+\\lambda)^{-1}(\\boldsymbol{X}'\\boldsymbol{X})^{-1})\\\\\n\\phi|\\text{dados}&\\sim\\hbox{Gama}\\left(\\frac{n}{2},\\frac{s_1}{2}\\right),\n\\end{align}\\] onde \\[s_1=SQR+\\frac{\\lambda}{1+\\lambda}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)'\\boldsymbol{X}'\\boldsymbol{X}^{-1}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)\\]\n\nExemplo mtcars\nVamos refazer o problema anterior considerando ainda \\(\\boldsymbol{\\beta}=\\text{0}_5\\) para diferentes valores de \\(\\lambda\\). Nesse caso\n\\[\\tilde{\\boldsymbol{\\beta}}=\\frac{1}{1+\\lambda}\\hat{\\boldsymbol{\\beta}}.\\]\n\nX &lt;- as.matrix(cbind(1,banco2[,-1]))\ny =  banco2[,1]\n\nSa = solve(t(X)%*%X)\nbeta_til &lt;- function(lambda) (Sa%*%t(X)%*%y)/(1+lambda)\n\nm=5\nlambda = seq(.001,4,.01)\nresp = NULL\nfor(l in lambda){\n  resp &lt;- rbind(resp, beta_til(l)[,1])\n}\n\nplot.new()\nplot.window(ylim=c(-5,2.5), xlim=c(0,m))\nlines(lambda, resp[,2],col=1, lwd = 2)\nabline(h = beta_emv[2,1], col = 1, lwd = 2,lty=2)\nlines(lambda, resp[,3],col=2, lwd = 2)\nabline(h = beta_emv[3,1], col = 2, lwd = 2,lty=2)\nlines(lambda, resp[,4],col=3, lwd= 2)\nabline(h = beta_emv[4,1], col = 3, lwd =2,lty=2)\nlines(lambda, resp[,5],col=4, lwd= 2)\nabline(h = beta_emv[5,1], col = 4, lwd = 2,lty=2)\naxis(1)\naxis(2)\nlegend('topright',names(banco2)[-1],col=1:4,bty='n',lty=1, bg = 'white')\n\n\n\n\n\n\n\n\nPodemos perceber que a priori \\(G\\) de Zellner é menos sensível que a priori conjugada. Selecionando \\(\\lambda=0,01\\), vamos simular amostras da posteriori e analisar se 0 é um valor plausível para cada \\(\\beta\\). Abaixo apresentamos as densidades a posteriori estimadas e os respectivos intervalos de credibilidade 95%.\n\nrequire(mvtnorm)\n\nCarregando pacotes exigidos: mvtnorm\n\n# simulando phi\nX= as.matrix(cbind(1,banco2[,-1]))\ny = banco2[,1]\nSX = solve(t(X)%*%X)\nlambda = .01\nbeta_emv &lt;- SX%*%t(X)%*%y\nSQR = t((y-X%*%beta_emv))%*%(y-X%*%beta_emv)\ns1 = SQR+(t(beta_emv)%*%t(X)%*%X%*%beta_emv)*lambda/(1+lambda)\nn = length(y)\nphi = rgamma(500,n/2, .5*drop(s1))\n\nmu = beta_emv/(1+lambda)\nbeta = array(NA_real_, c(500,5))\nfor(i in 1:500)beta[i,] = rmvnorm(1, mu,SX/(phi[i]*1.01) )\n\noo = par(mfrow=c(2,2))\nplot(density(beta[,2]), main =names(banco2)[2],lwd=2)\nqq = quantile(beta[,2], c(.025,.975))\nsegments(qq[1],0,qq[2],0, lwd = 3)\nplot(density(beta[,3]), main =names(banco2)[3],lwd=2)\nqq = quantile(beta[,3], c(.025,.975))\nsegments(qq[1],0,qq[2],0, lwd = 3)\nplot(density(beta[,4]), main =names(banco2)[4],lwd=2)\nqq = quantile(beta[,4], c(.025,.975))\nsegments(qq[1],0,qq[2],0, lwd = 3)\nplot(density(beta[,5]), main =names(banco2)[5],lwd=2)\nqq = quantile(beta[,5], c(.025,.975))\nsegments(qq[1],0,qq[2],0, lwd = 3)\n\n\n\n\n\n\n\npar(oo)\n\nObserve que zero está na região de alta densidade para todos os parâmetros, o que contradiz nossa análise exploratória. É possível que o modelo esteja mal especificado, especialmente por causa do número de variáveis regressoras.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>O modelo de regressão linear normal</span>"
    ]
  },
  {
    "objectID": "regressao.html#seleção-de-regressoras",
    "href": "regressao.html#seleção-de-regressoras",
    "title": "12  O modelo de regressão linear normal",
    "section": "12.3 Seleção de regressoras",
    "text": "12.3 Seleção de regressoras\nNo exemplo da última sessão, vimos que a distribuição a posteriori do coeficiente da variável cyl possui muita massa em torno de zero, o que deve implicar que esta variável não é relevante para o problema.\nPodemos utilizar o critério de informação do desvio (DIC) para escolher entre um subgrupo de regressoras. Recordemos que os critério de informação são descritos como\n\\[-2\\log L(\\hat{\\boldsymbol{\\beta}},\\hat{\\phi})+2k,\\] onde \\(L()\\) é a função de verossimilhança, \\(\\hat{\\boldsymbol{\\beta}}\\) são estimativas para os coeficientes de regressão, \\(\\hat{\\phi}\\) é uma estimativa para \\(\\phi\\) e \\(k\\) é uma penalidade relacionada ao número de parâmetros. No DIC, temos\\(\\hat{\\boldsymbol{\\beta}}=E(\\boldsymbol{\\beta}|\\boldsymbol{y})\\), \\(\\hat{\\phi}=E(\\phi|\\boldsymbol{y})\\) e \\[k=\\frac{1}{2}Var_{\\boldsymbol{\\beta},\\phi|\\boldsymbol{y}}(\\log L(\\boldsymbol{\\beta},\\phi)).\\]\n\nAlgoritmo: Calculando o DIC via Monte Carlo\n\nSimule \\((\\boldsymbol{\\beta},\\phi)_1,\\ldots,(\\boldsymbol{\\beta},\\phi)_B\\) da distribuição a posteriori de \\((\\boldsymbol{\\beta},\\phi)\\).\nEstime \\(E(\\boldsymbol{\\beta}|\\boldsymbol{x})\\) por \\[\\hat{\\boldsymbol{\\beta}}=\\frac{1}{B}\\sum_{j=1}^B \\boldsymbol{\\beta}j\\] e \\[\\hat{\\phi}=\\frac{1}{B}\\sum_{j=1}^{^B}\\phi_j.\\]\nFaça \\(v_j=-2\\log L(\\boldsymbol{\\beta}_j,\\phi_j)\\). Estime \\(k\\) por \\[k=\\frac{1}{2B}\\sum_{j=1}^B(v_j-\\bar{v})^2\\]\nCalcule o DIC:\n\n\\[DIC=-2\\log L(\\hat{\\boldsymbol{\\beta}},\\hat{\\phi})+2k.\\]\n\n\nmtcars (cont). Vamos calcular o DIC do modelo encontrado no exemplor anterior. Primeiro, vamos implementar a função \\(\\log L\\):\n\n# criando a função log-verossimilhança\nloglik &lt;- function(theta,y,X){\n  m = length(theta)\n  beta= matrix(theta[1:(m-1)], ncol = 1)\n  phi = theta[m]\n  mu = X%*%beta\n  sum( dnorm(y,mu,1/sqrt(phi) , log = T))\n}\n\nVamos calcular o DIC para o modelo com as regressoras cyl, disp, hp, wt.\n\nrequire(mvtnorm)\n# matriz de regressoras (com o intercepto)\nX = as.matrix(cbind(1,banco2[,-1]))\ny = banco2[,1]\nlambda = .01\nn = length(y)\nB = 5000\n# simulando phi\nSa = solve(t(X)%*%X)\nbeta_emv &lt;- Sa%*%t(X)%*%y\nSQR = t((y-X%*%beta_emv))%*%(y-X%*%beta_emv)\ns1 = SQR + (t(beta_emv) %*% t(X) %*%X%*% beta_emv )*lambda/(1+lambda)\nphi = rgamma(B,n/2, .5*drop(s1))\n\n# simulando mu\nmu = beta_emv/(1+lambda)\nbeta = array(NA_real_, c(B,ncol(X)))\nfor(i in 1:B)beta[i,] = rmvnorm(1, mu,Sa/(phi[i]*(1+lambda)) )\n\n# encontrando as estimativas a posteriori\nbeta_hat = colMeans(beta)\nphi_hat  = mean(phi)\n\n# encontrando vi e estimando k\nvi = apply( cbind(beta,phi), 1, function(theta) -2*loglik(theta,y,X))\nk  = var(vi)/2\nk\n\n[1] 13.03661\n\n# calculando o DIC\n-2*loglik(c(beta_hat,phi_hat), y, X) +2*k\n\n[1] 171.5675\n\n\nEm seguida, vamos calcular o DIC removendo uma variável de cada vez, explorando se haveria melhoria em remover variáveis. Obtivemos a tabela abaixo, o que nos levou a decisão de aceitar o modelo hp e wt\n\n\n        Regressoras DIC\n1 disp, cyl, hp, wt 171\n2      disp, hp, wt 170\n3       cyl, hp, wt 171\n4       cyl, hp, wt 172\n5     cyl, disp, hp 172\n6            hp, wt 169\n7          dips, wt 171\n8          disp, hp 171\n9                wt 175\n\n\nVamos verificar o modelo com as variáveis hp e wt. Abaixo apresentamos as densidades a posteriori estimadas e os respectivos intervalos de credibilidade 95%. Temos evidências claras do efeito negativo.\n\nrequire(mvtnorm)\n# simulando phi\nX= as.matrix(cbind(1,banco2[,-1][,3:4]))\ny = banco2[,1]\nSX = solve(t(X)%*%X)\nlambda = .01\nbeta_emv &lt;- SX%*%t(X)%*%y\nSQR = t((y-X%*%beta_emv))%*%(y-X%*%beta_emv)\ns1 = SQR+(t(beta_emv)%*%t(X)%*%X%*%beta_emv)*lambda/(1+lambda)\nn = length(y)\nphi = rgamma(500,n/2, .5*drop(s1))\n\nmu = beta_emv/(1+lambda)\nbeta = array(NA_real_, c(500,ncol(X)))\nfor(i in 1:500)beta[i,] = rmvnorm(1, mu,SX/(phi[i]*1.01) )\n\noo = par(mfrow=c(1,2))\nplot(density(beta[,2]), main =names(banco2)[2],lwd=2)\nqq = quantile(beta[,2], c(.025,.975))\nsegments(qq[1],0,qq[2],0, lwd = 3)\nplot(density(beta[,3]), main =names(banco2)[3],lwd=2)\nqq = quantile(beta[,3], c(.025,.975))\nsegments(qq[1],0,qq[2],0, lwd = 3)\n\n\n\n\n\n\n\npar(oo)\n\nPara verificar a adequação deste modelo, podemos utilizar a comparação usual entre a função de distribuição empírica e a preditiva a posteriori.\n\nn &lt;- length(y)\ny_pred &lt;- array(NA_real_, c(500, n))\nfor(i in 1:500){\n  y_pred[i,] &lt;- rnorm(n, X%*%beta[i,], 1/sqrt(phi[i]))\n}\n\nFd_sim = apply(y_pred,1, function(x){\n  Fd = ecdf(x)\n  Fd(sort(y))\n})\n\nqq = apply(Fd_sim,1, function(x) quantile(x,c(.025,.977)))\n\nplot(ecdf(y), main = '')\nlines(sort(y),qq[1,], col =2)\nlines(sort(y),qq[2,], col =2)\n\n\n\n\n\n\n\n\nObserve que o ajuste não é adequado, dando evidências de que modelo escolhido não se ajsuta bem para este conjunto de dados",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>O modelo de regressão linear normal</span>"
    ]
  },
  {
    "objectID": "regressao.html#exercícios",
    "href": "regressao.html#exercícios",
    "title": "12  O modelo de regressão linear normal",
    "section": "12.4 Exercícios",
    "text": "12.4 Exercícios\n\n12.4.1 1.\nOs dados deste exemplos se referem ao número de espécies de tartarugas nas várias Ilhas Galápagos, sob o nome gala, disponíveis na biblioteca faraway. O conjunto de dados contém 30 casos (ilhas) e sete variáveis.\nAs variáveis são Species\n\nSpecies - o número de espécies de tartarugas encontradas na ilha,\nEndemics — o número de espécies endêmicas\nArea — a área da ilha (km²)\nElevation — a maior elevação da ilha (m)\nNearest — a distância da ilha mais próxima (km)\nScruz — a distância da Ilha Santa Cruz (km)\nAdjacent — a área da ilha adjacente (km²).\n\nVerifique se é possível ajustar um modelo linear para a varíavel Species utilizando as demais como regressoras.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>O modelo de regressão linear normal</span>"
    ]
  }
]