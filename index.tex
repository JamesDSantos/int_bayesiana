% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[chapter]
\newtheorem{refsolution}{Solution}[chapter]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Introdução à Inferência Bayesiana},
  pdfauthor={James D Santos},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Introdução à Inferência Bayesiana}
\author{James D Santos}
\date{2023-10-03}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

Este material foi criado para a disciplina Introdução à Inferência
Bayesiana, do curso de bacharelado em Estatística, da Universidade
Federal do Amazonas.

\bookmarksetup{startatroot}

\chapter{}\label{section}

\bookmarksetup{startatroot}

\chapter{Introdução}\label{introduuxe7uxe3o}

Os objetivos desta aula são:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apresentar a notação
\item
  Explicar sobre as fontes de informação
\item
  Apresentar as inferências básicas
\item
  Discutir como se dá o processo de elicitação de prioris
\end{enumerate}

\section{Dados que serão utilizados nesse
capítulo}\label{dados-que-seruxe3o-utilizados-nesse-capuxedtulo}

A amostra abaixo se refere ao número mensal de suicídios registrados no
Amazonas nos anos 2021, 2022 e 2023.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{no\_suicidios }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{23}\NormalTok{,}\DecValTok{23}\NormalTok{, }\DecValTok{21}\NormalTok{,}
\DecValTok{22}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{31}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{23}\NormalTok{,}
\DecValTok{36}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{34}\NormalTok{,}
\DecValTok{22}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{22}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Desta amostra, inferimos que a média mensal é de 25,9 registros e que a
variância é 21,05.

\section{Notação}\label{notauxe7uxe3o}

Variáveis aleatórias cujos valores podem ser observados serão denotadas
por letras maiúsculas. Exemplos:

\begin{itemize}
\item
  \(X\) é o número de acidentes diários na Avenida Torquato Tapajós
\item
  \(Y\) é o nível máximo diário do Rio Negro
\end{itemize}

Valores observados de variáveis aleatórias serão denotados pela
respectiva letra minúscula.

Parâmetros serão considerados aleatórios, mas serão representados por
letras gregas minúsculas, como \(\theta\), \(\lambda\), etc.

Vetores aleatórios serão representados por letras em negrito. Exemplos:

\begin{itemize}
\item
  \(\mathbf{X} = \{X_1 , \ldots , X_n \}\) é um vetor de variáveis
  aleatórias.
\item
  \(\mathbf{x} = \{x_1 ,\ldots , x_n \}\) é um vetor observado de
  variáveis aleatórias.
\item
  \(\theta=\{\alpha,\beta\}\) é um vetor de parâmetros.
\end{itemize}

\begin{definition}[]\protect\hypertarget{def-Suporte}{}\label{def-Suporte}

O suporte de uma variável aleatória é o conjunto de todos os seus
possíveis valores. Quando necessário, o suporte de variáveis aleatórias
é representado pela versão caligráfica de sua letra correspondente.

Exemplos: o suporte de \(X\) é \(\mathcal{X}\) ; o suporte de Y é
\(\mathcal{Y}\) ; o suporte de \(Z\) é \(\mathcal{Z}\).

\end{definition}

\begin{definition}[]\protect\hypertarget{def-Espaco}{}\label{def-Espaco}

O espaço paramétrico é o conjunto de todos os possíveis valores do
parâmetro. Ele é representado pela versão maiúscula da letra grega
utilizada para seu respectivo parâmetro.

Exemplo: o espaço paramétrico do parâmetro \(\theta\) é representado por
\(\Theta\).

\end{definition}

Tanto a função de densidade quanto a de probabilidade serão denotadas
por funções começando com letras minúsculas. Por exemplo,

\[f(x|\lambda)=\lambda e^{-\lambda x}\] onde \(x,\lambda>0\) é a
densidade da distribuição exponencial, enquanto que

\[p(x|\lambda)=\frac{e^{-\lambda}\lambda^x}{x!}\] com \(x\in\mathbb{N}\)
e \(\lambda >0\) é a função de probabilidade da distribuição Poisson.

\section{Fontes de informação}\label{fontes-de-informauxe7uxe3o}

\subsection{A função de
verossimilhança}\label{a-funuxe7uxe3o-de-verossimilhanuxe7a}

Seja \(\mathbf{x} = \{x_1 , \ldots , x_n \}\) uma amostra observada.
Supomos que \(\mathbf{x}\) é uma das possíveis amostras das variáveis
aleatórias \(\mathbf{X} = \{X_1 , \ldots , X_n \}\). Supomos ainda que
\(X\sim F (.|\theta)\). Assim, condicionada ao conhecimento de
\(\theta\), a distribuição da amostra está completamente especificada.

::: \{\#def-Funcao de verossimilhanca\} Para \(\mathbf{x}\) fixado, a
função \[L:\Theta\Rightarrow [0,\infty)\] é denominada verossimilhança.
:::

Sua interpretação é a seguinte: para \(\theta_1,\theta_2\in\Theta\), se

\[L(\theta_1)>L(\theta_2),\] dizemos que \(\theta_1\) é mais verossímil
que \(\theta_2\). Isto porque a probabilidade de observar uma amostra na
vizinhança de \(\mathbf{x}\) é maior se considerarmos que \(\theta_1\) é
o valor do parâmetro. A verossimilhança é uma das fontes de informação
utilizada na inferência bayesiana (e a única fonte da inferência
frequentista).

\begin{example}[]\protect\hypertarget{exm-}{}\label{exm-}

Seja \(X_i\) o número de suicídios no \(i\)-ésimo mês da amostra e
suponha que \(X_1,\ldots,X_{36}\) é uma amostra aleatória proveniente do
modelo Poisson(\(\theta\)). Como \(\sum_i x_i=933\), a função de
verossimilhança será
\[L(\theta)=\prod_{i=1}^{36}\frac{e^{-\theta}\theta^{x_i}}{x_i!}\propto e^{-36\theta}\theta^{933}.\]

A próxima figura mostra os valores da função de verossimilhança para
diversos valores de \(\theta\) para a amostra observada.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# função de verossimilhança}
\NormalTok{vero }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(q)\{}
  \FunctionTok{sapply}\NormalTok{ ( q, }\ControlFlowTok{function}\NormalTok{(q) }\FunctionTok{prod}\NormalTok{(}\FunctionTok{dpois}\NormalTok{(no\_suicidios, q)) ) }
\NormalTok{\} }

\CommentTok{\# gráfico da função de verossimilhança}
\NormalTok{oo }\OtherTok{\textless{}{-}} \FunctionTok{par}\NormalTok{( }\AttributeTok{cex =} \FloatTok{1.2}\NormalTok{)}
\FunctionTok{curve}\NormalTok{( }\FunctionTok{vero}\NormalTok{(x),}\DecValTok{22}\NormalTok{,}\DecValTok{30}\NormalTok{, }\AttributeTok{xlab =} \FunctionTok{expression}\NormalTok{(theta), }\AttributeTok{ylab =} \FunctionTok{expression}\NormalTok{( }\FunctionTok{L}\NormalTok{(theta)) , }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{intro_files/figure-pdf/unnamed-chunk-2-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(oo)}
\end{Highlighting}
\end{Shaded}

Podemos notar que o valores mais verossímeis para \(\theta\) estão entre
24 e 28. Podemos ainda procurar o valor mais verossímil, denominado
\textbf{estimativa de máxima verossimilhança (emv).} Pode-se mostrar,
utilizando cálculo diferencial, que este valor é equivalente à média
amostral. Contudo, com o objetivo de utilizar ao máximo o poder
computacional que temos disponível, vamos encontrar esse valor
utilizando a função \texttt{optimize}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# menos o logaritmo da função de verossimilhança}
\NormalTok{lvero }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(q) }\SpecialCharTok{{-}}\FunctionTok{log}\NormalTok{( }\FunctionTok{vero}\NormalTok{(q))}

\CommentTok{\# encontrando a emv:}
\FunctionTok{optimise}\NormalTok{(lvero, }\FunctionTok{c}\NormalTok{(}\DecValTok{24}\NormalTok{,}\DecValTok{28}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$minimum
[1] 25.91666

$objective
[1] 105.4219
\end{verbatim}

O valor 25,9 é a estimativa de verossimilhança. Sob o ponto de vista
frequentista, esta seria a nossa estimativa para o valor de \(\theta\).

\end{example}

\subsection{A distribuição a
priori}\label{a-distribuiuxe7uxe3o-a-priori}

Sob o ponto de vista bayesiano, a informação existente sobre \(\theta\)
antes da observação da amostra deve ser levada em consideração. Isto é
feito traduzindo tal informação em termos de probabilidades.

\begin{definition}[]\protect\hypertarget{def-}{}\label{def-}

A distribuição de \(\theta\) é denominada \textbf{distribuição a
priori}.

\end{definition}

Os parâmetros da distribuição \textbf{a priori} são denominados
hiperparâmetros.

As distribuições a priori agregam o conhecimento sobre parâmetro antes
da observação da amostra (tal conhecimento pode pode vir da expertize
dos envolvidos ou ter sido gerado de uma amostra prévia).

As prioris podem ser muito ou pouco informativas, dependendo do grau de
crença sobre os valores particulares do espaço paramétrico. Em geral
isto é feito alterando a variância da distribuição:

\[\hbox{variância}=\frac{1}{\hbox{precisão}}\]

\begin{example}[]\protect\hypertarget{exm-}{}\label{exm-}

Nosso objetivo é encontrar uma distribuição a priori para \(\theta\),
que representa o número médio de suicídios mensais no Amazonas.
Primeiro, vamos obter algumas informações:

\begin{itemize}
\item
  Em 2024 foi noticiado que, no Brasil, ocorrem em média 38 suicídios
  por dia, algo em torno de 1.140 suicídios em um mês.
\item
  Para 2024, a população brasileira estava estimada em 212.600.000,
  enquanto que a população do Amazonas estava estimada em 4.281.209.
  Portanto, o Amazonas representa, aproximadamente 2\% da população
  brasileira.
\item
  Deste modo, pode-se inferir (a priori) que, em média, ocorrem 22,8
  suicídios mensais no Amazonas.
\end{itemize}

Podemos então procurar alguma distribuição a priori que reflita essa
informação. Por mera conveniência, vamos escolher
\(\theta\sim\hbox{Gama}(a,b)\), onde \(E(\theta)=\frac{a}{b}=22,8.\)

Um especialista em saúde pública poderia argumentar melhor se há motivos
para acreditar que essa média deveria ser maior ou não. Como não temos
essa informação disponível, podemos refletir esse fato aumentando a
variabilidade do modelo. O desvio padrão desta priori é

\[\sqrt{Var(\theta)}=\frac{\sqrt{a}}{b}=\frac{E(\theta)}{\sqrt{a}}=\frac{22,8}{\sqrt{a}}.\]

Vamos escolher esse desvio igual 5. Isto implica que
\[a=\left(\frac{22,8}{5}\right)^2=20,8\] e \[b=\frac{22,8}{20,8}=1,1.\]
Então, nossa informação a priori está traduzida no modelo
Gama(20.8,1.1). Abaixo, apresentamos a função densidade desse modelo.
Observe que esse modelo traz informações vagas sobre \(\theta\),
permitindo que ele assuma valores entre 10 e 30

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{curve}\NormalTok{(}\FunctionTok{dgamma}\NormalTok{(x,}\FloatTok{20.8}\NormalTok{, }\FloatTok{1.1}\NormalTok{), }\DecValTok{10}\NormalTok{,}\DecValTok{35}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{intro_files/figure-pdf/unnamed-chunk-4-1.pdf}

\end{example}

\subsection{Reunindo as fontes de informação - distribuição a
posteriori}\label{reunindo-as-fontes-de-informauxe7uxe3o---distribuiuxe7uxe3o-a-posteriori}

Sejam \(f(\boldsymbol{\theta})\) a densidade/função \textit{a priori}
para \(\boldsymbol{\theta}\) e \(L(\boldsymbol{\theta})\) a função de
verossimilhança.

Como \(\boldsymbol{\theta}\) é considerado aleatório, podemos analisar
sua distribuição \textbf{após} observar a amostra \(\boldsymbol{x}\), ou
seja \[\boldsymbol{\theta}|\boldsymbol{x}.\]

Esta distribuição é denominada \textit{posteriori}

::: \{\#thm-Teorema de Bayes\}

Seja \(\boldsymbol{x}\) uma amostra observada. Considere a priori
\(\theta\sim f(\theta)\) e a verossimilhança \(L(\theta)\). Então a
função de densidade (ou probabilidade) \textit{a posteriori} de
\(\theta|\boldsymbol{x}\) é dada por
\[f(\theta|\boldsymbol{x})=\frac{L(\theta)f(\theta)}{f(\boldsymbol{x})}.\]
O denominador é denominado distribuição preditiva, sendo igual a
\[f(\boldsymbol{x})=\sum_{\theta\in \Theta}L(\theta)f(\theta),\] se
\(\theta\) é v.a. discreta ou
\[f(\boldsymbol{x})=\int_{\Theta}L(\theta)f(\theta)d\theta\] se
\(\theta\) é v.a. contínua. :::

\begin{example}[]\protect\hypertarget{exm-}{}\label{exm-}

Considerando os dados de suicídios do começo desse capítulo, temos as
seguintes fontes de informação:

\begin{itemize}
\item
  Verossimilhança: \[L(\theta)\propto e^{-36\theta}\theta^{933}\]
\item
  Priori \[f(\theta)\propto \theta^{19,8}e^{-1,1\theta}\]
\end{itemize}

A distribuição a posteriori será

\[f(\theta|x_1,\ldots,x_{36})\propto \theta^{933+19,8}e^{-(36+1,1)\theta}=\theta^{952,8}e^{-37,1\theta}.\]
Assim, verificamos que
\(\theta|x_1,\ldots,x_{36}\sim\hbox{Gama}(953.8,37.1)\). Na figura
abaixo, apresentamos no mesmo gráfico, a função de verossimilhança,
priori e posteriori (fazendo as mudanças de escala necessárias). Observe
que a priori é mais dispersa que a verossimilhança. Essa, por sua vez,
restringe os valores de \(\theta\) que eram prováveis a priori. O
resutaldo é uma distribuição a posteriori próxima da função de
verossimilhança.

\includegraphics{intro_files/figure-pdf/unnamed-chunk-5-1.pdf}

\end{example}

\section{Inferência estatística}\label{inferuxeancia-estatuxedstica}

Denominamos por estatística qualquer função da amostra. Utilizamos
estatísticas para fazer as seguintes inferências:

\begin{itemize}
\item
  \emph{Estimação pontual}: trata-se de uma estatística com o objetivo
  de inferir o valor de \(\theta\). Tal estatística é denominada
  \textbf{estimador}.
\item
  \emph{Estimação por região}: trata-se de uma estatística, digamos
  \(T(\boldsymbol{X})\), com o objetivo de cobrir o valor de \(\theta\),
  ou seja, fazer a inferência \(\theta\in T(\boldsymbol{X})\). As
  estimações intervalares são as mais comuns, nas quais
  \(T(\boldsymbol{X})=(L(\boldsymbol{X}),U(\boldsymbol{X}))\).
\item
  \emph{Testes de hipóteses}: são estatísticas construídas para decidir
  se aceitamos a afirmação (hipótese) \(H:\theta\in \Theta_0\), onde
  \(\Theta_0\) é um subconjunto de \(\Theta\) conhecido por hipótese.
\end{itemize}

Note que a distribuição \textbf{a posteriori} é função da amostra.
Assim, toda função desta distribuição é uma estatística. Assim:

\begin{itemize}
\item
  \emph{Estimação pontual}: em geral é uma medida que representa a
  região de alta densidade (ou probabilidade) da \textbf{posteriori}. A
  média da posteriori, assim como a mediana ou a moda são escolhas
  comuns.
\item
  \emph{Estimação por regiões}: em geral procuramos por uma região \(T\)
  da posteriori que satisfaça
  \(P(\theta\in T(\boldsymbol{x})|\boldsymbol{x})=\gamma\), onde
  \(\gamma\) é denominado nível de credibilidade (não confundir com
  nível de confiança)
\item
  \emph{Testes de hipóteses}: em geral, aceitamos
  \(H:\theta\in\Theta_0\) se \(P(H|\boldsymbol{x})\) é elevada.
\end{itemize}

\begin{example}[]\protect\hypertarget{exm-}{}\label{exm-}

Para o nosso exemplo, obtivemos
\(\theta|\boldsymbol{x}\sim\hbox{Gama}(953.8,37.1)\). Então, uma
estimativa pontual para \(\theta\) é

\[E(\theta|\boldsymbol{x})=\frac{953,8}{37,1}=25,7\] registros de
suicídios mensais.

Podemos obter os quantis de 2,5\% e 97,5\% para construir um intervalo
com 95\% de credibilidade:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qgamma}\NormalTok{( }\FunctionTok{c}\NormalTok{(.}\DecValTok{025}\NormalTok{,.}\DecValTok{975}\NormalTok{), }\FloatTok{953.8}\NormalTok{, }\FloatTok{37.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 24.10301 27.36583
\end{verbatim}

logo, inferimos que \(\theta\in(24.1,27.36)\) com probabilidade 0,95.

Por último, recorde-se que discutimos, utilizando apenas os dados
nacionais, que \(\theta\) deveria está próximo de 22,8. Após observa a
amostra, existem evidências de que \(H:\theta>22,8\)? Para responder a
esse questionamento, podemos calcular

\[P(H|\boldsymbol{x})=P(\theta>22.8|\boldsymbol{x})\]

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{pgamma}\NormalTok{(}\FloatTok{22.8}\NormalTok{, }\FloatTok{953.8}\NormalTok{, }\FloatTok{37.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9998554
\end{verbatim}

logo, com uma probabilidade maior que 99,9\%, existem fortes evidências
de que o número médio mensal de suicídios no Amazonas é maior do que
22,8 registros por mês.

\end{example}

\section{Exercício}\label{exercuxedcio}

É fato que a taxa de suicídio é maior entre os homens, embora a
tentativa seja maior entre as mulheres. Uma das explicações está no fato
de que os homens tendem a utilizar métodos mais letais para o suicídio,
como armas de fogo, enquanto as mulheres são mais propensas a utilizar
métodos menos letais, como overdose de medicamentos.

Dados históricos, entre 1996 e 2017, sugerem que 79\% dos suicídios são
cometidos por homens.

Seja \(\rho\) a probabilidade de que um suicídio seja cometido por
alguém do sexo masculino no Estado do Amazonas. Considere a priori
\(\rho\sim\hbox{Beta}(a,b)\), onde sabemos que

\[f(\rho)\propto \rho^{a-1}(1-\rho)^{b-1},\] \[E(\rho)=\frac{a}{a+b},\]
e \[Var(\rho)=\frac{E(\rho)(1-E(\rho))}{a+b+1}.\]

\begin{itemize}
\item
  Encontre valores de \(a\) e \(b\) que reflitam a média de 0,79 mas que
  não restrinjam demais os valores possíveis para \(\rho\)
\item
  No Amazonas, entre os anos 2021 e 2023, dos 933 registros de
  suicídios, 726 foram cometidos por homens. Considere então o modelo
  \(X|\rho\sim\hbox{Binomial}(933,\rho)\). Mostre que
  \[f(\theta|x)\propto \rho^{726+a-1}(1-\rho)^{207+b-1}\] e conclua que
  \(\rho|x\sim\hbox{Beta}(a+933,b+207)\).
\item
  Estime \(\theta\) e construa um intervalo de 95\% de credibilidade
\item
  Teste a hipótese de que, para o Amazonas, a probabilidade de um
  indivíduo do sexo masculino cometer suicídio é maior do que 79\%.
\item
  Para problemas envolvendo proporções, é comum o uso a priori
  \(\rho\sim\hbox{Uniforme}(0,1)\), que é equivalente à
  \(\rho\sim\hbox{Beta}(1,1)\). Refaça este exercício com essa priori e
  discuta sobre as diferenças encontradas tanto nas estimações quanto no
  teste de hipóteses.
\end{itemize}

\section{Resumo da aula 1}\label{resumo-da-aula-1}

\begin{itemize}
\item
  Existem duas fontes de informação na inferência bayesiana: os dados
  (verossimilhança) e a informação anterior (priori)
\item
  A informação \textbf{a priori} é subjetiva: pessoas diferentes têm
  prioris diferentes
\item
  O Teorema de Bayes combina as duas fontes em uma nova informação, dada
  pela distribuição \textit{a posteriori}
\item
  Os objetivos da inferência (estimação e testes) são feitos a partir da
  distribuição \textbf{a posteriori}
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Famílias conjugadas}\label{famuxedlias-conjugadas}

\section{Dados que serão utilizados neste
capítulo}\label{dados-que-seruxe3o-utilizados-neste-capuxedtulo}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{require}\NormalTok{(gsheet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Carregando pacotes exigidos: gsheet
\end{verbatim}

\begin{verbatim}
Warning: pacote 'gsheet' foi compilado no R versão 4.4.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{url }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}https://docs.google.com/spreadsheets/d/1dLDCjA9a8UgXA9sJ1TNCVvRCDFITalewGkHlc\_\_Eg20/edit?usp=sharing\textquotesingle{}} 
\NormalTok{dt }\OtherTok{\textless{}{-}} \FunctionTok{gsheet2tbl}\NormalTok{(url)}
\FunctionTok{head}\NormalTok{(dt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  REGIAO              TIPO     SEXO  PERIODO   TOTAL
  <chr>               <chr>    <chr> <chr>     <dbl>
1 Região Norte        AGRESSAO Masc  2001-2003  7628
2 Região Nordeste     AGRESSAO Masc  2001-2003 29652
3 Região Sudeste      AGRESSAO Masc  2001-2003 63411
4 Região Sul          AGRESSAO Masc  2001-2003 12428
5 Região Centro-Oeste AGRESSAO Masc  2001-2003  9319
6 Região Norte        AGRESSAO Fem   2001-2003   677
\end{verbatim}

\section{Família de distribuições
conjugadas}\label{famuxedlia-de-distribuiuxe7uxf5es-conjugadas}

::: \{\#def-Priori conjugada\} Dizemos que a priori
\(f(\boldsymbol{\theta})\) é conjugada para a verossimilhança
\(L(\boldsymbol{\theta})\) se \emph{priori} e \emph{posteriori}
pertencem à mesma família de distribuições. :::

\begin{example}[]\protect\hypertarget{exm-}{}\label{exm-}

Sejam \(X_1,\ldots,X_n\) variáveis aleatórias independentes com
\(X|\theta\sim \hbox{Bernoulli}(\theta)\) e
\(\theta\sim\hbox{Beta}(a,b)\). Então

\[\begin{align}
    f(\theta|\boldsymbol{x})&\varpropto L(\theta)f(\theta) = \underbrace{\theta^{\sum_{i=1}^{n}x_i} (1-\theta)^{n-\sum_{i=1}^{n}x_i}}_{L(\theta)}\underbrace{\theta^{a-1}(1-\theta)^{b-1}}_{f(\theta)}\\
    &=\theta^{a+\sum_{i=1}^n x_i-1}(1-\theta)^{b+n-\sum_{i=1}^{n}x_i-1},
    \end{align}\] logo
\(\theta|\boldsymbol{x}\sim\hbox{Beta}(a+\sum_{i=1}^{n}x_i,b+n-\sum_{i=1}^{n}x_i)\)
e a \emph{priori} beta é conjugada para a verossimilhança Bernoulli.

\end{example}

Famílias conjugadas são extremamente úteis tanto sob o ponto de vista
algébrico quanto computacional. Entretanto, note que a definição de
família conjugada é ampla. Por exemplo, \emph{priori} e
\emph{posteriori} sempre pertencem à grande família de todas as
distribuições de probabilidade, sendo esta a família conjugada trivial.

Famílias conjugadas não triviais são raras, existindo principalmente
quando a distribuição condicional dos dados pertence à família
exponencial.

::: \{\#def-Família exponencial\} Definição Considere que \(\Theta\) tem
dimensão \(k\). Dizemos que \(X|\boldsymbol{\theta}\) pertence à família
exponencial (natural) se
\[f(x|\boldsymbol{\theta})=h(x)a(\boldsymbol{\theta})\exp\left\{\sum_{j=1}^k t_j(x)\theta_j\right\},\]
onde \(\mathcal{X}\) não depende de \(\boldsymbol{\theta}\). Além disso,
para a amostra (iid )\(X_1,\ldots,X_n|\boldsymbol{\theta}\),
\[f(\boldsymbol{x}|\boldsymbol{\theta})=h(\boldsymbol{x})a(\boldsymbol{\theta})^n\exp\left\{\sum_{j=1}^k T_j\theta_j\right\},\]
onde \(T_j=\sum_{i=1}^{n}t_j(x_i)\) :::

\begin{theorem}[]\protect\hypertarget{thm-}{}\label{thm-}

Se \(X|\boldsymbol{\theta}\) pertence à família exponencial, então
\[f(\boldsymbol{\theta})=c(\boldsymbol{r},s)a(\boldsymbol{\theta})^s\exp\left\{\sum_{j=1}^k r_j\theta_j\right\}\]
é uma \emph{priori} conjugada (ver O'Hagan (2005) para a existência
dessa distribuição). A \emph{posteriori} será dada por
\[f(\boldsymbol{\theta}|\boldsymbol{x})=c\left(\sum_{j=1}^k r_j+T_j,s+n\right)a(\boldsymbol{\theta})^{s+n}\exp\left\{\sum_{j=1}^k(r_j+T_j)\theta_j\right\}\]

\end{theorem}

Prova

\[\begin{align}
f(\boldsymbol{\theta}|\boldsymbol{x})&\varpropto \underbrace{a(\boldsymbol{\theta})^ne^{\sum_{j=1}^kT_j\theta_k}}_{L(\boldsymbol{\theta})}\underbrace{a(\boldsymbol{\theta})^s e^{\sum_{j=1}^k r_j\theta_k}}_{f(\boldsymbol{\theta})}\\
&a(\boldsymbol{\theta})^{n+s}e^{(\sum_{j=1}^{k}T_j+r_j)\theta_j}
\end{align}\]

Considere que \(\boldsymbol{\theta}\sim C(\boldsymbol{r},s)\) é a
distribuição conjugada da verossimilhança. Isto implica em
\(\boldsymbol{\theta}|\boldsymbol{x}\sim\hbox{C}(\boldsymbol{T}+\boldsymbol{r},s+n)\).
Note que a \emph{posteriori} atualiza a informação de \(s\) para \(s+n\)
e de \(r_j\) para \(T_j+r_j\). Logo, se imaginarmos que a priori é um
experimento hipotético, \(s\) seria o tamanho da amostra e
\(\boldsymbol{r}\) seriam as estatísticas suficientes deste modelo.

Abaixo, segue uma lista de algumas prioris conjugadas para modelos na
família exponencial.

\[\begin{array}{c|c|c}\hline
\text{Modelo} & \text{Priori} & \text{Posteriori} \\ \hline
\text{Bernoulli}(\theta) & \text{Beta}(a,b) & \text{Beta}(a+\sum_i x_i, b+n-\sum_i x_i) \\ \hline
\text{Poisson}(\lambda) & \text{Gama}(a,b) & \text{Gama}(a+\sum_i x_i, b+n) \\ \hline
\text{Multinomial}(p_1,\ldots,p_k) & \text{Dirichlet}(a_1,\ldots,a_k) & \text{Dirichlet}(a_1+x_1,\ldots,a_k+x_k) \\
\text{Exponencial}(\lambda) & \text{Gama}(a,b) & \text{Gama}(a+n, b+x_i) \\
\text{Normal}(\mu,\phi^{-1})&\text{Normal-Gama}(m_0,n_0,v_0,s_0^2)&
\text{Normal-Gama}(m_1,n_0+n,v_0+n,s_1^2)\\
& & m_1= \frac{n}{n_1}\bar{x}+\frac{n_0}{n_1}m_0\\
& & s_1^2=\frac{n_0n}{n_1^2}(\bar{x}-m_0)^2 + \frac{(n-1)s^2+n_0 s_0^2}{n_1} \\ \hline
\end{array}
\]

\section{Prioris conjugadas fora da família
exponencial}\label{prioris-conjugadas-fora-da-famuxedlia-exponencial}

Famílias conjugadas fora da família exponencial são raras. Seja
\(X_1,\ldots,X_n\) variáveis aleatória independentes com
\(X_1|\phi,\psi\sim\hbox{Binomial Negativa}(\psi,\phi)\), onde

\[f(x|\phi,\psi)=\frac{\Gamma(x+\psi)}{\Gamma(\psi)x!}\phi^\psi (1-\phi)^x,\]
com \(\psi>0\), \(\phi\in(0,1)\) e \(x\in\mathbb{N}\). Se \(\psi\) é
conhecido, então
\[L(\boldsymbol{\theta})=\underbrace{\left[\frac{\prod_{i=1}^n\Gamma(x_i+\psi)}{\Gamma(\psi)^n\prod_{i=1}^n x_i!}\right]}_{h(\boldsymbol{x})}\underbrace{\phi^{n\psi}}_{a(\phi)}\exp\left\{ \underbrace{\sum_{i=1}^n x_i}_{t(\boldsymbol{x})} \underbrace{\log(1-\phi)}_{w(\phi)}\right\} \]

Então, \[\begin{align}
f(\phi|\psi)&=c(r,s)a(\phi)^s e^{rw(\phi)}\\
&=c(r,s)\phi^{s\psi}(1-\phi)^{r}
\end{align}\] é uma \(*priori*\) conjugada. Da expressão acima segue que
\(\phi|\psi \sim \hbox{Beta}(s\psi+1,r+1)\). A \(*posteriori*\)
(condicional) por sua vez é dada por
\[f(\phi|\boldsymbol{x},\psi)\varpropto \phi^{\psi(s+n)}(1-\phi)^{r+\sum_{i=1}^{n}x_i},\]
onde ainda
\(\phi|\psi,\boldsymbol{x}\sim\hbox{Beta}(\psi(s+n)+1,r+\sum_{i=1}^{n}x_i+1)\)
Note que para fazer a inferência completa, ainda necessitamos de
\(\psi|\boldsymbol{x}\), uma vez que
\[f(\phi,\psi|\boldsymbol{x})=f(\phi|\psi,\boldsymbol{x})f(\psi|\boldsymbol{x}).\]
Outro método de obter a conjunta \((\phi,\psi|\boldsymbol{x})\), sem a
necessidade de calcular \(\psi|\boldsymbol{x}\) será discutido na
próxima aula.

\bookmarksetup{startatroot}

\chapter{O estimador de bayes}\label{o-estimador-de-bayes}

Considere o problema de tomar alguma decisão sobre \(\theta\) utilizando
uma estatística \(T(\mathbf{x})\).

Podemos determinar o quão ruim é a nossa decisão definindo uma função de
perda \(\mathcal{L}(\theta,T)\), com as seguintes características:

\begin{itemize}
\item
  \(\mathcal{L}(\theta,T)=0\) sempre que \(T\) for a decisão correta em
  relação à \(\theta\)
\item
  \(\mathcal{L}(\theta,T)>0\) em caso contrário.
\end{itemize}

Por exemplo, se \(T\) for um estimador para \(\theta\), a decisão
correta seria ter \(T=\theta\). Além disso, quanto mais afastado \(T\)
estiver de \(\theta\), pior é a decisão e maior deveria ser a perda.

No problema de estimação pontual, é usual utilizar a perda quadrática:
\[\mathcal{L}(\theta,T)=(T-\theta)^2,\] cujo esboço do gráfico é dado
abaixo:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot.new}\NormalTok{()}
\FunctionTok{plot.window}\NormalTok{(}\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\FunctionTok{curve}\NormalTok{( x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =}\NormalTok{ T)}
\FunctionTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{at =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\FunctionTok{expression}\NormalTok{(theta }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{),}\FunctionTok{expression}\NormalTok{(theta }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{),}\FunctionTok{expression}\NormalTok{(theta),}\FunctionTok{expression}\NormalTok{(theta }\SpecialCharTok{+}\DecValTok{1}\NormalTok{),}\FunctionTok{expression}\NormalTok{(theta }\SpecialCharTok{+}\DecValTok{2}\NormalTok{) ))}
\FunctionTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\FunctionTok{title}\NormalTok{( }\AttributeTok{ylab =} \StringTok{\textquotesingle{}Perda quadrática\textquotesingle{}}\NormalTok{,}\AttributeTok{xlab =} \StringTok{\textquotesingle{}T\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{estimacao_files/figure-pdf/unnamed-chunk-1-1.pdf}

Para uma decisão \(T\) podemos calcular a perda média
\[R_T(\theta)=E(\mathcal{L}(\theta,T))=\int L(\theta,T(\mathbf{x}))f(\mathbf{x}|\theta)d\mathbf{x}\]
A função acima é conhecida como risco da decisão \(T\) e é variável em
\(\theta\). Seu uso é simples: se \(R_T(\theta)<R_U(\theta)\), então em
média a decisão \(T\) tem menor perda que \(U\). Assim, a melhor escolha
entre as duas decisão é \(U\).

O risco associado à perda quadrática é denominado erro quadrático médio:

\[R_T(\theta)=E(T-\theta)^2= Var(T)+(E(T|\theta)-\theta)^2\] Ele possui
papel importante na inferência pontual frequentista, como por exemplo,
para definir o melhor estimador não viciado de variância uniformemente
mínima.

O risco de Bayes da decisão \(T\) é o valor esperado do seu respectivo
risco \textit{a priori}, \[r_T=\int R_T(\theta)f(\theta)d\theta,\] sendo
portanto uma constante. Qualquer decisão com o menor risco para todo
\(\theta\) também tem o menor risco de Bayes. Dizemos que \(T\) é o
estimador de Bayes se \(r_T<r_U\) para qualquer decisão \(U\).

Teorema

O estimador \(T\) que minimiza
\[\int \mathcal{L}(\theta,T(\mathbf{x}))f(\theta|\mathbf{x})d\theta\] é
o estimador de Bayes.

Exemplo

Vamos encontrar o estimador de Bayes para a perda quadrática. Temos que

\[\begin{align*}\int \mathcal{L}(\theta,T(\mathbf{x}))f(\theta|\mathbf{x})d\theta&=\int (T(\mathbf{x})-\theta)^2f(\theta|\mathbf{x})d\theta\\
    &=T(\mathbf{x})^2 +\int \theta^2f(\theta|\mathbf{x})d\theta-2T(\mathbf{x})\int \theta f(\theta|\mathbf{x})d\theta\\
    &= T(\mathbf{x})^2 + E(\theta^2|\mathbf{x}) -2T(\mathbf{x})E(\theta|\mathbf{x})\\
    &= T(\mathbf{x})^2 + E(\theta^2|\mathbf{x}) -2T(\mathbf{x})E(\theta|\mathbf{x}) \pm E(\theta|\mathbf{x})^2\\
    &=\left( T(\mathbf{x}) - E(\theta|\mathbf{x})\right)^2 +E(\theta^2|\mathbf{x})-E(\theta|\mathbf{x})^2\\
    &=\left( T(\mathbf{x}) - E(\theta|\mathbf{x})\right)^2 +Var(\theta|\mathbf{x})
\end{align*}\] A função acima é minimizada em
\(T(\mathbf{x})=E(\theta|\mathbf{x})\). Disto, mostra-se que
\[r_T\geq Var(\theta|\mathbf{x})\] e a variância da posteriori pode ser
utilizada como medida de erro. Como a unidade deste erro está ao
quadrado, é usual utilizarmos o desvio padrão da posteriori como medida
de erro.

\begin{example}[]\protect\hypertarget{exm-}{}\label{exm-}

Seja \(X_1,\ldots,X_n\) uma amostra aleatória do modelo
Exponencial(\(\theta\)) e considere a priori
\(\theta\sim\hbox{Gama}(a,b)\). Sabemos que
\(\theta|\boldsymbol{x}\sim\hbox{Gama}(s+n,r+\sum_{i=1}^n x_i)\). Então,
o estimador de Bayes segundo a perda quadrática é
\[T(\boldsymbol{x})=E(\theta|\boldsymbol{x})=\frac{s+n}{r+\sum_{i=1}^n x_i},\]
e o erro associado é
\[\sqrt{Var(\theta|\boldsymbol{x})}=\sqrt{\frac{s+n}{(r+\sum_{i=1}^n x_i)^2}}=\sqrt{\frac{E(\theta|\boldsymbol{x})^2}{s+n}}=\frac{E(\theta|\boldsymbol{x})}{\sqrt{n+s}}.\]

Note que, como esperado, o erro decresce na medida que \(n\) cresce.

\end{example}

\bookmarksetup{startatroot}

\chapter{O modelo Bernoulli}\label{o-modelo-bernoulli}

\section{O modelo univariado}\label{o-modelo-univariado}

A distribuição \(F\) é dita pertencer à família Bernoulli, com parâmetro
\(\theta\in(0,1)\) se sua função de probabilidade é dada por
\[f(x|\theta)=\theta^x(1-\theta)^{1-x},\] com \(x\in\{0,1\}\). É
imediato que \(\theta\) representa a probabilidade de \(\{X=1\}\), sendo
este evento conhecido como `sucesso' em alguns textos (em contrapartida,
\(\{X=0\}\) é conhecido como `fracasso').

Esta distribuição faz a importante conexão entre variáveis categóricas e
aleatórias, tendo papel fundamental na inferência não paramétrica. Por
exemplo, seja \(S\) o sexo de um indivíduo selecionado ao acaso. Tal
variável é categórica, podendo assumir os resultados \(A=\)\{feminino\}
ou \(A^c\). Contudo, pode-se definir a variável \(X=I(A)\), onde
\(I(.)\) é a função indicadora, definida por
\[I(A)=\left\{\begin{array}{ll} 1,&\hbox{ se $A$ ocorre,} \\ 0,&\hbox{ se $A^c$ ocorre.}\end{array}\right.
\] Deste modo \(X\sim\hbox{Bernoulli}(\theta)\).

Como discutido anteriormente, o modelo Bernoulli(\(\theta\)) pertence à
família exponencial e sua distribuição conjugada é a Beta\((a,b)\). A
distribuição a posteriori é dada por

\[f(\theta|\boldsymbol{x})\propto \theta^{a+\sum_{i=1}^n x_i-1}(1-\theta)^{n-\sum_{i=1}^n x_i+b-1},\]
ou seja,
\(\theta|\boldsymbol{x}\sim\hbox{Beta}(a+\sum_{i=1}^n x_i,n-\sum_{i=1}^n x_i + b).\)
Observe que \(a\) pode ser interpretado como o número de sucessos a
priori e \(b\) o número de fracassos. É comum utilizarmos a priori
Beta\((1,1)\), que é equivalente à Uniforme(0,1), como priori pouco
informativa.

\begin{example}[]\protect\hypertarget{exm-}{}\label{exm-}

\textbf{As duas clínicas}

Em 1846, Ignaz Philipp Semmelweis se tornou assistente na Primeira
Clínica de Obstetrícia do Hospital Geral de Viena, Áustria (algo como o
residente chefe). Nestas clínicas, o infanticídio era oferecido de
graça. Em troca, o parto serveria de treinamento para os médicos e
parteiras.

Naquela época, a febre puerperal era comum e muitas vezes fatal, sendo
que a mortalidade variava entre 10\% a 35\%.

Haviam duas maternidades no Hospital Geral de Viena, conhecidas como a
Primeira e a Segunda. A Primeira era considerada um local de morte e era
evitada quando possível. Abaixo temos os dados registrados pelo
Dr.~Semmelweis

\[\begin{array}{c|cc|cc}\hline \hbox{} & \hbox{Primeira} & \hbox{Clínica} & \hbox{Segunda} & \hbox{Clínica}\\ \hline
\hbox{Ano} & \hbox{Partos} & \hbox{Mortes} &\hbox{Partos} & \hbox{Mortes} \\ \hline 
1841 & 3036 & 237 & 2442 & 86 \\
1842 & 3287 & 518 & 2659 & 202 \\
1843 & 3060 & 274 & 2739 & 164 \\
1844 & 3157 & 260 & 2956 & 68 \\
1845 & 3492 & 241 & 3241 & 66 \\
1846 & 4010 & 459 & 3754 & 105 \\ \hline
\hbox{Total} & 20.042 & 1.989 & 17.791 & 691 \\ \hline
\end{array}\]

Pode-se considerar que cada parto gera duas possibilidades de eventos: a
sobrivência ou a morte da mãe. Considere que, dentro da mesma clínica,
esses eventos para cada mãe são indepentes e possuem a mesma
probabilidade. Seja \(\alpha\) a probabilidade de morte na Primeira
Clínica e \(\beta\) a mesma probabilidade para a Segunda Clínica. Então,
as funções de verossimilhança para cada probabilidade são

\[\begin{align}L(\alpha)&\propto \alpha^{1989} (1-\alpha)^{18053}\\L(\beta)&\propto \beta^{691} (1-\beta)^{17100}\end{align}\]
Utilizando distribuição Uniforme(0,1) como priori para \(\alpha\) e
\(\beta\), teremos que

\[\begin{align}\alpha|\boldsymbol{x}&\sim \hbox{Beta}(1990,18054) \\\beta|\boldsymbol{x}&\sim \hbox{Beta}(692,17101)\end{align}\]

Abaixo, mostramos as duas posterioris, de onde podemos concluir que a
probabilidade de morte na Primeira Clínica é certamente maior que na
Segunda.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{curve}\NormalTok{(}\FunctionTok{dbeta}\NormalTok{(x,}\DecValTok{1990}\NormalTok{,}\DecValTok{18054}\NormalTok{),}\FloatTok{0.03}\NormalTok{,.}\DecValTok{11}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{,}\AttributeTok{ylab=}\StringTok{\textquotesingle{}Densidades a posteriori\textquotesingle{}}\NormalTok{,}\AttributeTok{xlab=}\StringTok{\textquotesingle{}Probabilidade de morte\textquotesingle{}}\NormalTok{,}\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{300}\NormalTok{))}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{dbeta}\NormalTok{(x,}\DecValTok{692}\NormalTok{,}\DecValTok{17101}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =}\NormalTok{ T)}
\FunctionTok{text}\NormalTok{(}\FunctionTok{c}\NormalTok{(.}\DecValTok{04}\NormalTok{,.}\DecValTok{1}\NormalTok{),}\FunctionTok{c}\NormalTok{(}\DecValTok{290}\NormalTok{,}\DecValTok{210}\NormalTok{),}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Segunda Clínica\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Primeira Clínica\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bernoulli_files/figure-pdf/unnamed-chunk-1-1.pdf}

O Dr.~Semmelweis ainda não tinha descoberto o motivo dessas mortes até a
morte de seu amigo Jakob Kolletschka, que se cortou acidentalmente com
um bisturi durante uma autópsia. Durante a autópsia de Jakob, o
Dr.~Semmelweis viu semelhanças com as autópsias as mulheres que haviam
morrido por febre puerperal.

Na Primeira Clínica estudavam os alunos de medicina, que realizavam
autópsias. Na Segunda Clínia estudavam as parteiras, que não realizam
autópsias. A sua hipótese foi: estudantes de medicina carregavam
partículas cadavéricas que causavam a febre puerperal. Com essa
hipótese, ele instituiu que todos os médicos deveriam lavar as mãos
antes dos partos em maio de 1847. Abaixo, seguem os dados de Junho de
1848 até Março 1849, apenas para a Primeira Clínica

\[\begin{array}{c|cc} \hline \hbox{Período} & \hbox{Partos} & \hbox{Mortes}\\ \hline
\hbox{Jun/1847-Dez/1847} & 1841 & 56 \\ 
\hbox{Jan/1848-Dez/1848} & 3556 & 45 \\ 
\hbox{Jan/1849-Mar/1849} & 1198 & 41 \\ \hline
\hbox{Total} & 6.595 & 142 \\ \hline
\end{array}\]

Seja \(\gamma\) a probabilidade de morte por febre puerperal na Primeira
Clínica após a instrução de lavagem das mãos. Sua função de
verossimilhança é

\[L(\gamma)\propto \gamma^{142}(1-\gamma)^{6453}.\] Assumindo a priori
Uniforme(0,1) para \(\gamma,\) teremos que
\(\gamma|\boldsymbol{x}\sim\hbox{Beta}(143,6454)\). Abaixo, apresentamos
o gráfico das três densidades a posteriori obtidas, mostrando que
\(\gamma\) é certamente menor que as outras probabilidades.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{curve}\NormalTok{(}\FunctionTok{dbeta}\NormalTok{(x,}\DecValTok{1990}\NormalTok{,}\DecValTok{18054}\NormalTok{),}\DecValTok{0}\NormalTok{,.}\DecValTok{11}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{,}\AttributeTok{ylab=}\StringTok{\textquotesingle{}Densidades a posteriori\textquotesingle{}}\NormalTok{,}\AttributeTok{xlab=}\StringTok{\textquotesingle{}Probabilidade de morte\textquotesingle{}}\NormalTok{,}\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{300}\NormalTok{))}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{dbeta}\NormalTok{(x,}\DecValTok{692}\NormalTok{,}\DecValTok{17101}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =}\NormalTok{ T)}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{dbeta}\NormalTok{(x,}\DecValTok{143}\NormalTok{,}\DecValTok{6454}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =}\NormalTok{ T)}

\FunctionTok{text}\NormalTok{(}\FunctionTok{c}\NormalTok{(.}\DecValTok{01}\NormalTok{,.}\DecValTok{04}\NormalTok{,.}\DecValTok{1}\NormalTok{),}\FunctionTok{c}\NormalTok{(}\DecValTok{250}\NormalTok{,}\DecValTok{290}\NormalTok{,}\DecValTok{212}\NormalTok{),}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Primeira Clínica }\SpecialCharTok{\textbackslash{}n}\StringTok{(1847{-}1849)\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Segunda Clínica\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Primeira Clínica}\SpecialCharTok{\textbackslash{}n}\StringTok{(1841{-}1846)\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bernoulli_files/figure-pdf/unnamed-chunk-2-1.pdf}

Após a publicação de seus achados, as ideia do Dr.~Semmelweis ideias
foram amplamente rejeitadas por seus colegas médicos, que se sentiram
ofendidos com a sugestão de que poderiam estar causando a morte de seus
pacientes. A rejeição e o ridículo que Semmelweis enfrentou levaram a um
declínio em sua saúde mental. Ele foi internado no hospício em 1865,
onde morreu pouco tempo depois. Morreu em 13 de agosto de 1865, em um
hospício em Viena, aos 47 anos. A causa exata de sua morte ainda é
debatida, mas a teoria mais aceita é que ele morreu de septicemia, uma
infecção sanguínea, após ser espancado pelos guardas do hospício.

\end{example}

\section{O modelo multivariado}\label{o-modelo-multivariado}

A distribuição Bernoulli pode ser generalizada para o caso multivariado:
considere um evento aleatório com possibilidades \(A_1,\ldots,A_k\).
Seja \(X_j=I(A_j)\) e seja \(\theta_j\) a probabilidade do evento
resultar em \(A_j\). Então, o vetor \(\boldsymbol{X}=(X_1,\ldots,X_k)\)
tem distribuição Bernoulli multivariada, cuja função de probabilidade é
dada por
\[f(\boldsymbol{x}|\boldsymbol{\theta})=\prod_{j=1}^k\theta_j^{x_{j}},\]
onde \(x_j\in\{0,1\}\), \(\sum_{j=1}^kx_j=1\), \(\theta_j\in(0,1)\) e
\(\sum_{j=1}^k\theta_j=1\). É importante notar que vetor
\(\boldsymbol{\theta}\) tem apenas \(k-1\) parâmetros de fato, uma vez
que \(\theta_k=1-\sum_{j=1}^{k-1}\theta_j\). Conjuntos do tipo
\[\mathcal{S}^k=\left\{(\theta_1,\ldots,\theta_{k-1}):0<\theta_j<1,0<\sum_{j=1}^{k-1}\theta_j<1\right\}\]
são denominados simplex.

Seja \(\boldsymbol{x}_1,\ldots,\boldsymbol{x}_n\) uma amostra de vetores
Bernoulli Multivariada(\(\theta_1,\ldots,\theta_k\)), onde
\(\boldsymbol{x}_i=\{x_{i,1},\ldots,x_{i,k}\}\). Então,

\[\begin{align}L(\boldsymbol{\theta})&=\prod_{i=1}^nf(\boldsymbol{x}_i|\boldsymbol{\theta})=\prod_{i=1}^n\left(\prod_{j=1}^k \theta_j^{x_{i,j}}\right)=\prod_{j=1}^k \prod_{i=1}^n\theta_j^{x_{i,j}}\\&=\prod_{j=1}^k \theta_j^{\sum_{i=1}^nx_{i,j}}=\prod_{j=1}^n\theta_j^{n_j}\end{align}\]
onde \(n_j\) é o número de vezes que ocorreu a categoria \(A_j\). É
imediado que

\[L(\boldsymbol{\theta})=\exp\left\{\sum_{j=1}^k n_j\log(\theta_j)\right\},\]
o que implica que este modelo pertence à família exponencial. Seu modelo
conjugado é a Dirichlet(\(a_1,\ldots,a_k\)), cuja função densidade é
\[f(\theta_1,\ldots,\theta_k)=\frac{\Gamma\left(\sum_{j=1}^k a_j\right)}{\prod_{j=1}^k \Gamma(a_j)}\prod_{j=1}^k \theta^{a_j-1},\]
onde \((\theta_1,\ldots,\theta_{k-1})\) pertence ao simplex
\(\mathcal{S}^k\).

A Dirichlet\((a_1,\ldots,a_k)\) possui as seguintes propriedades:

\begin{itemize}
\tightlist
\item
  \(\theta_j\sim\hbox{Beta}(a_j,\sum_{i\neq j}a_i)\)
\item
  \((\theta_1,\ldots,\theta_i+\theta_j,\ldots,\theta_k)\sim\hbox{Diriculet}(a_1,\ldots,a_i+a_j,\ldots,a_k)\)
\end{itemize}

Da primeira propriedade, concluímos que
\[E(\theta_j)=\frac{a_j}{\sum_{i=1}^k a_i},\;\;Var(\theta_j)=\frac{E(\theta_j)(1-E(\theta_j))}{\sum_{i=1}^k a_i+1}.\]

Utilizando o modelo conjugado, a distribuição a posteriori de
\(\theta_1,\ldots,\theta_k\) é

\[f(\boldsymbol{\theta}|\boldsymbol{x})\propto \prod_{i=j}^k \theta_j^{n_j+a_j-1}\]
ou seja,
\(\boldsymbol{\theta}|\boldsymbol{x}\sim\hbox{Dirichlet}(n_1+a_1,\ldots,n_k+a_k)\).
Novamente, pode-se utilizar \(a_1=\cdots=a_k=1\) para obter uma priori
pouco informativa.

\begin{example}[]\protect\hypertarget{exm-}{}\label{exm-}

\textbf{Imagem corporal}

O projeto Estado nutricional e sua relação com a imagem corporal em
escolares do município de Manaus foi submetido ao LabEst em 2013. Nele,
estudantes identificavam como gostaria que fosse o seu corpo segundo a
Escala de Stunkard, apresentada na imagem abaixo. Em seguida, uma série
de medidas foram realizadas para determinar a real classificação do
estudante. Com base nessas informações, cada estudante foi classificado
segundo sua satisfação com o próprio corpo do seguinte modo:

\begin{itemize}
\item
  Satisfeito: seu desejo é equivalente ao seu estado atual.
\item
  Insatisfeito por excesso: o estudante gostaria ter medidas menores.
\item
  Insatisfeito por magreza: o estudante gostaria ter medidas maiores.
\end{itemize}

\begin{figure}[H]

{\centering \includegraphics{percepcao-da-imagem-corporal-do-estudante-01.jpg}

}

\caption{Escala de Stunkard}

\end{figure}%

Neste exemplo, vamos analisar o recorte dos resultados para alunos entre
16 e 17 anos, diferenciando entre os sexos. As frequências estão
sumarizadas na tabela abaixo.

\[\begin{array}{c|ccc|c}\hline
&\hbox{Satisfeito} & \hbox{Insatifeito por excesso} & \hbox{Insatisfeito por magreza} &\hbox{Total}\\ \hline
\hbox{Masculino} & 24 & 10 & 24 & 58 \\
\hbox{Feminino} & 14 & 22 & 24 & 60 \\ \hline
\end{array}
\] Cada estudante pode assumir uma das três classificações. Sejam
\(\alpha_S,\alpha_E,\alpha_M\) as probabilidades de alguém do sexo
masculino estar classificado como Satisfeito, Insatisfeito por Excesso
ou Insatisfeito por magreza, respectivamente. Então a função de
verossimilhança para \(\boldsymbol{\alpha}\) é

\[L(\boldsymbol{\alpha})=\alpha_S^{24}\alpha_E^{10}\alpha_M^{24}.\]
Analogamente, fazendo \(\boldsymbol{\beta}=(\beta_S,\beta_E,\beta_M)\),
as mesmas probabilidades para o sexo feminino, teremos que

\[L(\boldsymbol{\alpha})=\beta_S^{14}\beta_E^{22}\\beta_M^{24}.\]

Utilizando a priori Dirichelt(1,1,1) tanto para \(\boldsymbol{\alpha}\)
quanto para \(\boldsymbol{\beta}\), teremos que as respectivas
posterioris para \(\boldsymbol{\alpha}\) e \(\boldsymbol{\beta}\) são
Dirichelt(25,11,25) e Dirichlet(15,23,25).

As posterioris para \(\alpha_M\) e \(\beta_M\) são Beta(25,36)e
Beta(25,38). As estimativas pontuais são 0,40 e 0,39, respectivamente. A
imagem abaixo mostra a insatisfação por magreza entre os sexos deve ser
a mesma.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{curve}\NormalTok{(}\FunctionTok{dbeta}\NormalTok{(x,}\DecValTok{25}\NormalTok{,}\DecValTok{36}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{xlab =} \StringTok{\textquotesingle{}Probabilidade de insatisfação por magreza\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab=} \StringTok{\textquotesingle{}Densidades marginais a posteriori\textquotesingle{}}\NormalTok{, }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{8}\NormalTok{),}\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(.}\DecValTok{1}\NormalTok{,.}\DecValTok{8}\NormalTok{),}\AttributeTok{col=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{dbeta}\NormalTok{(x,}\DecValTok{25}\NormalTok{,}\DecValTok{38}\NormalTok{),}\AttributeTok{add =}\NormalTok{ T, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\FunctionTok{c}\NormalTok{(.}\DecValTok{6}\NormalTok{,.}\DecValTok{2}\NormalTok{),}\FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Masculino\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Feminino\textquotesingle{}}\NormalTok{),}\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bernoulli_files/figure-pdf/unnamed-chunk-3-1.pdf}

As posterioris para \(\alpha_E\) e \(\beta_E\) são Beta(11,50)e
Beta(23,40). As estimativas pontuais são 0,21 e 0,36, respectivamente. A
imagem abaixo mostra que as mulheres em geral parecem possuir maior
probabilidade de insatisfação por excesso.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{curve}\NormalTok{(}\FunctionTok{dbeta}\NormalTok{(x,}\DecValTok{11}\NormalTok{,}\DecValTok{50}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{xlab =} \StringTok{\textquotesingle{}Probabilidade de insatisfação por excesso\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab=} \StringTok{\textquotesingle{}Densidades marginais a posteriori\textquotesingle{}}\NormalTok{, }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{),}\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,.}\DecValTok{6}\NormalTok{))}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{dbeta}\NormalTok{(x,}\DecValTok{23}\NormalTok{,}\DecValTok{40}\NormalTok{),}\AttributeTok{add =}\NormalTok{ T, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\FunctionTok{c}\NormalTok{(.}\DecValTok{2}\NormalTok{,.}\DecValTok{4}\NormalTok{),}\FunctionTok{c}\NormalTok{(}\DecValTok{9}\NormalTok{,}\DecValTok{7}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Masculino\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Feminino\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bernoulli_files/figure-pdf/unnamed-chunk-4-1.pdf}

Podemos então construir a hipótese \(H:\alpha_E<\beta_E\) e, para
testá-la, devemos ser capazes de calcular

\[P(\alpha_E<\beta_E|\hbox{dados})\] Vamos discutir como realizar testes
de hipóteses na próxima aula.

\end{example}

\bookmarksetup{startatroot}

\chapter{Testes de hipóteses}\label{testes-de-hipuxf3teses}

\section{Testes baseados na teoria da
decisão}\label{testes-baseados-na-teoria-da-decisuxe3o}

Considere que \(X_1,\ldots,X_n\) é uma amostra do modelo
\(F(.|\theta)\). Seja \(H_0:\theta\in\Theta_0\) a hipótese nula. Um
teste de hipóteses é uma regra \(\varphi(x)\) que recebe o valor 1 se a
hipótese \(H_0\) é aceita e 0 em caso contrário.

Considere a função de perda 0-1:

\[ \mathcal{L}(\theta,\varphi)=\left\{\begin{array}{l}0,\;\;\hbox{se }\varphi(x)=I(\theta\in\Theta_0)\\1,\hbox{  se }\varphi(x)\neq I(\theta\in\Theta_0)\end{array}\right.\]

A média a posteriori da função de perda é

\[E_{\theta|X}(\mathcal{L}(\theta,\varphi))=\int_{\Theta_0}I(\varphi(x)\neq 1)\pi(\theta|x)d\theta\]

e o estimador de Bayes é o valor de \(varphi\) que minimiza a esperança
acima. Como \(\varphi\) é uma indicadora, teremos que

\[E_{\theta|X}(\mathcal{L}(\theta,\varphi))=\left\{\begin{array}{l}P(\theta\in\Theta_0|x),\hbox{ se }\varphi\neq 1\\
P(\theta\in\Theta_0^c|x),\hbox{ se }\varphi= 1
\end{array}\right.\]

Deste modo, aceitamos \(H_0\) se
\(P(\theta\in\Theta_0|x)>P(\theta\in\Theta_0^c|x)\) e rejeitamos em caso
contrário.

Exemplo Uma cadeia de \emph{fast food} deseja saber se vale a pena
trocar seus freezers tradicionais, que mantém a carne entre -\(17^o\)C e
\(-9^oC\) por um com uma nova tecnologia (e mais cara!) que mantém a
temperatura consistentemente em \(-17^oC\). Para tomar essa decisão, 32
bifes foram armazenados por 8 meses, sendo 16 bifes colocados no freezer
tradicional e 16 no novo. Em seguida, um chefe preparou os 32 bifes de
maneira idêntica e 16 clientes foram escolhidos ao acaso para avaliar o
sabor dos bifes. Cada cliente recebeu um bife de cada freezer, mas a
prova foi realizada às cegas.

Podemos considerar a variável \(Y_i=1\) se o \(i\)-ésimo cliente
preferiu o bife armazenado no freezer mais caro e \(Y_i=0\) em caso
contrário. Deste modo,
\(Y_1,\ldots,Y_{16}|\theta\sim\hbox{Bernoulli}(\theta)\). Claramente,
estamos interessados em testar \(H_0:\theta>1/2\).

Considere as seguintes prioris:

\includegraphics{teste_files/figure-pdf/unnamed-chunk-1-1.pdf}

A priori de Jeffreys dá mais massa para valores extremos, o que poderia
favorecer a hipótese \(H_0\). A priori Uniforme(0,1) é aquela que parece
não dar qualquer preferência. Por último, a priori Beta(2,2) pode ser
vista como uma resitência à rejeitar que os dois armazenamentos são
iguais.

Dos 16 clientes, 13 preferiram os bifes que foram amarzenados com a
tecnologia mais cara. Como as três prioris acima são casos particulares
da distribuição Beta\((a,b)\), decidiremos sobre \(H_0\) calculando

\[P(\theta>1/2|\textbf{y})=\int_0^{1/2}\frac{\theta^{13+a-1}(1-\theta)^{3+b-1}}{B(13+a,3+b)}\]

que pode ser falciemente obtida com o comando
\texttt{pbeta(.5,13+a,3+b,\ lower.tail\ =F)} Temos os seguintes
resultados:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Priori & \(P(H_0|\textbf{y})\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Jeffreys & 0,995 \\
Uniforme & 0,993 \\
Beta(2,2) & 0,990 \\
\end{longtable}

Considerando as prioris acima, a probabilidade a posteriori da hipótese
nula é de pelo menos 0,99, o que nos leva a concluir que o sabor da
carne é melhor preservado no freezer com alta tecnologia

Até o momento, assumimos que a perda associada ao erro no teste de
hipóteses é igual para qualquer decisão. Considere então a função de
perda \(a-b\), dada por

\[\mathcal{L}(\theta,\varphi)=\left\{\begin{array}{l}0,\hbox{ se }\varphi(x)=I(\theta\in\Theta_0)\\
a,\hbox{ se }\varphi(x)=0\hbox{ e }\theta\in\Theta_0\\b,\hbox{ se }\varphi(x)=1\hbox{ e }\theta\notin\Theta_0\end{array}\right.\]

Utilizando a função de perda acima, temos que cada tipo de erro possui
um valor de perda diferente. A média a posteriori dessa função de perda
é

\[E_{\theta|x}(\mathcal{L}(\theta,\varphi))=a\int_{\Theta_0}I(\varphi(x)=0)\pi(\theta|x)d\theta+b\int_{\Theta_0^c}I(\varphi(x)=1)\pi(\theta|x)d\theta\]

Se \(\varphi(x)=1\), teremos
\[E_{\theta|x}(\mathcal{L}(\theta,\varphi))=b\int_{\Theta_0^c}I(\varphi(x)=1)\pi(\theta|x)d\theta=bP(\theta\in\Theta_0^c|x)\]
e se \(varphi(x)=0\) teremos
\[E_{\theta|x}(\mathcal{L}(\theta,\varphi))=a\int_{\Theta_0}I(\varphi(x)=0)\pi(\theta|x)d\theta=aP(\theta\in\Theta_0|x)\]
logo, o estimador de Bayes para a perda \(a-b\) é
\[\varphi(x)=\left\{\begin{array}{l}1,\hbox{ se }bP(\theta\in\Theta_0^c|x)<aP(\theta\in\Theta_0|x)\\
0,\hbox{ se }bP(\theta\in\Theta_0^c|x)\geq aP(\theta\in\Theta_0|x)\end{array}\right.\]
ou, equivalentemente,
\[\varphi(x)=\left\{\begin{array}{l}1,\hbox{ se }P(\theta\in\Theta_0|x)>\frac{b}{a+b}\\
0,\hbox{ caso contrário}\end{array}\right.\]

Utilizando a função de perda \(a-b\), \(H_0\) é rejeitada sempre que a
probabilidade a posteriori é menor que \(b/(a+b)\).

\bookmarksetup{startatroot}

\chapter{O Fator de Bayes}\label{o-fator-de-bayes}

Considere novamente o problema dos freezers, da seção anterior. Note que
a teoria desenvolvida acima não consegue testar \(H_0:\theta=1/2\), uma
vez que este evento possui probabilidade nula a priori. Para contornar
este problema, suponha que existem dois modelos competidores:

\begin{itemize}
\item
  \(M_0: y_i\sim\hbox{Bernoulli}(1/2)\)
\item
  \(M_1: y_i|\theta\hbox{Bernoulli}(\theta), \theta\sim\hbox{Uniforme}(0,1)\)
\end{itemize}

Então, \[f(y_1,\ldots,y_{16}|M_0)=\left(\frac{1}{2}\right)^{16}\]

e
\[f(y_1,\ldots,y_{^16}|M_1)=\int_0^1\theta^{\sum_{i=1}^{16}y_i}(1-\theta)^{16-\sum_{i=1}^{16}y_i}d\theta=B\left(\sum_{i=1}^{16}y_i+1,16-\sum_{i=1}^{16}y_i+1\right)\]
Colocando uma probabilidade a priori para cada modelo, teremos

\[P(M_0|y_1,\ldots,y_{16})=\frac{\left(\frac{1}{2}\right)^{16}P(M_0)}{\left(\frac{1}{2}\right)^{16}P(M_0)+B\left(\sum_{i=1}^{16}y_i+1,16-\sum_{i=1}^{16}y_i+1\right)P(M_1)}\]
Ainda considerando o exemplo anterior, assumindo \(P(M_0)=P(M_1)\) e
lembrando que \(\sum_i y_i=13\), tem-se
\(P(M_0|y_1,\ldots,y_{16})=0,12\), o que nos leva a rejeitar \(H_0\).

Note que esta nova abordagem leva a uma mudança na priori, uma vez que

\[\pi(\theta)=P(M_0)\pi(\theta|M_0)+P(M_1)\pi(\theta|M_1)=P(M_0)I(\theta=\theta_0)+I(\theta\in (0,1))P(M_1)\]

Para o caso geral, considere os modelos competidores \(M_0\) e \(M_1\).
Sejam ainda \(\pi_0(.)\) e \(\pi_1(.)\) as prioris sob \(H_0\) e
\(H_1\), repectivamente. O Fator de Bayes é definido por

\[B_{01}(x)=\left.\frac{P(M_0|x)}{P(M_0)}\right/ \frac{P(M_1|x)}{P(M_1)}\]
onde \[P(M_i|x)=\int f(x|\theta)\pi_i(\theta)d\theta.\]

Se \(B_{01}(x)=1\), então não há diferença entre os modelos.
Alternativamente, quanto maior for o valor do fator de Bayes, maior é a
evidência a favor de \(H_0\). A escala de Jeffreys pode ser útil para
tomada de decisão:

Note que, se \(\pi_0(\theta)=I(\theta=\theta_0)\),
\(\pi_1(\theta)=I(\theta=\theta_1)\) e \(P(M_0)=P(M_1)\), o fator de
Bayes se torna a estatística do teste de Neyman-Pearson

\[B_{01}(x)=\frac{f(x|\theta_0)}{f(x|\theta_1)}\]

\bookmarksetup{startatroot}

\chapter{O modelo normal}\label{o-modelo-normal}

\section{A distribuição
normal-gama}\label{a-distribuiuxe7uxe3o-normal-gama}

Dizemos que \((X,Y)\sim NG(\mu,n_0,\nu,d^2)\) (lê-se normal-gama) se sua
função densidade conjunta é dada por

\[f(x,y)\propto y^{\frac{\nu+1}{2}-1}\exp\left\{-\frac{y}{2}n_0\left[(x-\mu)^2 + d^2\right]\right\}\]

onde \(\mu,x\in\mathbb{R}\) e \(d,y,c\in\mathbb{R}_+\). Colocando os
termos que não dependem de \(x\) junto com a constante de
proporcionalidade, podemos mostrar que

\[f(x|y)\propto \exp\left\{-\frac{y}{2}n_0(x-\mu)^2\right\}\] ou seja,
\(X|y\sim\hbox{Normal}(\mu,y^{-1}/n_0)\). Além disso, integrando
\(f(x,y)\) em \(x\), mostramos que

\[f(y)\propto y^{\frac{\nu+1}{2}-1}e^{-\frac{yn_0d^2}{2}}\int_{\mathbb{R}}\exp\left\{-\frac{y}{2}\left[n_0(x-\mu)^2\right]\right\}d\mu\propto y^{\frac{\nu}{2}-1}e^{-\frac{n_0d^2}{2}y}\]
ou seja, \(Y\sim\hbox{Gama}(\nu/2, n_0d^2/2)\). Por último, integrando
\(f(x,y)\) em \(y\) teremos

\[\begin{align}f(x)&\propto \int_0^\infty y^{\frac{\nu+1}{2}-1}\exp\left\{-\frac{y}{2}n_0\left[(x-\mu)^2 + d^2\right]\right\}dy \\&\propto \Gamma\left(\frac{\nu+1}{2}\right)\left\{1+\frac{\nu}{d^2}\frac{(x-\mu)^2}{\nu}\right\}^{-\frac{\nu+1}{2}}\end{align}\]
ou seja, \(X\sim t_{\nu}(\mu, d^2/\nu)\). Em especial, se \(\nu>1\)
então \(E(X)=\mu\) e, se \(\nu>2\) teremos que
\[Var(X)=\frac{d^2}{\nu-2}\]

\section{A função de
verossimilhança}\label{a-funuxe7uxe3o-de-verossimilhanuxe7a-1}

Seja \(X_1,\ldots,X_n\) uma amostra aleatória do modelo
\(X|\mu,\phi\sim\hbox{Normal}(\mu,\phi^{-1})\), onde \(\phi\),
denominado precisão, é o inverso da variância. A função de
verossimilhança deste modelo pode ser escrita como

\[L(\mu,\phi)\propto \phi^{\frac{n}{2}}\exp\left\{-\frac{n\phi}{2}(\bar{x}-\mu)^2 -\frac{ns^2\phi}{2}\right\}\]
onde \[s^2=\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2\] é a estimativa de
máxima verossimilhança para \(\phi^{-1}\).

\section{Posteriori com prioris
impróprias}\label{posteriori-com-prioris-impruxf3prias}

Considerando as prioris impróprias \(\pi(\phi)\propto \phi^{-1}\),
\(\pi(\mu)\propto 1\) e que \(\pi(\mu,\phi)=\pi(\mu)\pi(\phi)\), teremos
que

\[\pi(\mu,\phi|\boldsymbol{x})\propto \phi^{\frac{n}{2}-1}\left\{-\frac{\phi}{2}n\left[ (\bar{x}-\mu)^2 +s^2\right]\right\}\]
ou seja, \(\mu,\phi|\boldsymbol{x}\sim\hbox{NG}(\bar{x},n,n-1,s^2)\), o
que implica em:

\[\begin{align}
\mu|\phi,\boldsymbol{x}&\sim\hbox{Normal}\left(\bar{x},\frac{\phi^{-1}}{n}\right)\\
\phi|\boldsymbol{x}&\sim\hbox{Gama}\left(\frac{n-1}{2},\frac{ns^2}{2}\right)\\
\mu|\boldsymbol{x}&\sim t_{n-1}\left(\bar{x},\frac{s^2}{n-1}\right)
\end{align}\]

Disto, teremos que

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Parâmetro & Estimativa & Erro \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\mu\) & \(\bar{x}\) & \(\frac{s}{\sqrt{n-3}}\) \\
\(\phi\) & \(\frac{n-1}{ns^2}\) & \(\frac{\sqrt{2(n-1)}}{s^2n}\) \\
\end{longtable}

\section{Posteriori com a priori de
Jeffreys}\label{posteriori-com-a-priori-de-jeffreys}

O logaritmo da função de verossimilhança é

\[l(\mu,\phi)=\frac{n}{2}\log\phi -\frac{n}{2}\phi\left[(\bar{x}-\mu)^2 + s^2\right]\]

As derivadas de primeira ordem em \(\mu\) e \(\phi\) são \[\begin{align}
\frac{\partial}{\partial \mu}l(\mu,\phi)&=n\phi(\bar{x}-\mu)\\
\frac{\partial}{\partial \phi}l(\mu,\phi)&=\frac{n}{2\phi}-\frac{n}{2}\left[(\bar{x}-\mu)^2 + s^2\right]\\
\end{align}\]

e as de segunda ordem são \[\begin{align}
\frac{\partial^2}{\partial \mu^2}l(\mu,\phi)&=-n\phi\\
\frac{\partial^2}{\partial \phi^2}l(\mu,\phi)&=-\frac{n}{2\phi^2}\\
\frac{\partial^2}{\partial \mu\partial \phi}l(\mu,\phi)&=0\\
\end{align}
\] logo, a matriz de informação de Fisher é
\[\mathcal{I}_n(\mu,\phi)=n\left[\begin{array}{cc}\phi & 0 \\0 & \frac{1}{2\phi^2}\end{array}\right],\]
e a priori de Jeffreys é
\[\pi(\mu,\phi)\propto \sqrt{|\mathcal{I}_n(\mu,\phi)|}=\phi^{-1/2},\]
que implica na posteriori

\[\pi(\mu,\phi|\boldsymbol{x})\propto \phi^{\frac{n+1}{2}-1}\left\{-\frac{n\phi}{2}\left[(\bar{x}-\mu)^2 +s^2 \right]\right\}\]
ou seja, \(\mu,\phi|\boldsymbol{x}\sim\hbox{NG}(\bar{x},n,n,s^2)\), o
que implica em:

\[\begin{align}
\mu|\phi,\boldsymbol{x}&\sim\hbox{Normal}\left(\bar{x},\frac{\phi^{-1}}{n}\right)\\
\phi|\boldsymbol{x}&\sim\hbox{Gama}\left(\frac{n}{2},\frac{ns^2}{2}\right)\\
\mu|\boldsymbol{x}&\sim t_{n}\left(\bar{x},\frac{s^2}{n}\right)
\end{align}\]

Disto, teremos que

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Parâmetro & Estimativa & Erro \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\mu\) & \(\bar{x}\) & \(\frac{s}{\sqrt{n-2}}\) \\
\(\phi\) & \(\frac{1}{s^{2}}\) & \(\frac{\sqrt{2}}{s^2\sqrt{n}}\) \\
\end{longtable}

\section{Posteriori para a priori
conjugada}\label{posteriori-para-a-priori-conjugada}

Considere que \((\mu,\phi)\sim \hbox{NG}(m_0,n_0,\nu_0,s^2_0)\). Esta
priori é conjugada para o modelo normal, uma vez que

\[\begin{align}
\pi(\mu,\phi|\boldsymbol{x})&\propto \phi^{\frac{n}{2}}\exp\left\{-\frac{\phi}{2}n\left[(\bar{x}-\mu)^2+ s^2\right]\right\}\phi^{\frac{\nu_0+1}{2}-1}\exp\left\{-\frac{\phi}{2}n_0\left[(\mu-m_0)^2 + s_0^2\right]\right\}\\
&\phi^{\frac{\nu_0+n}{2}-1}\exp\left\{-\frac{\phi}{2}\left[n(\bar{x}-\mu)^2 + n_0(\mu-m_0)^2+ns^2 + n_0s^2_0\right]\right\}\end{align}.\]
Como
\[n(\bar{x}-\mu)^2 +n_0(\mu-m_0)^2 = (n+n_0)(\mu-m_1)^2+\frac{n n_0}{n+n_0}(\bar{x}-m_0)^2\]
onde \[\begin{align}
m_1&=\frac{n}{n+n_0}\bar{x}+\frac{n_0}{n+n_0}m_0
\end{align},\] teremos \[\begin{align}
\pi(\mu,\phi|\boldsymbol{x})&\propto \phi^{\frac{\nu_1+1}{2}-1}\exp\left\{-\frac{\phi}{2}n_1\left[(\mu-m_1)^2 + d_1^2\right]\right\}\end{align},\]
onde \[\begin{align}
\nu_1&=\nu_0+n\\
n_1&=n_0+n\\
m_1&=\frac{n}{n1}\bar{x}+\frac{n_0}{n_1}m_0\\
d_1^2& = \frac{n_0n}{n_1^2}(\bar{x}-m_0)^2+\frac{n}{n_1}s^2 + \frac{n_0}{n_1}s^2_0
\end{align}\] ou seja,
\(\mu,\phi|\boldsymbol{x}\sim\hbox{NG}(m_1,n+n_0,\nu_0+n,d_1^2)\), o que
implica em:

\[\begin{align}
\mu|\phi,\boldsymbol{x}&\sim\hbox{Normal}\left(m_1,\frac{\phi^{-1}}{n+n_0}\right)\\
\phi|\boldsymbol{x}&\sim\hbox{Gama}\left(\frac{n+\nu_0}{2},\frac{(n+n_0)d_1^2}{2}\right)\\
\mu|\boldsymbol{x}&\sim t_{n}\left(m_1,\frac{d^2_1}{\nu_0+n}\right)
\end{align}\]

Disto, teremos que

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3472}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4028}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Parâmetro
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimativa
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Erro
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\mu\) & \(\frac{n}{n+n_0}\bar{x}+\frac{n_0}{n+n_0}m_0\) &
\(\frac{d_1}{\sqrt{n+\nu_0-2}}\) \\
\(\phi\) & \(\frac{n+\nu_0}{(n+n_0)d_1^2}\) &
\(\frac{\sqrt{2(n+\nu_0)}}{d_1^2(n+n_0)}\) \\
\end{longtable}

\section{Detecção de outliers}\label{detecuxe7uxe3o-de-outliers}

\bookmarksetup{startatroot}

\chapter{Binomial negativa}\label{binomial-negativa}

\section{O modelo binomial negativo}\label{o-modelo-binomial-negativo}

A distribuição Poisson é muito comum em problemas de contagem. Como sua
esperança e variância são iguais, o termo sobredispersão foi cunhado na
literatura como uma variância maior que a média, o que seria indício de
que o modelo Poisson não é adequado (de modo análogo, há o conceito de
subdispersão, mas não é um fenômeno comum).

Dizemos que \(X|\rho,\phi\sim\hbox{Binomial Negativa}\) se

\[p(x|\rho,\phi)=\frac{\Gamma(\phi+x)}{x!\Gamma(\phi)}\rho^\phi(1-\rho)^x,\]
onde \(x\in\mathbb{N}\), \(\rho\in(0,1)\) e \(\phi>0\).

Existem diversos motivos para considerar o modelo binomial negativo uma
alternativa quando o modelo Poisson não parece ser adequado. Primeiro,
temos que \(E(X|\rho,\phi)=\phi(1-\rho)/\rho\) e
\(Var(X|\rho,\phi)=E(X|\rho,\phi)/\rho\), logo, a sobredispersão está
presente no modelo. Além disso, se
\(X|\lambda\sim\hbox{Poisson}(\lambda)\) e
\(\lambda\sim\hbox{Gama}(\phi, \rho/(1-\rho))\), então
\(X|\phi,\rho\sim\hbox{Binomial Negativa}(\phi,\rho)\) logo, este modelo
é uma mistura do modelo Poisson. Por último, fazendo
\[\mu=\phi\frac{1-\rho}{\rho}\Rightarrow \rho(\phi)=\frac{\phi}{\phi+\mu},\]
pode-de mostrar que
\[\lim_{\phi\rightarrow\infty}p(x|\phi)=\frac{e^{-\mu}\mu^x}{x!}\] ou
seja, o modelo Poisson também pode ser vist como um caso limite do
binomial negativo.

\section{\texorpdfstring{Priori para \(\phi\)
condicionado}{Priori para \textbackslash phi condicionado}}\label{priori-para-phi-condicionado}

Quando \(\phi\) é conhecido, a verossimilhança do modelo se torna

\[L(\rho|\phi)\propto \rho^{n\phi}(1-\rho)^{\sum_{i=1}^n x_i},\] logo, o
modelo Beta\((a,b)\) é conjugado, com a posteriori dada por
\[\rho|\boldsymbol{x},\phi\sim\hbox{Beta}\left(n\phi+a,\sum_{i=1}^n x_i+b\right).\]

A priori de Jeffreys é dada por

\[\pi(\rho)\propto \frac{1}{\rho(1-\rho)^{1/2}},\] o que implica na
posteriori
\[\rho|\boldsymbol{x},\phi\sim\hbox{Beta}\left(n\phi,\sum_{i=1}^n x_i+\frac{1}{2}\right).\]

\section{\texorpdfstring{Priori para
\(\phi\)}{Priori para \textbackslash phi}}\label{priori-para-phi}

Seja \(\pi(\phi)\pi(\rho)\) a priori para \((\phi,\rho)\). Então,
teremos que

\[\pi(\phi,\rho|\boldsymbol{x})\propto \frac{\prod_{i=1}^n\Gamma(\phi+x_i)}{\Gamma(\phi)^n}\rho^{n\phi}(1-\rho)^{\sum_{i=1}^n x_i}\pi(\phi)\pi(\rho).\]

Assumindo qualquer uma das prioris da seção anterior, teremos

\[\pi(\phi,\rho|\boldsymbol{x})\propto \frac{\prod_{i=1}^n\Gamma(\phi+x_i)}{\Gamma(\phi)^n}B\left(a_0+n\phi,b_0+\sum_{i=1}^nx_i\right)\pi(\phi)\pi(\rho|\phi,\boldsymbol{x}),\]

logo,

\[\pi(\phi|\boldsymbol{x})\propto \frac{\prod_{i=1}^n\Gamma(\phi+x_i)}{\Gamma(\phi)^n}B\left(a_0+n\phi,b_0+\sum_{i=1}^nx_i\right)\pi(\phi)\]

Como a posteriori de \(\phi\) não é uma distribuição conhecida,
precisamos construir um simulador. O algoritmo Metropolis-Hastings é uma
boa escolha, uma vez que a constante de proporcionalidade da densidade é
desconhecida.

Algoritmo Metropolis-Hastings

O Metropolis-Hastings simula se utiliza de uma distribuição que sabemos
simular (denominada proposta) para gerar uma cadeia de Markov cuja
distribuição estacionária é a distribuição de interesse.

Na \(j\)-ésima itereção, a simulação do valor proposto \(\phi^*\) é
baseada no valor atual da cadeia, \(\phi^{(j-1)}\). Como \(\phi>0\), a
proposta \(\phi^*\sim \hbox{Gamma}(\tau\phi^{(j-1)},\tau)\) é adequada
uma vez que \[E(\phi^*)=\phi^{(j-1)}\] e
\[\sqrt{Var(\phi^*)}=\frac{\phi^{(j-1)}}{\tau}\] Acima, \(\tau\) é
denominado \emph{tunning} (afinação em tradução livre) e deve ser
escolhido para que a cadeia tenha o número de aceites da proposta
controlado (algo em torno de 23\% ).

Abaixo, segue o algoritmo

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Faça \(j=0\) e escolha um valor para \(\phi^{(0)}\) (a estimativa de
  máxima verossimilhança, por exemplo). Faça um contador de aceites,
  começando com \(k=0\).
\item
  Para o passo \(j\):
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Simule \(\phi^*\sim\hbox{Gama}(\tau\phi^{j-1},\tau)\)
\item
  Calcule
\end{itemize}

\[prob = \frac{\pi(\phi^*|\boldsymbol{x})}{\pi(\phi^{(j-1)}|\boldsymbol{x})}\frac{g(\phi^{(j-1)}|\tau\phi^*,\tau)}{g(\phi^*|\tau\phi^{(j-1)},\tau)},\]
onde \(g(.|a,b)\) é a função densidade do modelo gama. + Simule
\(u\sim\hbox{Uniforme}(0,1)\). Se \(u<prob\), faça \(\phi^{(j)}=\phi^*\)
e \(k=k+1\) (houve um aceite). Senão, faça \(\phi^{(j)}=\phi^{(j-1)}\).

\bookmarksetup{startatroot}

\chapter{Aproximação normal e seu uso com o
Metropolis-Hastings}\label{aproximauxe7uxe3o-normal-e-seu-uso-com-o-metropolis-hastings}

\section{Aproximação da posteriori pela distribuição
normal}\label{aproximauxe7uxe3o-da-posteriori-pela-distribuiuxe7uxe3o-normal}

Assuma que \(\boldsymbol{\theta}\in\mathbb{R}^q\). Seja
\(\ell(\boldsymbol{\theta})=\log L(\boldsymbol{\theta})\) a função
log-verossimilhança e \(\hat{\boldsymbol{\theta}}\) a estimativa de
máxima verossimilhaça para \(\boldsymbol{\theta}\). Considere a seguinte
aproximação de \(\ell(\boldsymbol{\theta})\) em séries de Taylor

\[\ell(\boldsymbol{\theta})\approx  \ell(\hat{\boldsymbol{\theta}})+\frac{1}{2}(\boldsymbol{\theta}-\hat{\boldsymbol{\theta}})'\mathcal{H}(\hat{\boldsymbol{\theta}})(\boldsymbol{\theta}-\hat{\boldsymbol{\theta}})\]
onde \(\boldsymbol{\theta}\) é a matriz hessiana (de derivadas segunda)
aplicada em \(\hat{\boldsymbol{\theta}}\). Deste modo, teremos que
\[\pi(\boldsymbol{\theta}|\boldsymbol{x})\propto \exp\left\{-\frac{1}{2}(\boldsymbol{\theta}-\hat{\boldsymbol{\theta}})'\left[-\mathcal{H}(\hat{\boldsymbol{\theta}})\right](\boldsymbol{\theta}-\hat{\boldsymbol{\theta}})\right\}\pi(\boldsymbol{\theta})\]

Utilizando a priori imprópria \(\pi(\boldsymbol{\theta})\), temos que
\(\boldsymbol{\theta}|\boldsymbol{x}\approx \hbox{Normal}(\hat{\boldsymbol{\theta}},-\mathcal{H}(\hat{\boldsymbol{\theta}})^{-1})\).

Note que as informações necessárias para a aproximação da posteriori
acima podem ser obtidas via função \texttt{optim}.

Exemplo A amostra abaixo foi simulada do modelo Gama\((\alpha,\beta)\)
(o valor dos parâmetros foram omitidos de propósito)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 0.2769550 1.1902521 1.1543901 0.6836040 1.2951363 0.8468467 0.7626888
 [8] 0.3830976 0.2270072 0.2785412 0.3853067 0.4818242 0.2021683 0.8914625
[15] 0.7718524 0.9455476 0.8702839 0.5309044 1.2858882 1.0415047
\end{verbatim}

Como \(\alpha,\beta>0\), considere que \(\alpha=\exp\{\theta_1\}\) e
\(\beta=\exp\{\theta_2\}\) (deste modo,
\(\boldsymbol{\theta}\in\mathbb{R}^2\)).

A função de log-verossimilhança deste modelo é

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logveross }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta)\{ }\FunctionTok{sum}\NormalTok{(}\FunctionTok{dgamma}\NormalTok{(x, }\FunctionTok{exp}\NormalTok{(theta[}\DecValTok{1}\NormalTok{]), }\FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{]), }\AttributeTok{log =}\NormalTok{ T))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Podemos utilizar a função \texttt{optim} para obter as estimativas de
máxima verossimilhança e a matriz hessiana. Contudo, primeiro devemos
observar que esta função é um minimizador, logo, queremos que
\(\boldsymbol{\theta}\) que minimize \(-\ell({\boldsymbol{\theta}})\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{( }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(q) }\SpecialCharTok{{-}}\FunctionTok{logveross}\NormalTok{(q), }\AttributeTok{hessian =}\NormalTok{ T)}
\NormalTok{opt}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$par
[1] 1.245897 1.567152

$value
[1] 7.435047

$counts
function gradient 
      65       NA 

$convergence
[1] 0

$message
NULL

$hessian
          [,1]      [,2]
[1,]  80.46195 -69.52104
[2,] -69.52104  69.52342
\end{verbatim}

No objeto \texttt{opt}, a lista \texttt{par} é o vetor com as
estimativas de máxima verossimilhança, enquanto que \texttt{hessian} é o
valor de \(-\mathcal{H}(\hat{\boldsymbol{\theta}})\).

A inversa de \texttt{opt\$hessian} vai dar a matriz de covariância entre
\(\theta_1\) e \(\theta_2\) a posteriori.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Sigma }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(opt}\SpecialCharTok{$}\NormalTok{hessian)}
\NormalTok{Sigma}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
           [,1]       [,2]
[1,] 0.09138023 0.09137711
[2,] 0.09137711 0.10575762
\end{verbatim}

Agora, podemos simular \(\theta_1\) e \(\theta_2\) a posteriori:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{require}\NormalTok{(mvtnorm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Carregando pacotes exigidos: mvtnorm
\end{verbatim}

\begin{verbatim}
Warning: pacote 'mvtnorm' foi compilado no R versão 4.4.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta\_sim }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(}\DecValTok{500}\NormalTok{, opt}\SpecialCharTok{$}\NormalTok{par, Sigma)}
\end{Highlighting}
\end{Shaded}

Por último, podemos fazer inferências sobre \(\alpha=\exp\{\theta_1\}\)
e \(\beta=\exp\{\theta_2\}\):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# intervalos de credibilidade para alfa}
\FunctionTok{quantile}\NormalTok{(}\FunctionTok{exp}\NormalTok{(theta\_sim[,}\DecValTok{1}\NormalTok{]), }\FunctionTok{c}\NormalTok{(.}\DecValTok{025}\NormalTok{,.}\DecValTok{975}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    2.5%    97.5% 
1.978850 6.350455 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# intervalos de credibilidade para beta}
\FunctionTok{quantile}\NormalTok{(}\FunctionTok{exp}\NormalTok{(theta\_sim[,}\DecValTok{2}\NormalTok{]), }\FunctionTok{c}\NormalTok{(.}\DecValTok{025}\NormalTok{,.}\DecValTok{975}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    2.5%    97.5% 
2.582863 8.865732 
\end{verbatim}

\section{Metropolis revisitado}\label{metropolis-revisitado}

A diferença entre o algoritmo Metropolis e o Metropolis-Hastings está na
escolha da distribuição proposta. No primeiro, a proposta é simétrica,
\[g(x|y)=g(y|x).\] Com isso, teremos que
\[\frac{f(x)}{f(y)}\frac{g(y|x)}{g(x|y)}=\frac{f(x)}{f(y)}\] e a
probabilidade de aceitação da cadeia é baseada somente na distribuição
alvo \(f\).

No algoritmo Metropolis, é comum escolher a distribuição proposta como
sendo uma normal. Uma escolha razoável é utilizar como proposta
aproximação normal vista na seção anterior.

Exemplo Consideremos novamente a amostra do exemplo anterior. A função
de verossimilhança é
\[L(\theta)=\prod_{i=1}^n \frac{\beta(\theta_2)^{\alpha(\theta_1)}}{\Gamma(\alpha(\theta_1))} x_i^{\alpha(\theta_1)-1}e^{-\beta(\theta_2)x_i}\]
onde \(\alpha(\theta_1)=e^{\theta_1}\),
\(\beta(\theta_2)=e^{\theta_2}\). Além disso ,considere ad prioris
independentes \(\theta_i\sim\hbox{Normal}(0,100)\). Então, devemos
simular do modelo

\[\pi(\theta|\boldsymbol{x})\propto \left[\frac{\beta(\theta_2)^{\alpha(\theta_1)}}{\Gamma(\alpha(\theta_1))}\right]^n \left[\prod_{i=1}^n x_i\right]^{\alpha(\theta_1)}e^{-\beta(\theta_2)\sum_{i=1}^{n}x_i}e^{-\frac{1}{200}(\theta_1^2 + \theta_2^2)}\]

A posteriori aproximada, que encontramos no exemplo anterior é
\[\boldsymbol{\theta}|\boldsymbol{x}\approx N \left[ \left(\begin{array}{c}1,24\\1,56 \end{array}\right),\left(\begin{array}{cc}0,09 & 0,09\\0,09 &0,11\end{array}\right)\right]\]

Vamos aproveitar a estrutura de covariâncias acima para usar a proposta

\[\boldsymbol{\theta}^*|\boldsymbol{x}\sim N \left[ \boldsymbol{\theta}^{(j-1)},\tau\left(\begin{array}{cc}0,09 & 0,09\\0,09 &0,11\end{array}\right)\right]\]
onde \(\boldsymbol{\theta}^*\) é o candidato gerado e
\(\boldsymbol{\theta}^{(j)}\) é o estado atual da cadeia e \(\tau\) é o
\textbf{tunning} da cadeia.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{10000} \CommentTok{\# número de iterações}
\NormalTok{theta }\OtherTok{\textless{}{-}} \FunctionTok{array}\NormalTok{(}\ConstantTok{NA\_real\_}\NormalTok{, }\FunctionTok{c}\NormalTok{(B,}\DecValTok{2}\NormalTok{))}

\NormalTok{theta[}\DecValTok{1}\NormalTok{,] }\OtherTok{\textless{}{-}}\NormalTok{ opt}\SpecialCharTok{$}\NormalTok{par }\CommentTok{\# valor inicial da cadeia é a emv}
\NormalTok{tau }\OtherTok{\textless{}{-}} \DecValTok{1}             \CommentTok{\# tunning}
\NormalTok{cont }\OtherTok{\textless{}{-}} \DecValTok{0}            \CommentTok{\# contador de aceites}

\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{B)\{}
  \CommentTok{\#simule um candidato}
\NormalTok{  theta\_cand }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, theta[j}\DecValTok{{-}1}\NormalTok{,], tau}\SpecialCharTok{*}\NormalTok{Sigma)}
  
  \CommentTok{\# calcule a probabilidade do salto}
\NormalTok{  lnum }\OtherTok{\textless{}{-}} \FunctionTok{logveross}\NormalTok{(theta\_cand) }\SpecialCharTok{+}
    \FunctionTok{sum}\NormalTok{(}\FunctionTok{dnorm}\NormalTok{(theta\_cand[}\DecValTok{1}\NormalTok{,],}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{, }\AttributeTok{log =}\NormalTok{ T))}
  
\NormalTok{  lden }\OtherTok{\textless{}{-}} \FunctionTok{logveross}\NormalTok{(theta[j}\DecValTok{{-}1}\NormalTok{,]) }\SpecialCharTok{+}
    \FunctionTok{sum}\NormalTok{(}\FunctionTok{dnorm}\NormalTok{(theta[j}\DecValTok{{-}1}\NormalTok{,],}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{, }\AttributeTok{log =}\NormalTok{ T))}
  
\NormalTok{  prob }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{( lnum }\SpecialCharTok{{-}}\NormalTok{ lden)}
  
  \CommentTok{\# verifique o salto}
\NormalTok{  u }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{)}
  \ControlFlowTok{if}\NormalTok{( u }\SpecialCharTok{\textless{}}\NormalTok{ prob)\{}
\NormalTok{    theta[j, ] }\OtherTok{\textless{}{-}}\NormalTok{ theta\_cand}
\NormalTok{    cont }\OtherTok{\textless{}{-}}\NormalTok{ cont}\SpecialCharTok{+}\DecValTok{1}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{    theta[j,] }\OtherTok{\textless{}{-}}\NormalTok{ theta[j}\DecValTok{{-}1}\NormalTok{,]}
\NormalTok{  \}}
\NormalTok{\}}

\NormalTok{cont}\SpecialCharTok{/}\NormalTok{B}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5628
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta\_sim }\OtherTok{\textless{}{-}}\NormalTok{ theta[ }\FunctionTok{seq}\NormalTok{(B}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, B, }\DecValTok{15}\NormalTok{),]}
\FunctionTok{acf}\NormalTok{(theta\_sim)}
\end{Highlighting}
\end{Shaded}

\includegraphics{aproximacaoNormal_files/figure-pdf/unnamed-chunk-8-1.pdf}

Por fim, as estimativas intervalares para \((\alpha,\beta)\) são

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(}\FunctionTok{exp}\NormalTok{(theta\_sim[,}\DecValTok{1}\NormalTok{]), }\FunctionTok{c}\NormalTok{(.}\DecValTok{025}\NormalTok{,.}\DecValTok{975}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    2.5%    97.5% 
1.699256 5.644593 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# intervalos de credibilidade para beta}
\FunctionTok{quantile}\NormalTok{(}\FunctionTok{exp}\NormalTok{(theta\_sim[,}\DecValTok{2}\NormalTok{]), }\FunctionTok{c}\NormalTok{(.}\DecValTok{025}\NormalTok{,.}\DecValTok{975}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    2.5%    97.5% 
2.087772 7.675778 
\end{verbatim}

\bookmarksetup{startatroot}

\chapter{Misturas de
distribuições}\label{misturas-de-distribuiuxe7uxf5es}

Dizemos que \(X\sim f(.)\) é uma mistura se existe uma variável \(Z\)
tal que

\[f(x)=\int f(x|z)f(z)dz.\]

A variável \(Z\) é denominada latente e a função \(f(x,z)\) é denominada
modelo aumentado.

Seja \((X_1,Z_1),\ldots,(X_n,Y_n)\) uma amostra aleatória do modelo
aumentado \(f(x,z|\theta)\) e seja \(\pi(\theta)\) a priori para
\(\theta\). Existem situações nas quais é mais fácil simular do modelo
\[\pi(\theta,\boldsymbol{z}|\boldsymbol{x})\varpropto f(\boldsymbol{x},\boldsymbol{z}|\theta)\pi(\theta).\]

A distribuição de \(\theta\) (ou \(\boldsymbol{Z}\)) dado as demais
variáveis do modelo é denominada condicional completa, que neste
problema são:

\[f(\boldsymbol{z}|\boldsymbol{x},\theta)\] e
\[\pi(\theta|\boldsymbol{z},\boldsymbol{x},\theta)\]

Em particular, se é fácil simular das condicionais completas, podemos
utilizar o Amostrador de Gibbs, que consiste no seguinte algoritmo:

Amostrador de Gibbs

Inicie a cadeia com fazendo \(j=0\) e escolhendo \(\theta^{(0)}\)

No \(j\)-ésimo passo:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Simule
  \(\boldsymbol{z^{(j)}}\sim f(\boldsymbol{z}|\boldsymbol{x},\theta^{(j-1)})\)
\item
  Simule \(\theta^{(j)}\sim \pi(\theta|\boldsymbol{z},\boldsymbol{x})\)
\end{enumerate}

O Amostrador de Gibbs é uma cadeia de Markov cuja distribuição
estacionária é \(\pi(\theta,\boldsymbol{z}|\boldsymbol{x})\)

\section{Modelos com inflação de
zeros}\label{modelos-com-inflauxe7uxe3o-de-zeros}

Quando são observados mais zeros do que o esperado pelo modelo de
contagem assumido para a verossimilhança, é usual considerar um modelo
com inflação de zeros. Nesse tipo de modelo, assumimos que existe uma
variável \(Z|p\sim\hbox{Bernoulli}(\rho)\) tal que:

\[X=\left\{\begin{array}{ll}0, & \hbox{se }Z=1\ \\ Y,&\hbox{se }Z=0\end{array}\right.\]
onde \(Y\sim h(.|\theta)\) é o modelo de contagem. Apenas \(X\) é
observado e, como

\[\begin{align}P(X=0|\theta,p)&=P(X=0|Z=0,\theta)P(Z=0|\rho)+P(X=0|Z=1,\theta)P(Z=1|\rho)\\&=(1-\rho)h(0|\theta)+\rho\end{align}\]
a probabilidade de observar um zero está entre \(h(0|\theta)\) e 1, o
que caracteriza a inflação.

Agora, considere um modelo inflacionado de zeros aumentado:

\[f(x,z|\theta,\rho)=f(x|z,\theta)f(z|\rho)=f(x|z,\theta)\rho^z(1-\rho)^{1-z}.\]
Note que

\[f(x|z,\theta)=\left\{
\begin{array}{ll}
h(x|\theta),&\hbox{ se }z=0,\\
I(x=0),&\hbox{ se }z=1\\
\end{array}\right.\] logo, a distribuição conjunta
\(f(x,z|\theta,\rho)\) é dada por

\[\begin{array}{c|cc}\hline & x=0 & \hbox{qualquer }x> 0 \\ \hline
z=0 & h(0|\theta)(1-\rho) & h(x|\theta)(1-\rho) \\
z=1 & \rho & 0 \\ \hline
\end{array}
\] Então,

\[\begin{align}
\prod_{i=1}^n f(x_i,z_i|\theta,\rho)&=\prod_{i=1}^n [h(0|\theta)(1-\rho)]^{I(x_i=0,z_i=0)}[h(x_i|\theta)(1-\rho)]^{I(x_i>0,z_i=0)}\rho^{I(x_i=0,z_i=1)}\\
&=\prod_{i=1}^n [h(x_i|\theta)(1-\rho)]^{I(z_i=0)}\rho^{I(x_i=0,z_i=1)}\\
&=\prod_{i=1}^n(1-\rho)^{I(z_i=0)}\rho^{I(x_i=0,z_i=1)}\prod_{i=1}^n [h(x_i|\theta)]^{I(z_i=0)}\end{align}\]
e, notando que \(I(z_i=0)=1-z_i,\)

\[\begin{align}
\prod_{i=1}^n f(x_i,z_i|\theta,\rho)&=
(1-\rho)^{n-\sum_{i=1}^n z_i}\rho^{\sum_{i=1}^n z_iI(x_i=0)}\prod_{i=1}^n [h(x_i|\theta)]^{1-z_i}\end{align}\]

Considere, a priori, que \(\theta\) e \(\rho\) são independentes. Seja
\(\pi(\theta)\) a priori para \(\theta\) e considere que
\(\rho\sim\hbox{Beta}(a,b)\). Então, as condicionais completas para
\(\theta\) e \(\rho\) são

\[\begin{align}
\pi(\theta|\rho,\boldsymbol{z},\boldsymbol{x})&\propto \prod_{i=1}^n h(x_i|\theta)^{1-z_i}\pi(\theta),\\
\pi(\rho|\theta,\boldsymbol{z},\boldsymbol{x})&\propto \rho^{\sum_{i=1}^n z_iI(x_i=0)+a-1}(1-\rho)^{n-\sum_{i=1}^n z_i+b-1},\\
\end{align}\]

Para a condicional completa de \(z_i\), notemos que
\[P(Z_i=1|x_i>0)=\frac{P(Z_i=1,X_i>0)}{P(X_i>0)}=0,\] e que

\[P(Z_i=z|x_i=0)= \left\{\begin{array}{ll}\frac{P(Z_i=0,X_i=0)}{P(X_i=0)}=\frac{h(0|\theta)(1-\rho)}{\rho+(1-\rho)h(0|\theta)},&,z=0\\
\frac{P(Z_i=1,X_i=0)}{P(X_i=0)}=\frac{\rho}{\rho+(1-\rho)h(0|\theta)},&z=1\end{array}\right.,\]
logo
\[\pi(z_i|\theta,\rho,\boldsymbol{x},\boldsymbol{z}_{(-i)})=\left\{\begin{array}{ll}\hbox{Bernoulli}\left( \frac{\rho}{\rho+(1-\rho)h(0|\theta)}\right),&\hbox{ se }x_i=0\\
I(z_i=0),&\hbox{ se } x_i>0\\ \end{array}\right.\]

Portanto, um amostrador de Gibbs para um modelo inflacionado de zeros é

Amostrador de Gibbs para o modelo inflado de zeros

Faça \(j=0\) e dê os valores iniciais \(\theta^{(0)}\) e \(\rho^{(0)}\).

No \(j\)-ésimo passo:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Para \(i\in\{1,\ldots,n\}\), se \(x_i>0\) faça \(z_i=0\). Senão,
  simule
  \[z_i^{(j)}\sim \hbox{Bernoulli}\left(\frac{\rho^{(j-1)}}{\rho^{(j-1)}+(1-\rho^{(j-1)})h(x_i|\theta^{(j-1)})}\right)\]
\item
  Simule
  \(\rho^{(j)}\sim\hbox{Beta}(a+\sum_{i=1}^n z_i^{(j)}I(x_i=0),b+n-\sum_{i=1}^n z_i^{(j)})\)
\item
  Simule \(\theta^{(j)}\) de
  \[\pi(\theta|\rho^{(j)},\boldsymbol{z}^{(j)},\boldsymbol{x})\propto \prod_{i=1}^n h(x_i|\theta^{(j)})^{1-z_i^{(j)}}\pi(\theta^{(j)}).\]
\end{enumerate}

Exemplo - A Poisson inflada de zeros

Neste exemplo, vamos considerar que a distribuição da contagem é
Poisson(\(\theta\)) e que \(\theta\sim\hbox{Gama}(r,s)\). Então,

\[\begin{align}
\pi(\theta|\rho^{(j)},\boldsymbol{z}^{(j)},\boldsymbol{x})&\propto \prod_{i=1}^{n} h(x_{i} | \theta )^{ 1-z_{i}^{(j)} }\pi(\theta)= 
 \prod_{i=1}^{n} \left[\frac{ e^{-\theta}\theta^{x_i} }{x_i!}\right]^{1-z_{i}^{(j)}}\frac{s^r}{\Gamma(r)}\theta^{r-1} e^{-s\theta}\\&\propto \theta^{\sum_{i=1}^n x_i(1-z_i^{(j)})+r-1}e^{-(n-\sum_{i=1}^n z_i^{(j)}+s)\theta}
 \end{align},\]

ou seja,
\(\theta^{(j)}|\rho^{(j)},\boldsymbol{z}^{(j)},\boldsymbol{x}\sim\hbox{Gama}(\sum_{i=1}^n x_i(1-z_i^{(j)})+r,n-\sum_{i=1}^n z_i^{(j)}+s)\)

Os dados abaixo representam o número anual de furacões atlânticos
grandes (categoria 4 ou 5) entre 1987 e 2012, nos Estados Unidos.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fur }\OtherTok{\textless{}{-}}  \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{ ,}\DecValTok{1}\NormalTok{,}
\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{,}
\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{,}
\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A frequência relativa de zeros é 0,58. Considerando o modelo
Poisson\((\theta)\) com \(\pi(\theta)\propto \theta^{-1}\), temos que

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r1 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(fur)}
\NormalTok{s1 }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(fur)}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{table}\NormalTok{(fur)}\SpecialCharTok{/}\NormalTok{s1, }\AttributeTok{type=} \StringTok{\textquotesingle{}p\textquotesingle{}}\NormalTok{, }\AttributeTok{xlab=}\StringTok{\textquotesingle{}No. anual de mortes pod fur\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab =} \StringTok{\textquotesingle{}Probabilidade\textquotesingle{}}\NormalTok{, }\AttributeTok{col =} \StringTok{\textquotesingle{}cyan3\textquotesingle{}}\NormalTok{, }\AttributeTok{pch=}\DecValTok{16}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}\FunctionTok{table}\NormalTok{(fur)}\SpecialCharTok{/}\NormalTok{s1, }\AttributeTok{col =} \StringTok{\textquotesingle{}cyan3\textquotesingle{}}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\FunctionTok{dnbinom}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{size =}\NormalTok{ r1, }\AttributeTok{prob =}\NormalTok{ s1}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{s1)), }\AttributeTok{pch=}\DecValTok{16}\NormalTok{, }\AttributeTok{col =} \StringTok{\textquotesingle{}brown\textquotesingle{}}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\FunctionTok{dnbinom}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{size =}\NormalTok{ r1, }\AttributeTok{prob =}\NormalTok{ s1}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{s1)), }\AttributeTok{col =} \StringTok{\textquotesingle{}brown\textquotesingle{}}\NormalTok{)}

\FunctionTok{legend}\NormalTok{(}\StringTok{\textquotesingle{}bottomleft\textquotesingle{}}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Freq. relativa\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Pred. post. Poisson\textquotesingle{}}\NormalTok{), }\AttributeTok{fill=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}cyan3\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}brown\textquotesingle{}}\NormalTok{), }\AttributeTok{bty=}\StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{misturas_files/figure-pdf/unnamed-chunk-2-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# hiperparâmetros para rho}
\NormalTok{a }\OtherTok{=}\NormalTok{ b }\OtherTok{=} \DecValTok{1}

\CommentTok{\# hiperparâmetros para theta}
\NormalTok{r}\OtherTok{=}\NormalTok{.}\DecValTok{1}
\NormalTok{s}\OtherTok{=}\NormalTok{.}\DecValTok{1}

\CommentTok{\# tamanho da amostra}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(fur) }

\CommentTok{\# valores iniciais da cadeia}
\NormalTok{theta }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(fur)}
\NormalTok{rho }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(fur }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}

\CommentTok{\# amostrador de Gibbs}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{50000}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B)\{}
  \CommentTok{\# simulando z}
\NormalTok{  z }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{  prob }\OtherTok{\textless{}{-}}\NormalTok{ rho[i]}\SpecialCharTok{/}\NormalTok{ ( (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{rho[i])}\SpecialCharTok{*}\FunctionTok{dpois}\NormalTok{(}\DecValTok{0}\NormalTok{,theta[i]) }\SpecialCharTok{+}\NormalTok{ rho[i])}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n)\{}
    \ControlFlowTok{if}\NormalTok{(fur[j] }\SpecialCharTok{\textgreater{}}\DecValTok{0}\NormalTok{)\{ z[j] }\OtherTok{\textless{}{-}} \DecValTok{0}\NormalTok{\} }\ControlFlowTok{else}\NormalTok{\{}
\NormalTok{      z[j] }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,prob)}
\NormalTok{    \}}
\NormalTok{  \}}

  \CommentTok{\# simulando rho}
\NormalTok{  rho[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{( }\DecValTok{1}\NormalTok{, a }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{( z }\SpecialCharTok{*}\NormalTok{ (fur }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)) , n}\SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(z)}\SpecialCharTok{+}\NormalTok{ b )}
  
  \CommentTok{\# simulando theta}
\NormalTok{  theta[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{rgamma}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{sum}\NormalTok{( fur}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{z) ) }\SpecialCharTok{+}\NormalTok{ r,  n }\SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(z) }\SpecialCharTok{+}\NormalTok{ s)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Vamos descartar a metade das simulações e usar um \textbf{thinning}
igual a 15:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta\_sim }\OtherTok{\textless{}{-}}\NormalTok{ theta[}\FunctionTok{seq}\NormalTok{(B}\SpecialCharTok{/}\DecValTok{2}\NormalTok{,B,}\DecValTok{15}\NormalTok{)]}
\NormalTok{rho\_sim }\OtherTok{\textless{}{-}}\NormalTok{ rho[}\FunctionTok{seq}\NormalTok{(B}\SpecialCharTok{/}\DecValTok{2}\NormalTok{,B,}\DecValTok{15}\NormalTok{)]}

\NormalTok{oo }\OtherTok{\textless{}{-}} \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{ts.plot}\NormalTok{(theta\_sim, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{ts.plot}\NormalTok{(rho\_sim, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{acf}\NormalTok{(theta\_sim)}
\FunctionTok{acf}\NormalTok{(rho\_sim)}
\end{Highlighting}
\end{Shaded}

\includegraphics{misturas_files/figure-pdf/unnamed-chunk-4-1.pdf}

Vamos estimar as probabilidade de ocorrerem \(k\) mortes via preditiva
posteriori:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# tamanho do vetor simulado}
\NormalTok{Bs }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(theta\_sim)}

\NormalTok{x\_til }\OtherTok{\textless{}{-}} \FunctionTok{array}\NormalTok{( }\ConstantTok{NA\_real\_}\NormalTok{, }\FunctionTok{c}\NormalTok{(Bs,n))}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{Bs)\{}
\NormalTok{  z }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{( n, }\DecValTok{1}\NormalTok{, rho\_sim[j])}
\NormalTok{  x\_til[j,] }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{z)}\SpecialCharTok{*}\FunctionTok{rpois}\NormalTok{(n, theta\_sim[j])}
\NormalTok{\}}

\CommentTok{\# probabilidades estimadas via ZIP}
\NormalTok{p\_zip }\OtherTok{\textless{}{-}} \FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(x\_til))}

\NormalTok{p\_zip}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x_til
           0            1            2            3            4            5 
6.154077e-01 1.972682e-01 1.162152e-01 4.766739e-02 1.693507e-02 4.476028e-03 
           6            7            8            9 
1.545845e-03 4.383739e-04 2.307231e-05 2.307231e-05 
\end{verbatim}

Abaixo mostramos as probabilidades preditas do modelo ZIP, do modelo
Poisson a e frequência relativa.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r1 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(fur)}
\NormalTok{s1 }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(fur)}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{table}\NormalTok{(fur)}\SpecialCharTok{/}\NormalTok{s1, }\AttributeTok{type=} \StringTok{\textquotesingle{}p\textquotesingle{}}\NormalTok{, }\AttributeTok{xlab=}\StringTok{\textquotesingle{}No. anual de mortes pod fur\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab =} \StringTok{\textquotesingle{}Probabilidade\textquotesingle{}}\NormalTok{, }\AttributeTok{col =} \StringTok{\textquotesingle{}cyan3\textquotesingle{}}\NormalTok{, }\AttributeTok{pch=}\DecValTok{16}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}\FunctionTok{table}\NormalTok{(fur)}\SpecialCharTok{/}\NormalTok{s1, }\AttributeTok{col =} \StringTok{\textquotesingle{}cyan3\textquotesingle{}}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\FunctionTok{dnbinom}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{size =}\NormalTok{ r1, }\AttributeTok{prob =}\NormalTok{ s1}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{s1)), }\AttributeTok{pch=}\DecValTok{16}\NormalTok{, }\AttributeTok{col =} \StringTok{\textquotesingle{}brown\textquotesingle{}}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\FunctionTok{dnbinom}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{size =}\NormalTok{ r1, }\AttributeTok{prob =}\NormalTok{ s1}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{s1)), }\AttributeTok{col =} \StringTok{\textquotesingle{}brown\textquotesingle{}}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{names}\NormalTok{(p\_zip),p\_zip, }\AttributeTok{pch=}\DecValTok{16}\NormalTok{,}\AttributeTok{col =} \StringTok{\textquotesingle{}magenta\textquotesingle{}}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{names}\NormalTok{(p\_zip),p\_zip,}\AttributeTok{col =} \StringTok{\textquotesingle{}magenta\textquotesingle{}}\NormalTok{)}

\FunctionTok{legend}\NormalTok{(}\StringTok{\textquotesingle{}bottomleft\textquotesingle{}}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Freq. relativa\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Pred. post. Poisson\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Pred. post. ZIP\textquotesingle{}}\NormalTok{), }\AttributeTok{fill=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}cyan3\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}brown\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}magenta\textquotesingle{}}\NormalTok{), }\AttributeTok{bty=}\StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{misturas_files/figure-pdf/unnamed-chunk-6-1.pdf}

\subsection{Exercício}\label{exercuxedcio-1}

Abaixo, segue o número anual de tornados em Lafayette Parish, Louisiana,
entre 1950 e 2012.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tor }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{,}
\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{,}
\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{,}
\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Ajuste o modelo Poisson.
\item
  Ajuste o modelo Poisson inflado de zeros.
\end{itemize}

\section{Mistura escalonada de
normais}\label{mistura-escalonada-de-normais}

\section{Misturas finitas com
número}\label{misturas-finitas-com-nuxfamero}

Dizemos que \(X|\boldsymbol{\theta},\boldsymbol{p},\kappa\) é um modelo
de mistura finito se sua função de densidade/probabilidade é dada por

\[f(x| \boldsymbol{\theta},\boldsymbol{p} ,\kappa )=\sum_{k=1}^\kappa p_k f_k(x|\boldsymbol{\theta}_k).\]

Cada função \(f(.|\boldsymbol{\theta}_k)\) é denominada componente da
mistura e o número de componentes pode ser desconhecido.

Assim como o modelo com zeros inflacionados, podemos utilizar uma
variável latente
\(\textbf{z}_i|\kappa=(z_{i,1},\ldots,z_{i,\kappa})\sim\hbox{Multinomial}(p_1\ldots,p_\kappa|\sum_{k=1}^\kappa z_{ik}=1)\),
obtendo o seguinte modelo aumentado

\[f(x_i|\boldsymbol{\theta},\textbf{z}_i,\kappa)=\prod_{k=1}^\kappa \left[f\left(x_i|\boldsymbol{\theta}_k\right)\right]^{z_{i,k}}\]

A função de verossimilhança aumentada para este modelo é

\[\prod_{i=1}^n f(x_i|\boldsymbol{\theta},\textbf{z}_i,\kappa)=\prod_{i=1}^n\prod_{k=1}^\kappa \left[f\left(x_i|\boldsymbol{\theta}_k\right)\right]^{z_{i,k}}.\]

Considere as prioris
\(\pi(\boldsymbol{\theta}|\kappa)=\prod_{k=1}^\kappa \pi(\boldsymbol{\theta}_k)\)
e \(\textbf{p}|\kappa\sim\hbox{Dirichlet}(a_1,\ldots,a_\kappa)\), onde
\[f(\textbf{p}|\kappa)\propto \prod_{k=1}^\kappa p_k^{a_k-1}\] com
\(\sum_{k=1}^\kappa p_k=1\). As condicionais completas para este
problema são

\begin{itemize}
\item
  \(\begin{align}f(\boldsymbol{\theta}_k|resto)\propto \prod_{i:z_{i,k}=1}f(x_i|\boldsymbol{\theta}_k)\pi(\boldsymbol{\theta}_k)\end{align}\)
\item
  \(\begin{align}f(\textbf{z}_i|resto)\propto \prod_{k=1}^\kappa \left[p_kf(x_i|\boldsymbol{\theta}_k)\right]^{z_{i,k}}\end{align}\)
  ou seja,
  \(\textbf{z}_i|rest\sim\hbox{Multinomial}(\tilde{p}_1,\ldots,\tilde{p}_\kappa)\),
  onde
\end{itemize}

\[\tilde{p}_k=\frac{p_kf(x_i|\boldsymbol{\theta}_k)}{\sum_{k=1}^\kappa p_kf(x_i|\boldsymbol{\theta}_k)}\]
*
\(f(\textbf{p}|resto)\propto \prod_{k=1}^\kappa p_k^{\sum_{i=1}^n z_{i,k}+a_k-1}\),
ou seja
\(\textbf{p}|resto\sim\hbox{Dirichlet}(a_1+\sum_{i=1}^n z_{i,1},\ldots,a_\kappa+\sum_{i=1}^n z_{i,\kappa})\)

Se necessário, podemos atrbuir a priori
\[\pi(\kappa)=\frac{1}{M},\kappa=1,2,\ldots,M\] para obter a condicional
completa
\[\pi(\kappa|resto)=\frac{\prod_{i=1}^n\prod_{k=1}^\kappa f(x_i|\boldsymbol{\theta}_k)^{z_{i,k}}\pi(\boldsymbol{\theta}_k)\pi(\textbf{p}|\kappa)\pi(\textbf{z}_i|\kappa)}{\sum_{\kappa=1}^M \prod_{i=1}^n\prod_{k=1}^\kappa f(x_i|\boldsymbol{\theta}_k)^{z_{i,k}}\pi(\boldsymbol{\theta}_k)\pi(\textbf{p}|\kappa)\pi(\textbf{z}_i|\kappa)},\kappa=1,\ldots,M.\]

\subsection{O velho fiel}\label{o-velho-fiel}

O banco de dados \texttt{faithful} mostra a duração e o tempo até a
próxima erupção do geiser Velho Fiel, no parque Yellowstone. Abaixo
mostramos o diagrama do tempo de espera entre erupções

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(faithful}\SpecialCharTok{$}\NormalTok{waiting)}
\end{Highlighting}
\end{Shaded}

\includegraphics{misturas_files/figure-pdf/unnamed-chunk-8-1.pdf}

É possível notar classes, uma com tempo e entre erupções menor que 70
com tempo maior. Temos as seguintes estimativas iniciais:

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# elementos na classe 1}
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ faithful}\SpecialCharTok{$}\NormalTok{waiting}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{\textless{}} \DecValTok{70}
\CommentTok{\# proporção na classe 1}
\FunctionTok{mean}\NormalTok{(z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3786765
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# média e desvio padrão na classe 1}
\FunctionTok{mean}\NormalTok{( x[z])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 55.15534
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{( x[z])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6.266558
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# elementos na classe 2}
\CommentTok{\# proporção na classe 2}
\FunctionTok{mean}\NormalTok{(z}\SpecialCharTok{==}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6213235
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# média e desvio padrão na classe 2}
\FunctionTok{mean}\NormalTok{( x[z}\SpecialCharTok{==}\NormalTok{F])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 80.49112
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{( x[z}\SpecialCharTok{==}\NormalTok{F])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5.456667
\end{verbatim}

Vamos considerar que as duas componentes possuem distribuição normal.
Para cada componente, teremos as seguintes prioris:

\[\pi(\mu_i,\phi_i)=\frac{\phi^{1/2}_i}{\sqrt{2\pi C}}e^{-\frac{\phi_i}{2C}(\mu_i-m_i)^2}\frac{b^a}{\Gamma(a)}\phi_i^{a-1}e^{b\phi_i},\]

\[p\sim\hbox{Beta}(r,s)\]

\[z_i\sim\hbox{Bernoulli}(p)\]

O modelo aumentado é
\[f(x_i|\mu,\phi,z_{i})=\left[\frac{\phi_1^{1/2}}{\sqrt{2\pi}}e^{-\frac{\phi_1}{2}(x_i-\mu_1)}\right]^{z_i}\left[\frac{\phi_2^{1/2}}{\sqrt{2\pi}}e^{-\frac{\phi_2}{2}(x_i-\mu_2)}\right]^{1-z_i}\]
As condicionais completas são:

\[\begin{align}f(\mu_1|resto) &\propto \exp\left\{-\frac{\phi_1}{2}\sum_{i=1}^n z_i(x_i-\mu_1)^2\right\}\exp\left\{-\frac{\phi_1}{2C} z_i(\mu_1-m_1)^2\right\}\\&\propto \exp\left\{-\frac{\phi_1}{2}\left(\sum_{i=1}^n z_i+C^{-1}\right) \left(\mu_1-\frac{\sum_{i=1}^{n}x_iz_i+m_1C^{-1}}{\sum_{i=1}^n z_i+C^{-1}}\right)^2\right\}\end{align}\]

\[\begin{align}f(\mu_2|resto) &\propto \exp\left\{-\frac{\phi_2}{2}\sum_{i=1}^n (1-z_i)(x_i-\mu_2)^2\right\}\exp\left\{-\frac{\phi_2}{2C} (1-z_i)(\mu_2-m_2)^2\right\}\\&\propto \exp\left\{-\frac{\phi_2}{2}\left(\sum_{i=1}^n (1-z_i)+C^{-1}\right) \left(\mu_2-\frac{\sum_{i=1}^{n}x_i(1-z_i)+m_1C^{-1}}{\sum_{i=1}^n (1-z_i)+C^{-1}}\right)^2\right\}\end{align}\]

\[\begin{align}f(\phi_2|resto)&\propto \phi_2^{-\frac{1}{2}\sum_{i=1}^{n}z_i}
e^{-\frac{\phi_2}{2}\sum_{i=1}^n (1-z_i)(x_i-\mu_2)^2}\phi^{-1/2}_2e^{-\frac{\phi_2}{2}(\mu_2-m_2)^2}\phi_2^{a/2-1}e^{-\phi_2 b/2}\\ &\propto \phi_2^{\frac{1}{2}(1+a+\sum_{i=1}^{n}(1-z_i)-1}e^{-\frac{\phi_2}{2}[\sum_{i=1}^n(1-z_i)(x_i-\mu_2)^2 +(\mu_2-m_2)^2 + b]}\end{align}\]
\[\begin{align}f(\phi_1|resto)&\propto \phi^{-\frac{1}{2}\sum_{i=1}^{n}z_i}
e^{-\frac{\phi_1}{2}\sum_{i=1}^n z_i(x_i-\mu_1)^2}\phi^{-1/2}e^{-\frac{\phi_1}{2}(\mu_1-m_1)^2}\phi_1^{a/2-1}e^{-\phi_1 b/2}\\ &\propto \phi_1^{\frac{1}{2}(1+a+\sum_{i=1}^{n}z_i)-1}e^{-\frac{\phi_1}{2}[\sum_{i=1}^nz_i(x_i-\mu_1)^2 +(\mu_1-m_1)^2 + b]}\end{align}\]

\[\begin{align}f(p|resto)\propto \prod_{i=1}^n p^{z_i}(1-p)^{1-z_i}p^{r-1}(1-p)^{s-1}\propto p^{r+\sum_{i=1}^n z_i-1}(1-p)^{s+\sum_{i=1}^n (1-z_i)-1}\end{align}\]

\[f(z_i|resto)\propto\left[ p\frac{\phi_1^{1/2}}{\sqrt{2\pi}}e^{-\frac{\phi_1}{2}(x_i-\mu_1)^2}\right]^{z_i}\left[ (1-p)\frac{\phi_2^{1/2}}{\sqrt{2\pi}}e^{-\frac{\phi_2}{2}(x_i-\mu_2)^2}\right]^{1-z_i}\]

Abaixo implementamos o amostrador de Gibbs

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{50000}

\CommentTok{\# hiperparmametros}
\NormalTok{m1 }\OtherTok{\textless{}{-}} \DecValTok{65}
\NormalTok{m2 }\OtherTok{\textless{}{-}} \DecValTok{80}
\NormalTok{C }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{r}\OtherTok{=} \DecValTok{4}\NormalTok{; s }\OtherTok{=} \DecValTok{6}
\NormalTok{a }\OtherTok{=} \DecValTok{1}\NormalTok{; b }\OtherTok{=}\NormalTok{ .}\DecValTok{1}

\CommentTok{\# valores iniciais}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{\textless{}} \DecValTok{70}
\NormalTok{phi1 }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\DecValTok{36}
\NormalTok{phi2 }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\DecValTok{25}
\NormalTok{mu1 }\OtherTok{=}\NormalTok{ mu2 }\OtherTok{=}\NormalTok{  p }\OtherTok{=} \ConstantTok{NULL}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B)\{}
  \CommentTok{\# mu dado o resto}
\NormalTok{  m1\_post }\OtherTok{\textless{}{-}}\NormalTok{ ( }\FunctionTok{sum}\NormalTok{(x}\SpecialCharTok{*}\NormalTok{z) }\SpecialCharTok{+}\NormalTok{ m1}\SpecialCharTok{/}\NormalTok{C) }\SpecialCharTok{/}\NormalTok{ ( }\FunctionTok{sum}\NormalTok{(z) }\SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{/}\NormalTok{C )}
\NormalTok{  m2\_post }\OtherTok{\textless{}{-}}\NormalTok{ ( }\FunctionTok{sum}\NormalTok{(x}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{z)) }\SpecialCharTok{+}\NormalTok{ m1}\SpecialCharTok{/}\NormalTok{C) }\SpecialCharTok{/}\NormalTok{ ( }\FunctionTok{sum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{z) }\SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{/}\NormalTok{C )}
\NormalTok{  s1\_post }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ ( ( }\FunctionTok{sum}\NormalTok{(z) }\SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{/}\NormalTok{C )}\SpecialCharTok{*}\NormalTok{phi1[i] )}
\NormalTok{  s2\_post }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ ( ( }\FunctionTok{sum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{z) }\SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{/}\NormalTok{C )}\SpecialCharTok{*}\NormalTok{phi2[i] )}
  
\NormalTok{  mu1[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, m1\_post, }\FunctionTok{sqrt}\NormalTok{( s1\_post) )}
\NormalTok{  mu2[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, m2\_post, }\FunctionTok{sqrt}\NormalTok{( s2\_post) )}
  
  \CommentTok{\# phi dado resto}
\NormalTok{  phi1[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{rgamma}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1} \SpecialCharTok{+}\NormalTok{ a }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{(z), }\FunctionTok{sum}\NormalTok{( z}\SpecialCharTok{*}\NormalTok{(x }\SpecialCharTok{{-}}\NormalTok{ mu1[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{])}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ (mu1[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{m1)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ b)}
\NormalTok{  phi2[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{rgamma}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1} \SpecialCharTok{+}\NormalTok{ a }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{z), }\FunctionTok{sum}\NormalTok{( (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{z)}\SpecialCharTok{*}\NormalTok{(x }\SpecialCharTok{{-}}\NormalTok{ mu2[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{])}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ (mu2[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{m2)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ b)}
  
  \CommentTok{\# p dado resto}
\NormalTok{  p[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(}\DecValTok{1}\NormalTok{, r }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{(z), s }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{z) )}
  
  \CommentTok{\# z dado resto}
\NormalTok{  aux1 }\OtherTok{\textless{}{-}}\NormalTok{ p[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{]}\SpecialCharTok{*}\FunctionTok{dnorm}\NormalTok{(x,mu1[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(phi1[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{]))}
\NormalTok{  aux2 }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{])}\SpecialCharTok{*}\FunctionTok{dnorm}\NormalTok{(x,mu2[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(phi2[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{]))}
  
\NormalTok{  z }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\DecValTok{1}\NormalTok{, aux1}\SpecialCharTok{/}\NormalTok{( aux1 }\SpecialCharTok{+}\NormalTok{ aux2))}
\NormalTok{\}}
\CommentTok{\# }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(mu1[}\FunctionTok{seq}\NormalTok{(B}\SpecialCharTok{/}\DecValTok{2}\NormalTok{,B,}\DecValTok{30}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\includegraphics{misturas_files/figure-pdf/unnamed-chunk-12-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(mu2[}\FunctionTok{seq}\NormalTok{(B}\SpecialCharTok{/}\DecValTok{2}\NormalTok{,B,}\DecValTok{30}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\includegraphics{misturas_files/figure-pdf/unnamed-chunk-12-2.pdf}

\bookmarksetup{startatroot}

\chapter{O modelo Poisson revisitado}\label{o-modelo-poisson-revisitado}

\section{Verossimilhança, prioris e
posterioris}\label{verossimilhanuxe7a-prioris-e-posterioris}

Dizemos que \(X|\theta\) tem distribuição Poisson se sua função de
probabilidade é dada por \[f(x|\theta)=\frac{e^{-\theta}\theta^x}{x!},\]
onde \(x=0,1,\ldots\) e \(\theta>0\). O parâmetro \(\theta\) é
denominado taxa. Para este modelo \[E(X|\theta)=Var(X|\theta)=\theta.\]

Esta é uma das distribuições para contagens mais importantes. A
verossimilhança deste modelo, para uma amostra de vaiid, é dada por
\[L(\theta)=\frac{e^{-n\theta}\theta^{\sum_{i=1}^{n}x_i}}{\prod_{i=1}^{n}x_i!}.\]
O modelo Poisson pertence à família exponencial e sua \textit{priori}
conjugada é \(\theta\sim\hbox{Gama}(r,s)\), onde \(r\) e \(s\) podem ser
interpretados como o total da contagem e o tamanho da amostra
\textit{a priori}.

Neste caso, a \textit{posteriori} é
\(\hbox{Gama}(r+\sum_{i=1}^n x_i+r, s+n)\).

A média da \textit{posteriori} é
\[E(\theta|\mathbf{x})=\frac{\sum_{i=1}^{n}x_i+r}{n+s}=\frac{n}{n+s}\bar{x}+\frac{s}{n+s}E(\theta),\]
onde fica claro que este estimador é uma média ponderada das informações
provenientes das duas fontes de informação (sendo \(\bar{x}\) a
estimativa de máxima verossimilhança e \(E(\theta)\) a média
\textit{a priori}).

Se \(n\gg s\), então a média a posteriori dará maior peso para a
informação dos dados.

A informação de Fisher é \[\mathcal{I}(\theta)=\frac{1}{\theta}\]

Assim, a priori de Jeffreys é dada por
\[f(\theta)\propto \theta^{-\frac{1}{2}},\] sendo, portanto, uma priori
imprópria. Contudo,
\[f(\theta|\mathbf{x})\propto e^{-n\theta}\theta^{\sum_{i=1}^{n}x_i} \theta^{-\frac{1}{2}},\]
logo, a posteriori é própria, tendo distribuição
\(Gama(\sum_{i=1}^{n}x_i+1/2,n)\).

Considere a posteriori \(\theta|\mathbf{x}\sim\hbox{Gama}(r_1,s_1)\).
Podemos retirar uma amostra da preditiva \textit{a posteriori} do
seguinte modo:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Gere \(\theta_j\sim\hbox{Gama}(r_1,s_1)\)
\end{enumerate}

2.Gere \(\tilde{\mathbf{x}}\sim\hbox{Poisson}(\theta_j)\).

\section{O modelo Poisson para taxas}\label{o-modelo-poisson-para-taxas}

A taxa é o cociente entre o número de casos de um evento em determinado
intervalo de tempo e a população em risco, definida em um espaço e no
mesmo intervalo de tempo (``pessoas-tempo'\,'). Note que, pela
definição, a taxa é uma estatística.

Seja \(n\) o tamanho da população no espaço/tempo e seja \(y\) o número
de casos do evento de interesse. Então,

\[\hbox{taxa} = \frac{y}{n}\]

Contudo, como \(n\) tende a ser muito maior que \(y\), é comum reportar
a taxa vezes \(10^k\), para algum \(k>0\).

Exemplo: Segundo o Anuário de Segurança Pública 2022, em 2021 houveram
68.885 casos de estupro. Considerando uma população de 212,7 milhões de
habitantes, a taxa de estupro para aquele ano foi de
\[\frac{68.885}{212.700.000}=3,23\times 10^{-4}\] casos por pessoa-ano.
Como \(n\) tende a ser maior que \(y\), é comum considerar.

Multiplicando a taxa por \(10^5\), temos uma taxa de 32,3 casos para
cada 100.000 habitantes.

Agora,considere que \(\theta\) é o parâmetro taxa. Então,

\[\hat{\theta}=\frac{y}{n}\] é a estimativa para \(\theta\). Como \(y\)
é uma contagem, é razoável supor que \[\theta =\frac{1}{n}E(Y|\theta).\]
e um modelo possível seria \(y|\theta\sim\hbox{Poisson}(\theta n)\).

Agora, considere que uma população está particionada em \(m\)
localidades. Para um dado intervalo de tempo, sejam \(n_i\) e \(y_i\) a
população da localidade \(i\) e seu respectivo número de casos
observados. Suponha ainda que a taxa \(\theta\) é comum para a pooulação
e que \(y_i\) é condicionalmente independente de \(y_j\) dado
\(\theta\). Assumindo a distribuição Poisson, teremos

\[L(\theta)=\prod_{i=1}^m\frac{e^{-\theta n_i}(\theta n_i)^{y_i}}{y_i!}\varpropto \theta^{\sum_{i=1}^m y_i}e^{-\theta \sum_{i=1}^m n_i}=\theta^{\sum_{i=1}^n y_i}e^{-\theta N},\]
onde \(N=\sum_{i=1}^m n_i\) é o tamanho da população. Como a
verossimilhança pertence à família exponencial, temos que o modelo
Gama\((a,b)\) é conjugado gerando a posteriori

\[\theta|\mathbf{y}\sim\hbox{Gama}\left(\sum_{i=1}^{m}y_i+a,N+b\right).\]

A prioris impróprias \(\pi(\theta)\varpropto \theta^{-1}\) e
\(\pi(\theta)\varpropto \theta^{-1/2}\) geram, respectivamente, as
posterioris \(\hbox{Gama}(\sum_{i=1}^m y_i,N)\) e
\(\hbox{Gama}(\sum_{i=1}^m y_i+1/2,N)\).

\section{Exemplo 1: crime de estupro de vulnerável no interior do
Amazonas}\label{exemplo-1-crime-de-estupro-de-vulneruxe1vel-no-interior-do-amazonas}

Os dados a seguir foram cedidos pelo Observatório de Violência de Gênero
no Amazonas e compreendem os anos entre 2010 e 2012.

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Cidade & vitimas & Populacao feminina \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Amatura & 3 & 639 \\
Atalaia do Norte & 6 & 905 \\
Barreirinha & 12 & 1899 \\
Benjamin Constant & 2 & 2036 \\
Boa Vista do Ramos & 6 & 1060 \\
Fonte Boa & 0 & 1438 \\
Jutai & 1 & 1143 \\
Maues & 13 & 3421 \\
Nhamunda & 9 & 1168 \\
Parintins & 20 & 6700 \\
Santo Antonio do Ica & 7 & 1608 \\
Sao Paulo de Olivenca & 5 & 2033 \\
Tabatinga & 8 & 3095 \\
Tonantins & 1 & 1186 \\
\end{longtable}

Considerando a priori \(\pi(\theta)\varpropto \theta^{-1/2}\) teremos:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# banco de dados}
\NormalTok{casos }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{( }\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{1}\NormalTok{ )   }

\NormalTok{pop }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{639}\NormalTok{, }\DecValTok{905}\NormalTok{, }\DecValTok{1899}\NormalTok{, }\DecValTok{2036}\NormalTok{, }\DecValTok{1060}\NormalTok{, }\DecValTok{1438}\NormalTok{, }\DecValTok{1143}\NormalTok{, }\DecValTok{3421}\NormalTok{, }\DecValTok{1168}\NormalTok{, }\DecValTok{6700}\NormalTok{, }\DecValTok{1608}\NormalTok{, }\DecValTok{2033}\NormalTok{, }\DecValTok{3095}\NormalTok{, }\DecValTok{1186}\NormalTok{)}
\NormalTok{pop }\OtherTok{\textless{}{-}}\NormalTok{ pop}\SpecialCharTok{/}\DecValTok{10}\SpecialCharTok{\^{}}\DecValTok{5}

\NormalTok{municipios }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{( }\StringTok{\textquotesingle{}Amatura\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Atl.Norte\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Barr\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}BC\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}BV Ramos\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Fonte B\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Jutai\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Maues\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Nhamunda\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Parintins\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}StoIca\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}SP Olivenca\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Tbt\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Tonantins\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# posteriori}
\NormalTok{a\_post }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(casos) }\SpecialCharTok{+}\NormalTok{ .}\DecValTok{5}
\NormalTok{b\_post }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(pop)}

\FunctionTok{curve}\NormalTok{(}\FunctionTok{dgamma}\NormalTok{(x,a\_post,b\_post),}\DecValTok{230}\NormalTok{,}\DecValTok{450}\NormalTok{ ,}\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{xlab =} \FunctionTok{expression}\NormalTok{(theta), }\AttributeTok{ylab =} \StringTok{\textquotesingle{}densidade a posteriori\textquotesingle{}}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\includegraphics{poisson_files/figure-pdf/unnamed-chunk-2-1.pdf}

Abaixo, simulamos 50.000 amostras da preditiva a posteriori

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{50000} \CommentTok{\# número de simulações da preditiva a posteriori}
\NormalTok{m }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(casos)}

\NormalTok{pred\_mun }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5000}\NormalTok{)\{}
\NormalTok{  theta }\OtherTok{\textless{}{-}} \FunctionTok{rgamma}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{sum}\NormalTok{(casos) }\SpecialCharTok{+}\NormalTok{ .}\DecValTok{5}\NormalTok{, }\FunctionTok{sum}\NormalTok{(pop))}
\NormalTok{  pred\_mun }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(pred\_mun, }\FunctionTok{rpois}\NormalTok{(m , theta }\SpecialCharTok{*}\NormalTok{ pop))}
\NormalTok{\}}

\NormalTok{pred\_mun }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(pred\_mun)}
\FunctionTok{names}\NormalTok{(pred\_mun) }\OtherTok{\textless{}{-}}\NormalTok{ municipios}
\FunctionTok{boxplot}\NormalTok{(pred\_mun)}
\FunctionTok{points}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{14}\NormalTok{,casos, }\AttributeTok{pch=}\DecValTok{16}\NormalTok{,}\AttributeTok{cex =} \FloatTok{1.2}\NormalTok{, }\AttributeTok{col =}\StringTok{\textquotesingle{}tomato\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{poisson_files/figure-pdf/unnamed-chunk-3-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oo }\OtherTok{\textless{}{-}} \FunctionTok{par}\NormalTok{()}
\NormalTok{mat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{m,}\AttributeTok{ncol=}\DecValTok{2}\NormalTok{)}
\NormalTok{mat }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(mat, }\FunctionTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{,}\DecValTok{16}\NormalTok{))}
\FunctionTok{layout}\NormalTok{(mat, }\AttributeTok{heights =} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{14}\NormalTok{,.}\DecValTok{5}\NormalTok{,.}\DecValTok{5}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{m)\{}
\NormalTok{  freq }\OtherTok{\textless{}{-}} \FunctionTok{prop.table}\NormalTok{( }\FunctionTok{table}\NormalTok{(pred\_mun[,i]) )}
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{cex =}\NormalTok{ .}\DecValTok{8}\NormalTok{)}
  \FunctionTok{plot.new}\NormalTok{()}
  \FunctionTok{plot.window}\NormalTok{(}\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{46}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,.}\DecValTok{3}\NormalTok{))}
  \FunctionTok{points}\NormalTok{(}\FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{names}\NormalTok{(freq)), freq, }\AttributeTok{type=}\StringTok{\textquotesingle{}h\textquotesingle{}}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
  \FunctionTok{title}\NormalTok{(}\AttributeTok{ylab=}\NormalTok{municipios[i])}
  \FunctionTok{points}\NormalTok{(casos[i],}\DecValTok{0}\NormalTok{,}\AttributeTok{pch=}\DecValTok{16}\NormalTok{,}\AttributeTok{col=}\StringTok{\textquotesingle{}tomato\textquotesingle{}}\NormalTok{,}\AttributeTok{cex=} \FloatTok{1.2}\NormalTok{)}
  
\NormalTok{\}}
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
  \FunctionTok{plot.new}\NormalTok{()}
  \FunctionTok{plot.window}\NormalTok{(}\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{46}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,.}\DecValTok{1}\NormalTok{))}
  \FunctionTok{segments}\NormalTok{(}\DecValTok{0}\NormalTok{,.}\DecValTok{05}\NormalTok{,}\DecValTok{46}\NormalTok{,.}\DecValTok{05}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
 \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{45}\NormalTok{,}\DecValTok{5}\NormalTok{))\{}
    \FunctionTok{segments}\NormalTok{(j,.}\DecValTok{05}\NormalTok{,j,.}\DecValTok{03}\NormalTok{)}
    \FunctionTok{text}\NormalTok{(j,.}\DecValTok{01}\NormalTok{,j)}
\NormalTok{  \}}
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
  \FunctionTok{plot.new}\NormalTok{()}
  \FunctionTok{plot.window}\NormalTok{(}\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{46}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,.}\DecValTok{1}\NormalTok{))}
  \FunctionTok{segments}\NormalTok{(}\DecValTok{0}\NormalTok{,.}\DecValTok{05}\NormalTok{,}\DecValTok{46}\NormalTok{,.}\DecValTok{05}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{45}\NormalTok{,}\DecValTok{5}\NormalTok{))\{}
    \FunctionTok{segments}\NormalTok{(j,.}\DecValTok{05}\NormalTok{,j,.}\DecValTok{03}\NormalTok{)}
    \FunctionTok{text}\NormalTok{(j,.}\DecValTok{01}\NormalTok{,j)}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\includegraphics{poisson_files/figure-pdf/unnamed-chunk-4-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(oo)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in par(oo): parâmetro gráfico "cin" não pode ser especificado
\end{verbatim}

\begin{verbatim}
Warning in par(oo): parâmetro gráfico "cra" não pode ser especificado
\end{verbatim}

\begin{verbatim}
Warning in par(oo): parâmetro gráfico "csi" não pode ser especificado
\end{verbatim}

\begin{verbatim}
Warning in par(oo): parâmetro gráfico "cxy" não pode ser especificado
\end{verbatim}

\begin{verbatim}
Warning in par(oo): parâmetro gráfico "din" não pode ser especificado
\end{verbatim}

\begin{verbatim}
Warning in par(oo): parâmetro gráfico "page" não pode ser especificado
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{m)\{}
\NormalTok{p[i] }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{min}\NormalTok{(}\FunctionTok{mean}\NormalTok{(pred\_mun[,i] }\SpecialCharTok{\textgreater{}}\NormalTok{ casos[i]),}
\FunctionTok{mean}\NormalTok{(pred\_mun[,i] }\SpecialCharTok{\textless{}}\NormalTok{ casos[i]))}
\NormalTok{\}}

\FunctionTok{data.frame}\NormalTok{(municipios,p)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    municipios      p
1      Amatura 0.3372
2    Atl.Norte 0.0676
3         Barr 0.0300
4           BC 0.0248
5     BV Ramos 0.1340
6      Fonte B 0.0000
7        Jutai 0.0508
8        Maues 0.5224
9     Nhamunda 0.0144
10   Parintins 0.6220
11      StoIca 0.3524
12 SP Olivenca 0.3896
13         Tbt 0.4316
14   Tonantins 0.0396
\end{verbatim}

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{0}{1}
\end{CSLReferences}




\end{document}
