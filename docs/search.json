[
  {
    "objectID": "regressao.html",
    "href": "regressao.html",
    "title": "12  O modelo de regressão linear normal",
    "section": "",
    "text": "12.1 Introdução\nDenominamos regressão o problema de determinar a média de \\(Y\\) dado o vetor \\(\\boldsymbol{x}'=(x_1,\\ldots,x_q)\\). Em particular se\n\\[E(Y|\\boldsymbol{x},\\boldsymbol{\\beta})=\\boldsymbol{x}'\\boldsymbol{\\beta}=\\sum_{j=1}^q x_j\\beta_j,\\] dizemos que a regressão é linear (em \\(\\boldsymbol{\\beta}\\)) e o objetivo passa a ser fazer inferências sobre \\(\\boldsymbol{\\beta}\\).\nNo problema de regressão, as seguintes nomenclaturas são usuais:\nOs termos input e output são usuais em aprendizagem de máquina, enquanto que variável independente e dependente são usuais na área da saúde (vale ressaltar que não há relação com a independência sob o ponto de vista da probabilidade, pois claramente \\(Y\\) e \\(\\boldsymbol{X}\\) são dependentes).\nSejam \\(Y_1,\\ldots,Y_n\\) uma amostra de variáveis aleatórias independentes. Sejam \\(\\boldsymbol{x}_1,\\ldots,\\boldsymbol{x}_n\\) vetores de dimensão \\(q\\). Então, o modelo de regressão linear normal é dado por\n\\[y|\\boldsymbol{x},\\boldsymbol{\\beta}\\sim\\hbox{Normal}(\\boldsymbol{x}'\\boldsymbol{\\beta},\\sigma^2).\\]\nComo \\(Y_1,\\ldots,Y_n\\) são independentes, temos que \\(\\boldsymbol{Y}'=(Y_1,\\ldots,Y_n)\\) tem distribuição normal multivariada com média\n\\[E(\\boldsymbol{Y}|\\boldsymbol{x}_1,\\ldots,\\boldsymbol{x}_n)=\\left(\\begin{array}{c}E(Y_1|\\boldsymbol{x}_1)\\\\ \\vdots \\\\ E(Y_n|\\boldsymbol{x}_n)\\end{array}\\right)=\\left(\\begin{array}{c}\\boldsymbol{x}_1'\\boldsymbol{\\beta}\\\\ \\vdots \\\\ \\boldsymbol{x}_n'\\boldsymbol{\\beta}\\end{array}\\right)=\\underbrace{\\left(\\begin{array}{c}\\boldsymbol{x}_1'\\\\ \\vdots \\\\ \\boldsymbol{x}_n'\\end{array}\\right)}_{\\boldsymbol{X}}\\boldsymbol{\\beta}=\\boldsymbol{X}\\boldsymbol{\\beta}\\]\ne como \\(Var(Y_i|\\boldsymbol{X})=Var(Y_i|\\boldsymbol{x}_i)=\\sigma^2\\) e, para \\(i\\neq j,\\) \\(Cov(Y_i,Y_j|\\boldsymbol{X})=0\\), teremos que \\[Var(\\boldsymbol{Y}|\\boldsymbol{X})=\\sigma^2\\text{I}_n\\] Portanto \\(\\boldsymbol{Y}|\\boldsymbol{X},\\boldsymbol{\\beta},\\sigma^2\\sim\\hbox{Normal}(\\boldsymbol{X}\\boldsymbol{\\beta},\\sigma^2\\text{I}_q)\\). Sua função de verossimilhança é\n\\[L(\\boldsymbol{\\beta},\\sigma^2)\\propto \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{\\frac{n}{2}}e^{-\\frac{1}{2\\sigma^2}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})'(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})}.\\] Note que\n\\[\\begin{align}\n(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})'(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})&= \\boldsymbol{y}'\\boldsymbol{y}+\\boldsymbol{\\beta}'(\\boldsymbol{X}'\\boldsymbol{X})\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'\\boldsymbol{X}'\\boldsymbol{y}\\\\&=\n\\boldsymbol{y}'\\boldsymbol{y}+\\boldsymbol{\\beta}'(\\boldsymbol{X}'\\boldsymbol{X})\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'(\\boldsymbol{X}'\\boldsymbol{X})\\underbrace{(\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{y}}_{\\hat{\\boldsymbol{\\beta}}}\n\\\\&=\n\\boldsymbol{y}'\\boldsymbol{y}+\\boldsymbol{\\beta}'(\\boldsymbol{X}'\\boldsymbol{X})\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}\\pm \\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}\\\\&=(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})+\\boldsymbol{y}'\\boldsymbol{y}-\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}\\end{align}\n\\] Observe que \\[\\begin{align}\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}&=((\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{y})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{y}\\\\\n&=((\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{y})'\\boldsymbol{X}'\\boldsymbol{y}\\\\&=\n\\boldsymbol{y}\\boldsymbol{X}(\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{y}\\end{align}\\] logo\n\\[\\begin{align}\n(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})'(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})&= (\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})+\\underbrace{\\boldsymbol{y}'(\\text{I}_n-\\boldsymbol{X}(\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}')\\boldsymbol{y}}_{SQR}\\end{align}\n\\] onde \\(SQR\\) é a sigla para soma de quadrados de resíduos. Esse termo tem esse nome porque \\[SQR=(\\boldsymbol{y}-\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{y}-\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}})\\] e \\[\\boldsymbol{r}=\\boldsymbol{y}-\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}\\] é denominado vetor de resíduos. Portanto, a função de verossimilhança pode ser reescrita como\n\\[L(\\boldsymbol{\\beta},\\sigma^2)\\propto \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{\\frac{n}{2}}e^{-\\frac{1}{2\\sigma^2}(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})}e^{-\\frac{1}{2\\sigma^2}SQR}.\\] ## Priori conjugada e suas limitações\nSeja \\(\\phi=1/\\sigma^2\\). Então, a priori normal-gama, também escrita como \\[\\begin{align}\\boldsymbol{\\beta}|\\phi,\\boldsymbol{X}&\\sim\\hbox{Normal}(\\boldsymbol{\\beta}_0,\\phi^{-1} C_0^{-1})\\\\ \\phi&\\sim\\hbox{Gama}\\left(\\frac{n_0}{2},\\frac{s_0}{2}\\right)\\end{align}\\] é conjugada para o modelo linear. A posteriori é dada por\n\\[\\begin{align}f(\\boldsymbol{\\beta},\\phi|\\text{dados})&\\propto \\phi^\\frac{n}{2}e^{-\\frac{\\phi}{2}(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})-\\frac{\\phi}{2}SQR}\\times\\\\&|\\phi\\boldsymbol{C}_0|^{1/2}e^{-\\frac{\\phi}{2}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)'\\boldsymbol{C}_0(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)]}\\phi^{\\frac{n_0}{2}-1}e^{-\\frac{s_0}{2}\\phi}\\\\&=\\phi^{\\frac{q}{2}}e^{-\\frac{\\phi}{2}[(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})+(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)'\\boldsymbol{C}_0(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)]}\\phi^{\\frac{n+n_0}{2}-1}e^{-\\frac{\\phi}{2}(SQR+s_0)}\\end{align}\\] Como \\[\\begin{align}\n&(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})+(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)'\\boldsymbol{C}_0(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)\\\\&=\\boldsymbol{\\beta}'[\\underbrace{(\\boldsymbol{X}'\\boldsymbol{X})+\\boldsymbol{C}_0}_{\\boldsymbol{C}_1}]\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'( (\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{C}_0\\boldsymbol{\\beta}_0)+\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{\\beta}_0'\\boldsymbol{C}_0\\boldsymbol{\\beta}_0\\\\\n&=\\boldsymbol{\\beta}'\\boldsymbol{C}_1\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'\\boldsymbol{C}_1[\\underbrace{\\boldsymbol{C}_1^{-1}( (\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{C}_0\\boldsymbol{\\beta}_0)}_{\\tilde{\\boldsymbol{\\beta}}}]+\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{\\beta}_0'\\boldsymbol{C}_0\\boldsymbol{\\beta}_0\\\\\n&=\\boldsymbol{\\beta}'\\boldsymbol{C}_1\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'\\boldsymbol{C}_1\\tilde{\\boldsymbol{\\beta}}\\pm\\tilde{\\boldsymbol{\\beta}}'\\boldsymbol{C}_1\\tilde{\\boldsymbol{\\beta}}+\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{\\beta}_0'\\boldsymbol{C}_0\\boldsymbol{\\beta}_0\\\\\n&=(\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{\\beta}})'\\boldsymbol{C}_1(\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{\\beta}})+\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{\\beta}_0'\\boldsymbol{C}_0\\boldsymbol{\\beta}_0-\\tilde{\\boldsymbol{\\beta}}'\\boldsymbol{C}_1\\tilde{\\boldsymbol{\\beta}}\n\\end{align}\\] Além disso, pode-se mostrar que \\[\\hat{\\boldsymbol{\\beta}}'(\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{\\beta}_0'\\boldsymbol{C}_0\\boldsymbol{\\beta}_0-\\tilde{\\boldsymbol{\\beta}}'\\boldsymbol{C}_1\\tilde{\\boldsymbol{\\beta}}=(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)'(\\boldsymbol{C}_0+(\\boldsymbol{X}'\\boldsymbol{X}^{-1}))^{-1}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)\\] \\[\\begin{align}f(\\boldsymbol{\\beta},\\phi|\\text{dados})&\\propto\\phi^{\\frac{q}{2}}e^{-\\frac{\\phi}{2}(\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{\\beta}})'\\boldsymbol{C}_1(\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{\\beta}})}\\phi^{\\frac{n+n_0}{2}-1}e^{-\\frac{\\phi}{2}(SQR+s_0+(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)'(\\boldsymbol{C}_0+(\\boldsymbol{X}'\\boldsymbol{X}^{-1}))^{-1}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0))}\\end{align}\\] ou seja \\[\\begin{align}\\boldsymbol{\\beta}|\\phi,\\hbox{dados}&\\sim\\hbox{Normal}(\\tilde{\\boldsymbol{\\beta}},\\phi^{-1}\\boldsymbol{C}_1^{-1})\\\\\n\\phi|\\text{dados}&\\sim\\hbox{Gama}\\left(\\frac{n_0+n}{2},\\frac{s_1}{2}\\right)\n\\end{align}\\] onde \\[s_1=SQR+s_0+(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)'(\\boldsymbol{C}_0+(\\boldsymbol{X}'\\boldsymbol{X})^{-1})^{-1}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)\\]\nO estimador de Bayes para \\(\\boldsymbol{\\beta}\\) é\n\\[\\tilde{\\boldsymbol{\\beta}}=\\boldsymbol{C}_1^{-1}[ (\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}+\\boldsymbol{C}_0\\boldsymbol{\\beta}_0]\\]\nPara elicitar corretamente a priori conjugada é necessário elicitar a matriz \\(\\boldsymbol{C}_0\\), que é um desafio. A solução mais simples é considerar que \\(\\boldsymbol{C_0}=\\lambda \\text{I}_q\\) e valores pequenos de \\(\\lambda\\) levam a uma informação a priori mais difusa. Essa solução é utilizada na inferência frequentista, conforme podemos ver abaixo.\nO exemplo acima mostrou um comportamento aparentemente contraditório. Ao fazer \\(\\lambda\\) pequeno, espera-se que a inferência seja baseada nos dados. Em contradição, a posteriori favoreceu o modelo nulo, no qual todos os coeficientes da regressão são nulos. Esse comportamento é conhecido como Paradoxo de Lindley e ele ocorre porque a probabilidade a priori da região de interesse (onde os coeficientes são não-nulos) se espalha muito, diminuindo a massa da prior sobre valores onde a verossimilhança é alta.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>O modelo de regressão linear normal</span>"
    ]
  },
  {
    "objectID": "regressao.html#introdução",
    "href": "regressao.html#introdução",
    "title": "12  O modelo de regressão linear normal",
    "section": "",
    "text": "\\(Y\\): variável resposta, output, resultado, variável dependente\n\\(\\boldsymbol{x}\\): variáveis regressoras, input, variáveis independentes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA relação com a Regressão Ridge O estimador frequentista existe apenas quando \\(\\boldsymbol{X}'\\boldsymbol{X}\\) tem inversa. Essa inversa pode não existir quando \\(q&gt;n\\) ou quando as colunas de \\(\\boldsymbol{X}\\) são altamente correlacionada (esse fenômeno é denominado multicolinearidade). Uma solução é utilizar o estimador de regressão de Ridge, dado por\n\\[\\hat{\\boldsymbol{\\beta}}_{R}(\\lambda)=[\\lambda\\text{I}_q+ \\boldsymbol{X}'\\boldsymbol{X}]^{-1}\\boldsymbol{X}'\\boldsymbol{Y},\\] onde \\(\\lambda\\) é denominado shrinkage (algo como parâmetro de contração, embora também possa ser denominado parâmetro de regularização). A escolha de \\(\\lambda\\) é um problema em aberto, pois não há garantias de resultados ótimos.\nSob o ponto de vista bayesiano, fazendo \\(\\boldsymbol{\\beta}_0=\\text{0}_q\\) e \\(\\boldsymbol{C}_0=\\lambda\\text{I}_q\\), temos que\n\\[\\begin{align}\\tilde{\\boldsymbol{\\beta}}&=(\\lambda\\text{I}_q +\\boldsymbol{X}'\\boldsymbol{X})^{-1} (\\boldsymbol{X}'\\boldsymbol{X})\\hat{\\boldsymbol{\\beta}}\\\\&=\n(\\lambda\\text{I}_q +\\boldsymbol{X}'\\boldsymbol{X})^{-1} (\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{Y}\\\\&=\n[\\lambda\\text{I}_q+ \\boldsymbol{X}'\\boldsymbol{X}]^{-1}\\boldsymbol{X}'\\boldsymbol{Y}=\\hat{\\beta}(\\lambda),\\end{align}\\] ou seja, o estimador de Bayes é o estimador da regressão Ridge. Neste caso, o aumento de \\(\\lambda\\) implica no aumento da crença a priori de que \\(\\boldsymbol{\\beta}\\) é zero.\n\n\nExemplo. mtcars\nO banco de dados mtcars, disponível no R, apresenta estatísticas de design e performance de 32 automóveis. Considere as seguintes variáveis:\n\nmpg: milhas/galão\ncyl: número de cilindros\ndisp: deslocamento (polegadas cúbicas)\nhp: potência bruta\nwt: peso (em libras)\n\nConsiderando mpg como variável resposta, vamos analisar sua relação com as demais.\n\nbanco  &lt;- mtcars[,c(\"mpg\",\"cyl\", \"disp\",\"hp\",\"wt\")]\npairs(banco)\n\n\n\n\n\n\n\n\nAlgumas relações, como disp - deslocamento - e hp - potência bruta, não parecem lineares. Podemos linearizar a relação aplicando a transformação logarítmica\n\nbanco2  &lt;- banco\nbanco2$disp &lt;- log ( banco$disp )\nbanco2$hp   &lt;- log ( banco$hp )\npairs(banco2)\n\n\n\n\n\n\n\n\nVamos ajustar um modelo de regressão linear com \\(\\boldsymbol{\\beta}_0=\\text{0}_6\\), \\(n_0=0,01,s_0=0,01\\), \\(\\boldsymbol{C}_0=\\lambda\\text{I}_6\\) para diferentes valores de \\(\\lambda\\). Abaixo, apresentamos as estimativas de Bayes para \\(\\boldsymbol{\\beta}\\) com diferentes valores de \\(\\lambda\\). Para verificar como as estimativas são influenciadas por \\(\\lambda\\), vamos representar as estimativas de máxima verossimilhança nas linhas tracejadas.\n\nX &lt;- as.matrix(cbind(1,banco2[,-1]))\ny =  banco2[,1]\n\nbeta_til &lt;- function(lambda) solve( diag(lambda,5)+t(X)%*%X)%*%t(X)%*%y\nbeta_emv &lt;- solve(t(X)%*%X)%*%t(X)%*%y\n\nm = 20\nlambda = seq(.01,m,.01)\nresp = NULL\nfor(l in lambda){\n  resp &lt;- rbind(resp, beta_til(l)[,1])\n}\n\nplot.new()\nplot.window(ylim=c(-5,10), xlim=c(0,m))\nlines(lambda, resp[,2],col=1,lwd=2)\nabline(h=beta_emv[2,1],col=1, lty = 2,lwd=2)\nlines(lambda, resp[,3],col=2,lwd=2)\nabline(h=beta_emv[3,1],col=2, lty = 2,lwd=2)\nlines(lambda, resp[,4],col=3,lwd=2)\nabline(h=beta_emv[4,1],col=3, lty = 2,lwd=2)\nlines(lambda, resp[,5],col=4,lwd=2)\nabline(h=beta_emv[5,1],col=4, lty = 2,lwd=2)\n\naxis(1)\naxis(2)\nlegend('topright',names(banco2)[-1],col=1:4,bty='n',lty=1)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>O modelo de regressão linear normal</span>"
    ]
  },
  {
    "objectID": "regressao.html#a-priori-g-de-zellner",
    "href": "regressao.html#a-priori-g-de-zellner",
    "title": "12  O modelo de regressão linear normal",
    "section": "12.2 A priori \\(G\\) de Zellner",
    "text": "12.2 A priori \\(G\\) de Zellner\nA maior dificuldade na priori conjugada está na dificuldade em introduzir a estrutura de correlação a priori para \\(\\boldsymbol{\\beta}\\). A priori \\(G\\) de Zellner resolve essa questão. Para tanto, observe que \\[\\hat{\\boldsymbol{\\beta}}\\sim\\hbox{Normal}(\\boldsymbol{\\beta},\\phi^{-1}(\\boldsymbol{X}'\\boldsymbol{X})^{-1})\\] A ideia central é utilizar a matriz \\((\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\) para construção da estrutura de covariâncias a priori para \\(\\boldsymbol{\\beta}\\).\nA priori de \\(G\\) de Zellner é dada por \\[\\begin{align}\\boldsymbol{\\beta}|\\phi&\\sim\\hbox{Normal}(\\boldsymbol{\\beta}_0,\\phi^{-1}\\lambda^{-1}(\\boldsymbol{X}'\\boldsymbol{X})^{-1})\\\\f(\\phi)&\\propto \\frac{1}{\\phi}\\end{align}\\] Deste modo ,a posteriori é dada por\n\\[\\begin{align}f(\\boldsymbol{\\beta},\\phi|\\text{dados})&\\propto \\phi^\\frac{n}{2}e^{-\\frac{\\phi}{2}(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})-\\frac{\\phi}{2}SQR}\\times\\\\&|\\phi\\boldsymbol{X}'\\boldsymbol{X}|^{1/2}e^{-\\frac{\\phi}{2}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)'\\lambda(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)]}\\frac{1}{\\phi}\\\\&=\\phi^{\\frac{q}{2}}e^{-\\frac{\\phi}{2}[(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})'(\\boldsymbol{X}'\\boldsymbol{X})(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})+\\lambda(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)'\\boldsymbol{X}'\\boldsymbol{X}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)]}\\phi^{\\frac{n}{2}-1}e^{\\frac{\\phi}{2}SQR}\\end{align}\\] Pode mostrar que o termo na primeira exponencial acima é dado por \\[(\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{\\beta}})'(1+\\lambda)\\boldsymbol{X}'\\boldsymbol{X}(\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{\\beta}})+(\\boldsymbol{\\beta}_0-\\tilde{\\boldsymbol{\\beta}})'\\frac{\\lambda}{1+\\lambda}\\boldsymbol{X}'\\boldsymbol{X}(\\boldsymbol{\\beta}_0-\\tilde{\\boldsymbol{\\beta}})\\] onde \\[\\tilde{\\boldsymbol{\\beta}}=\\frac{\\lambda}{1+\\lambda}\\boldsymbol{\\beta}_0+\\frac{1}{1+\\lambda}\\hat{\\boldsymbol{\\beta}}.\\] Então, \\[\\begin{align}\n\\boldsymbol{\\beta}|\\phi,\\hbox{dados}&\\sim\\hbox{Normal}(\\tilde{\\boldsymbol{\\beta}}, \\phi^{-1}(1+\\lambda)^{-1}(\\boldsymbol{X}'\\boldsymbol{X})^{-1})\\\\\n\\phi|\\text{dados}&\\sim\\hbox{Gama}\\left(\\frac{n}{2},\\frac{s_1}{2}\\right),\n\\end{align}\\] onde \\[s_1=SQR+\\frac{\\lambda}{1+\\lambda}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)'\\boldsymbol{X}'\\boldsymbol{X}^{-1}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_0)\\]\n\nExemplo mtcars\nVamos refazer o problema anterior considerando ainda \\(\\boldsymbol{\\beta}=\\text{0}_5\\) para diferentes valores de \\(\\lambda\\). Nesse caso\n\\[\\tilde{\\boldsymbol{\\beta}}=\\frac{1}{1+\\lambda}\\hat{\\boldsymbol{\\beta}}.\\]\n\nX &lt;- as.matrix(cbind(1,banco2[,-1]))\ny =  banco2[,1]\n\nS = solve(t(X)%*%X)\nbeta_til &lt;- function(lambda) (S%*%t(X)%*%y)/(1+lambda)\n\nm=5\nlambda = seq(.01,4,.01)\nresp = NULL\nfor(l in lambda){\n  resp &lt;- rbind(resp, beta_til(l)[,1])\n}\n\nplot.new()\nplot.window(ylim=c(-5,1), xlim=c(0,m))\nlines(lambda, resp[,2],col=1)\nlines(lambda, resp[,3],col=2)\nlines(lambda, resp[,4],col=3)\nlines(lambda, resp[,5],col=4)\naxis(1)\naxis(2)\nlegend('topright',names(banco2)[-1],col=1:4,bty='n',lty=1)\n\n\n\n\n\n\n\n\nPodemos perceber que a priori \\(G\\) de Zellner é menos sensível que a priori conjugada. Selecionando \\(\\lambda=0,1\\), vamos simular amostras da posteriori e analisar se 0 é um valor plausível para cada \\(\\beta\\).\n\nrequire(mvtnorm)\n\nCarregando pacotes exigidos: mvtnorm\n\n# simulando phi\nS = solve(t(X)%*%X)\nbeta_hat &lt;- S%*%t(X)%*%y\nSQR = t((y-X%*%beta_hat))%*%(y-X%*%beta_hat)\ns1 = SQR+(t(beta_hat)%*%t(X)%*%X%*%beta_hat)*.01/(1+.01)\nn = length(y)\nphi = rgamma(500,n/2, .5*(SQR+drop(s1)))\n\nmu = beta_til(.01)\nbeta = array(NA_real_, c(500,5))\nfor(i in 1:500)beta[i,] = rmvnorm(1, mu,S/(phi[i]*1.01) )\n\noo = par(mfrow=c(2,2))\nplot(density(beta[,2]), main =names(banco2)[2],lwd=2)\nplot(density(beta[,3]), main =names(banco2)[3],lwd=2)\nplot(density(beta[,4]), main =names(banco2)[4],lwd=2)\nplot(density(beta[,5]), main =names(banco2)[5],lwd=2)\n\n\n\n\n\n\n\npar(oo)\n\nObserve que o valor zero está na região de alta densidade para todos os parâmetros, o que contradiz nossa análise exploratória e mostra uma limitação na escolha de \\(\\lambda\\)\n\nO exemplo acima mostrou um comportamento aparentemente contraditório. Ao fazer \\(\\lambda\\) pequeno, espera-se que a inferência seja baseada nos dados. Em contradição, a posteriori favoreceu o modelo nulo, no qual todos os coeficientes da regressão são nulos. Esse comportamento é conhecido como Paradoxo de Lindley e ele ocorre porque a probabilidade a priori da região de interesse (onde os coeficientes são não-nulos) se espalha muito, diminuindo a massa da prior sobre valores onde a verossimilhança é alta.\nUma solução para o Paradoxo de Lindley é",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>O modelo de regressão linear normal</span>"
    ]
  },
  {
    "objectID": "regressao.html#seleção-de-regressoras",
    "href": "regressao.html#seleção-de-regressoras",
    "title": "12  O modelo de regressão linear normal",
    "section": "12.3 Seleção de regressoras",
    "text": "12.3 Seleção de regressoras\nNo exemplo da última sessão, vimos que a distribuição a posteriori do coeficiente da variável cyl possui muita massa em torno de zero, o que deve implicar que esta variável não é relevante para o problema.\nPodemos utilizar o critério de informação do desvio (DIC) para escolher entre um subgrupo de regressoras. Recordemos que os critério de informação são descritos como\n\\[-2\\log L(\\hat{\\boldsymbol{\\beta}},\\hat{\\phi})+2k,\\] onde \\(L()\\) é a função de verossimilhança, \\(\\hat{\\boldsymbol{\\beta}}\\) são estimativas para os coeficientes de regressão, \\(\\hat{\\phi}\\) é uma estimativa para \\(\\phi\\) e \\(k\\) é uma penalidade relacionada ao número de parâmetros. No DIC, temos\\(\\hat{\\boldsymbol{\\beta}}=E(\\boldsymbol{\\beta}|\\boldsymbol{y})\\), \\(\\hat{\\phi}=E(\\phi|\\boldsymbol{y})\\) e \\[k=\\frac{1}{2}Var_{\\boldsymbol{\\beta},\\phi|\\boldsymbol{y}}(\\log L(\\boldsymbol{\\beta},\\phi)).\\]\n\nAlgoritmo: Calculando o DIC via Monte Carlo\n\nSimule \\((\\boldsymbol{\\beta},\\phi)_1,\\ldots,(\\boldsymbol{\\beta},\\phi)_B\\) da distribuição a posteriori de \\((\\boldsymbol{\\beta},\\phi)\\).\nEstime \\(E(\\boldsymbol{\\beta}|\\boldsymbol{x})\\) por \\[\\hat{\\boldsymbol{\\beta}}=\\frac{1}{B}\\sum_{j=1}^B \\boldsymbol{\\beta}j\\] e \\[\\hat{\\phi}=\\frac{1}{B}\\sum_{j=1}^{^B}\\phi_j.\\]\nFaça \\(v_j=-2\\log L(\\boldsymbol{\\beta}_j,\\phi_j)\\). Estime \\(k\\) por \\[k=\\frac{1}{2B}\\sum_{j=1}^B(v_j-\\bar{v})^2\\]\nCalcule o DIC:\n\n\\[DIC=-2\\log L(\\hat{\\boldsymbol{\\beta}},\\hat{\\phi})+2k.\\]\n\n\nmtcars (cont). Vamos calcular o DIC do modelo encontrado no exemplor anterior. Primeiro, vamos implementar a função \\(\\log L\\):\n\n# criando a função log-verossimilhança\nloglik &lt;- function(theta,y,X){\n  m = length(theta)\n  beta= matrix(theta[1:(m-1)], ncol = 1)\n  phi = theta[m]\n  mu = X%*%beta\n  sum( dnorm(y,mu,1/sqrt(phi) , log = T))\n}\n\nVamos calcular o DIC para o modelo com as regressoras cyl, disp, hp, wt.\n\nrequire(mvtnorm)\n# matriz de regressoras (com o intercepto)\nX = as.matrix(cbind(1,banco2[,-1]))\ny = banco2[,1]\nlambda = 5^2\nn = length(y)\nB = 5000\n# simulando phi\nSa = solve(t(X)%*%X)\nbeta_emv &lt;- Sa%*%t(X)%*%y\nSQR = t((y-X%*%beta_emv))%*%(y-X%*%beta_emv)\ns1 = SQR + (t(beta_emv) %*% t(X) %*%X%*% beta_emv )*lambda/(1+lambda)\nphi = rgamma(B,n/2, .5*(SQR+drop(s1)))\n\n# simulando mu\nmu = beta_emv/(1+lambda)\nbeta = array(NA_real_, c(B,5))\nfor(i in 1:B)beta[i,] = rmvnorm(1, mu,Sa/(phi[i]*(1+lambda)) )\n\n# encontrando as estimativas a posteriori\nbeta_hat = colMeans(beta)\nphi_hat  = mean(phi)\n\n# encontrando vi e estimando k\nvi = apply( cbind(beta,phi), 1, function(theta) -2*loglik(theta,y,X))\nk  = var(vi)/2\nk\n\n[1] 3.633099\n\n# calculando o DIC\n-2*loglik(c(beta_hat,phi_hat), y, X) +2*k\n\n[1] 290.3083",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>O modelo de regressão linear normal</span>"
    ]
  }
]