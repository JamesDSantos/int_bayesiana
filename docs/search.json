[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introdução",
    "section": "",
    "text": "2.1 Dados que serão utilizados nesse capítulo\nA amostra abaixo se refere ao número mensal de suicídios registrados no Amazonas nos anos 2021, 2022 e 2023.\nno_suicidios &lt;- c(19, 26, 30, 28, 25, 23,23, 21,\n22, 27, 31, 22, 23, 21, 29, 27, 26, 23,\n36, 27, 24, 21, 18, 22, 34, 27, 26, 26, 34,\n22, 27, 25, 32, 36, 28, 22 )\nDesta amostra, inferimos que a média mensal é de 25,9 registros e que a variância é 21,05.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#notação",
    "href": "intro.html#notação",
    "title": "2  Introdução",
    "section": "2.2 Notação",
    "text": "2.2 Notação\nVariáveis aleatórias cujos valores podem ser observados serão denotadas por letras maiúsculas. Exemplos:\n\n\\(X\\) é o número de acidentes diários na Avenida Torquato Tapajós\n\\(Y\\) é o nível máximo diário do Rio Negro\n\nValores observados de variáveis aleatórias serão denotados pela respectiva letra minúscula.\nParâmetros serão considerados aleatórios, mas serão representados por letras gregas minúsculas, como \\(\\theta\\), \\(\\lambda\\), etc.\nVetores aleatórios serão representados por letras em negrito. Exemplos:\n\n\\(\\mathbf{X} = \\{X_1 , \\ldots , X_n \\}\\) é um vetor de variáveis aleatórias.\n\\(\\mathbf{x} = \\{x_1 ,\\ldots , x_n \\}\\) é um vetor observado de variáveis aleatórias.\n\\(\\theta=\\{\\alpha,\\beta\\}\\) é um vetor de parâmetros.\n\n\nDefinition 2.1 O suporte de uma variável aleatória é o conjunto de todos os seus possíveis valores. Quando necessário, o suporte de variáveis aleatórias é representado pela versão caligráfica de sua letra correspondente.\nExemplos: o suporte de \\(X\\) é \\(\\mathcal{X}\\) ; o suporte de Y é \\(\\mathcal{Y}\\) ; o suporte de \\(Z\\) é \\(\\mathcal{Z}\\).\n\n\nDefinition 2.2 O espaço paramétrico é o conjunto de todos os possíveis valores do parâmetro. Ele é representado pela versão maiúscula da letra grega utilizada para seu respectivo parâmetro.\nExemplo: o espaço paramétrico do parâmetro \\(\\theta\\) é representado por \\(\\Theta\\).\n\nTanto a função de densidade quanto a de probabilidade serão denotadas por funções começando com letras minúsculas. Por exemplo,\n\\[f(x|\\lambda)=\\lambda e^{-\\lambda x}\\] onde \\(x,\\lambda&gt;0\\) é a densidade da distribuição exponencial, enquanto que\n\\[p(x|\\lambda)=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\] com \\(x\\in\\mathbb{N}\\) e \\(\\lambda &gt;0\\) é a função de probabilidade da distribuição Poisson.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#fontes-de-informação",
    "href": "intro.html#fontes-de-informação",
    "title": "2  Introdução",
    "section": "2.3 Fontes de informação",
    "text": "2.3 Fontes de informação\n\n2.3.1 A função de verossimilhança\nSeja \\(\\mathbf{x} = \\{x_1 , \\ldots , x_n \\}\\) uma amostra observada. Supomos que \\(\\mathbf{x}\\) é uma das possíveis amostras das variáveis aleatórias \\(\\mathbf{X} = \\{X_1 , \\ldots , X_n \\}\\). Supomos ainda que \\(X\\sim F (.|\\theta)\\). Assim, condicionada ao conhecimento de \\(\\theta\\), a distribuição da amostra está completamente especificada.\n::: {#def-Funcao de verossimilhanca} Para \\(\\mathbf{x}\\) fixado, a função \\[L:\\Theta\\Rightarrow [0,\\infty)\\] é denominada verossimilhança. :::\nSua interpretação é a seguinte: para \\(\\theta_1,\\theta_2\\in\\Theta\\), se\n\\[L(\\theta_1)&gt;L(\\theta_2),\\] dizemos que \\(\\theta_1\\) é mais verossímil que \\(\\theta_2\\). Isto porque a probabilidade de observar uma amostra na vizinhança de \\(\\mathbf{x}\\) é maior se considerarmos que \\(\\theta_1\\) é o valor do parâmetro. A verossimilhança é uma das fontes de informação utilizada na inferência bayesiana (e a única fonte da inferência frequentista).\n\nExample 2.1 Seja \\(X_i\\) o número de suicídios no \\(i\\)-ésimo mês da amostra e suponha que \\(X_1,\\ldots,X_{36}\\) é uma amostra aleatória proveniente do modelo Poisson(\\(\\theta\\)). Como \\(\\sum_i x_i=933\\), a função de verossimilhança será [L()=_{i=1}{36}e{-36}^{933}.]\nA próxima figura mostra os valores da função de verossimilhança para diversos valores de \\(\\theta\\) para a amostra observada.\n\n# função de verossimilhança\nvero &lt;- function(q){\n  sapply ( q, function(q) prod(dpois(no_suicidios, q)) ) \n} \n\n# gráfico da função de verossimilhança\noo &lt;- par( cex = 1.2)\ncurve( vero(x),22,30, xlab = expression(theta), ylab = expression( L(theta)) , lwd = 2)\n\n\n\n\n\n\n\npar(oo)\n\nPodemos notar que o valores mais verossímeis para \\(\\theta\\) estão entre 24 e 28. Podemos ainda procurar o valor mais verossímil, denominado estimativa de máxima verossimilhança (emv). Pode-se mostrar, utilizando cálculo diferencial, que este valor é equivalente à média amostral. Contudo, com o objetivo de utilizar ao máximo o poder computacional que temos disponível, vamos encontrar esse valor utilizando a função optimize.\n\n# menos o logaritmo da função de verossimilhança\nlvero &lt;- function(q) -log( vero(q))\n\n# encontrando a emv:\noptimise(lvero, c(24,28))\n\n$minimum\n[1] 25.91666\n\n$objective\n[1] 105.4219\n\n\nO valor 25,9 é a estimativa de verossimilhança. Sob o ponto de vista frequentista, esta seria a nossa estimativa para o valor de \\(\\theta\\).\n\n\n\n2.3.2 A distribuição a priori\nSob o ponto de vista bayesiano, a informação existente sobre \\(\\theta\\) antes da observação da amostra deve ser levada em consideração. Isto é feito traduzindo tal informação em termos de probabilidades.\n\nDefinition 2.3 A distribuição de \\(\\theta\\) é denominada distribuição a priori.\n\n\nOs parâmetros da distribuição a priori são denominados hiperparâmetros.\n\nAs distribuições a priori agregam o conhecimento sobre parâmetro antes da observação da amostra (tal conhecimento pode pode vir da expertize dos envolvidos ou ter sido gerado de uma amostra prévia).\nAs prioris podem ser muito ou pouco informativas, dependendo do grau de crença sobre os valores particulares do espaço paramétrico. Em geral isto é feito alterando a variância da distribuição:\n\\[\\hbox{variância}=\\frac{1}{\\hbox{precisão}}\\]\n\nExample 2.2 Nosso objetivo é encontrar uma distribuição a priori para \\(\\theta\\), que representa o número médio de suicídios mensais no Amazonas. Primeiro, vamos obter algumas informações:\n\nEm 2024 foi noticiado que, no Brasil, ocorrem em média 38 suicídios por dia, algo em torno de 1.140 suicídios em um mês.\nPara 2024, a população brasileira estava estimada em 212.600.000, enquanto que a população do Amazonas estava estimada em 4.281.209. Portanto, o Amazonas representa, aproximadamente 2% da população brasileira.\nDeste modo, pode-se inferir (a priori) que, em média, ocorrem 22,8 suicídios mensais no Amazonas.\n\nPodemos então procurar alguma distribuição a priori que reflita essa informação. Por mera conveniência, vamos escolher \\(\\theta\\sim\\hbox{Gama}(a,b)\\), onde \\(E(\\theta)=\\frac{a}{b}=22,8.\\)\nUm especialista em saúde pública poderia argumentar melhor se há motivos para acreditar que essa média deveria ser maior ou não. Como não temos essa informação disponível, podemos refletir esse fato aumentando a variabilidade do modelo. O desvio padrão desta priori é\n\\[\\sqrt{Var(\\theta)}=\\frac{\\sqrt{a}}{b}=\\frac{E(\\theta)}{\\sqrt{a}}=\\frac{22,8}{\\sqrt{a}}.\\]\nVamos escolher esse desvio igual 5. Isto implica que \\[a=\\left(\\frac{22,8}{5}\\right)^2=20,8\\] e \\[b=\\frac{22,8}{20,8}=1,1.\\] Então, nossa informação a priori está traduzida no modelo Gama(20.8,1.1). Abaixo, apresentamos a função densidade desse modelo. Observe que esse modelo traz informações vagas sobre \\(\\theta\\), permitindo que ele assuma valores entre 10 e 30\n\ncurve(dgamma(x,20.8, 1.1), 10,35)\n\n\n\n\n\n\n\n\n\n\n\n2.3.3 Reunindo as fontes de informação - distribuição a posteriori\nSejam \\(f(\\boldsymbol{\\theta})\\) a densidade/função para \\(\\boldsymbol{\\theta}\\) e \\(L(\\boldsymbol{\\theta})\\) a função de verossimilhança.\nComo \\(\\boldsymbol{\\theta}\\) é considerado aleatório, podemos analisar sua distribuição após observar a amostra \\(\\boldsymbol{x}\\), ou seja \\[\\boldsymbol{\\theta}|\\boldsymbol{x}.\\]\nEsta distribuição é denominada \n::: {#thm-Teorema de Bayes}\nSeja \\(\\boldsymbol{x}\\) uma amostra observada. Considere a priori \\(\\theta\\sim f(\\theta)\\) e a verossimilhança \\(L(\\theta)\\). Então a função de densidade (ou probabilidade) de \\(\\theta|\\boldsymbol{x}\\) é dada por \\[f(\\theta|\\boldsymbol{x})=\\frac{L(\\theta)f(\\theta)}{f(\\boldsymbol{x})}.\\] O denominador é denominado distribuição preditiva, sendo igual a \\[f(\\boldsymbol{x})=\\sum_{\\theta\\in \\Theta}L(\\theta)f(\\theta),\\] se \\(\\theta\\) é v.a. discreta ou \\[f(\\boldsymbol{x})=\\int_{\\Theta}L(\\theta)f(\\theta)d\\theta\\] se \\(\\theta\\) é v.a. contínua. :::\n\nExample 2.3 Considerando os dados de suicídios do começo desse capítulo, temos as seguintes fontes de informação:\n\nVerossimilhança: \\[L(\\theta)\\propto e^{-36\\theta}\\theta^{933}\\]\nPriori \\[f(\\theta)\\propto \\theta^{19,8}e^{-1,1\\theta}\\]\n\nA distribuição a posteriori será\n\\[f(\\theta|x_1,\\ldots,x_{36})\\propto \\theta^{933+19,8}e^{-(36+1,1)\\theta}=\\theta^{952,8}e^{-37,1\\theta}.\\] Assim, verificamos que \\(\\theta|x_1,\\ldots,x_{36}\\sim\\hbox{Gama}(953.8,37.1)\\). Na figura abaixo, apresentamos no mesmo gráfico, a função de verossimilhança, priori e posteriori (fazendo as mudanças de escala necessárias). Observe que a priori é mais dispersa que a verossimilhança. Essa, por sua vez, restringe os valores de \\(\\theta\\) que eram prováveis a priori. O resutaldo é uma distribuição a posteriori próxima da função de verossimilhança.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#inferência-estatística",
    "href": "intro.html#inferência-estatística",
    "title": "2  Introdução",
    "section": "2.4 Inferência estatística",
    "text": "2.4 Inferência estatística\nDenominamos por estatística qualquer função da amostra. Utilizamos estatísticas para fazer as seguintes inferências:\n\nEstimação pontual: trata-se de uma estatística com o objetivo de inferir o valor de \\(\\theta\\). Tal estatística é denominada estimador.\nEstimação por região: trata-se de uma estatística, digamos \\(T(\\boldsymbol{X})\\), com o objetivo de cobrir o valor de \\(\\theta\\), ou seja, fazer a inferência \\(\\theta\\in T(\\boldsymbol{X})\\). As estimações intervalares são as mais comuns, nas quais \\(T(\\boldsymbol{X})=(L(\\boldsymbol{X}),U(\\boldsymbol{X}))\\).\nTestes de hipóteses: são estatísticas construídas para decidir se aceitamos a afirmação (hipótese) \\(H:\\theta\\in \\Theta_0\\), onde \\(\\Theta_0\\) é um subconjunto de \\(\\Theta\\) conhecido por hipótese.\n\nNote que a distribuição a posteriori é função da amostra. Assim, toda função desta distribuição é uma estatística. Assim:\n\nEstimação pontual: em geral é uma medida que representa a região de alta densidade (ou probabilidade) da posteriori. A média da posteriori, assim como a mediana ou a moda são escolhas comuns.\nEstimação por regiões: em geral procuramos por uma região \\(T\\) da posteriori que satisfaça \\(P(\\theta\\in T(\\boldsymbol{x})|\\boldsymbol{x})=\\gamma\\), onde \\(\\gamma\\) é denominado nível de credibilidade (não confundir com nível de confiança)\nTestes de hipóteses: em geral, aceitamos \\(H:\\theta\\in\\Theta_0\\) se \\(P(H|\\boldsymbol{x})\\) é elevada.\n\n\nExample 2.4 Para o nosso exemplo, obtivemos \\(\\theta|\\boldsymbol{x}\\sim\\hbox{Gama}(953.8,37.1)\\). Então, uma estimativa pontual para \\(\\theta\\) é\n\\[E(\\theta|\\boldsymbol{x})=\\frac{953,8}{37,1}=25,7\\] registros de suicídios mensais.\nPodemos obter os quantis de 2,5% e 97,5% para construir um intervalo com 95% de credibilidade:\n\nqgamma( c(.025,.975), 953.8, 37.1)\n\n[1] 24.10301 27.36583\n\n\nlogo, inferimos que \\(\\theta\\in(24.1,27.36)\\) com probabilidade 0,95.\nPor último, recorde-se que discutimos, utilizando apenas os dados nacionais, que \\(\\theta\\) deveria está próximo de 22,8. Após observa a amostra, existem evidências de que \\(H:\\theta&gt;22,8\\)? Para responder a esse questionamento, podemos calcular\n\\[P(H|\\boldsymbol{x})=P(\\theta&gt;22.8|\\boldsymbol{x})\\]\n\n1-pgamma(22.8, 953.8, 37.1)\n\n[1] 0.9998554\n\n\nlogo, com uma probabilidade maior que 99,9%, existem fortes evidências de que o número médio mensal de suicídios no Amazonas é maior do que 22,8 registros por mês.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#exercício",
    "href": "intro.html#exercício",
    "title": "2  Introdução",
    "section": "2.5 Exercício",
    "text": "2.5 Exercício\n\nÉ fato que a taxa de suicídio é maior entre os homens, embora a tentativa seja maior entre as mulheres. Uma das explicações está no fato de que os homens tendem a utilizar métodos mais letais para o suicídio, como armas de fogo, enquanto as mulheres são mais propensas a utilizar métodos menos letais, como overdose de medicamentos.\nDados históricos, entre 1996 e 2017, sugerem que 79% dos suicídios são cometidos por homens.\nSeja \\(\\rho\\) a probabilidade de que um suicídio seja cometido por alguém do sexo masculino no Estado do Amazonas. Considere a priori \\(\\rho\\sim\\hbox{Beta}(a,b)\\), onde sabemos que\n\\[f(\\rho)\\propto \\rho^{a-1}(1-\\rho)^{b-1},\\] \\[E(\\rho)=\\frac{a}{a+b},\\] e \\[Var(\\rho)=\\frac{E(\\rho)(1-E(\\rho))}{a+b+1}.\\]\n\nEncontre valores de \\(a\\) e \\(b\\) que reflitam a média de 0,75 mas que não restrinjam demais os valores possíveis para \\(\\rho\\)\nNo Amazonas, entre os anos 2021 e 2023, dos 933 registros de suicídios, 726 foram cometidos por homens. Considere então o modelo \\(X|\\rho\\sim\\hbox{Binomial}(933,\\rho)\\). Mostre que \\[f(\\theta|x)\\propto \\rho^{726+a-1}(1-\\rho)^{207+b-1}\\] e conclua que \\(\\rho|x\\sim\\hbox{Beta}(a+933,b+207)\\).\nEstime \\(\\theta\\) e construa um intervalo de 95% de credibilidade\nTeste a hipótese de que, para o Amazonas, a probabilidade de um indivíduo do sexo masculino cometer suicídio é maior do que 79%.\nPara problemas envolvendo proporções, é comum o uso a priori \\(\\rho\\sim\\hbox{Uniforme}(0,1)\\), que é equivalente à \\(\\rho\\sim\\hbox{Beta}(1,1)\\). Refaça este exercício com essa priori e discuta sobre as diferenças encontradas tanto nas estimações quanto no teste de hipóteses.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#resumo-da-aula-1",
    "href": "intro.html#resumo-da-aula-1",
    "title": "2  Introdução",
    "section": "2.6 Resumo da aula 1",
    "text": "2.6 Resumo da aula 1\n\nExistem duas fontes de informação na inferência bayesiana: os dados (verossimilhança) e a informação anterior (priori)\nA informação a priori é subjetiva: pessoas diferentes têm prioris diferentes\nO Teorema de Bayes combina as duas fontes em uma nova informação, dada pela distribuição \nOs objetivos da inferência (estimação e testes) são feitos a partir da distribuição a posteriori",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "normal.html",
    "href": "normal.html",
    "title": "6  O modelo normal",
    "section": "",
    "text": "6.1 A distribuição normal-gama\nDizemos que \\((X,Y)\\sim NG(\\mu,n_0,\\nu,d^2)\\) (lê-se normal-gama) se sua função densidade conjunta é dada por\n\\[f(x,y)\\propto y^{\\frac{\\nu+1}{2}-1}\\exp\\left\\{-\\frac{y}{2}n_0\\left[(x-\\mu)^2 + d^2\\right]\\right\\}\\]\nonde \\(\\mu,x\\in\\mathbb{R}\\) e \\(d,y,c\\in\\mathbb{R}_+\\). Colocando os termos que não dependem de \\(x\\) junto com a constante de proporcionalidade, podemos mostrar que\n\\[f(x|y)\\propto \\exp\\left\\{-\\frac{y}{2}n_0(x-\\mu)^2\\right\\}\\] ou seja, \\(X|y\\sim\\hbox{Normal}(\\mu,y^{-1}/n_0)\\). Além disso, integrando \\(f(x,y)\\) em \\(x\\), mostramos que\n\\[f(y)\\propto y^{\\frac{\\nu+1}{2}-1}e^{-\\frac{yn_0d^2}{2}}\\int_{\\mathbb{R}}\\exp\\left\\{-\\frac{y}{2}\\left[n_0(x-\\mu)^2\\right]\\right\\}d\\mu\\propto y^{\\frac{\\nu}{2}-1}e^{-\\frac{n_0d^2}{2}y}\\] ou seja, \\(Y\\sim\\hbox{Gama}(\\nu/2, n_0d^2/2)\\). Por último, integrando \\(f(x,y)\\) em \\(y\\) teremos\n\\[\\begin{align}f(x)&\\propto \\int_0^\\infty y^{\\frac{\\nu+1}{2}-1}\\exp\\left\\{-\\frac{y}{2}n_0\\left[(x-\\mu)^2 + d^2\\right]\\right\\}dy \\\\&\\propto \\Gamma\\left(\\frac{\\nu+1}{2}\\right)\\left\\{1+\\frac{\\nu}{d^2}\\frac{(x-\\mu)^2}{\\nu}\\right\\}^{-\\frac{\\nu+1}{2}}\\end{align}\\] ou seja, \\(X\\sim t_{\\nu}(\\mu, d^2/\\nu)\\). Em especial, se \\(\\nu&gt;1\\) então \\(E(X)=\\mu\\) e, se \\(\\nu&gt;2\\) teremos que \\[Var(X)=\\frac{d^2}{\\nu-2}\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>O modelo normal</span>"
    ]
  },
  {
    "objectID": "normal.html#a-função-de-verossimilhança",
    "href": "normal.html#a-função-de-verossimilhança",
    "title": "6  O modelo normal",
    "section": "6.2 A função de verossimilhança",
    "text": "6.2 A função de verossimilhança\nSeja \\(X_1,\\ldots,X_n\\) uma amostra aleatória do modelo \\(X|\\mu,\\phi\\sim\\hbox{Normal}(\\mu,\\phi^{-1})\\), onde \\(\\phi\\), denominado precisão, é o inverso da variância. A função de verossimilhança deste modelo pode ser escrita como\n\\[L(\\mu,\\phi)\\propto \\phi^{\\frac{n}{2}}\\exp\\left\\{-\\frac{n\\phi}{2}(\\bar{x}-\\mu)^2 -\\frac{ns^2\\phi}{2}\\right\\}\\] onde \\[s^2=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar{x})^2\\] é a estimativa de máxima verossimilhança para \\(\\phi^{-1}\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>O modelo normal</span>"
    ]
  },
  {
    "objectID": "normal.html#posteriori-com-prioris-impróprias",
    "href": "normal.html#posteriori-com-prioris-impróprias",
    "title": "6  O modelo normal",
    "section": "6.3 Posteriori com prioris impróprias",
    "text": "6.3 Posteriori com prioris impróprias\nConsiderando as prioris impróprias \\(\\pi(\\phi)\\propto \\phi^{-1}\\), \\(\\pi(\\mu)\\propto 1\\) e que \\(\\pi(\\mu,\\phi)=\\pi(\\mu)\\pi(\\phi)\\), teremos que\n\\[\\pi(\\mu,\\phi|\\boldsymbol{x})\\propto \\phi^{\\frac{n}{2}-1}\\left\\{-\\frac{\\phi}{2}n\\left[ (\\bar{x}-\\mu)^2 +s^2\\right]\\right\\}\\] ou seja, \\(\\mu,\\phi|\\boldsymbol{x}\\sim\\hbox{NG}(\\bar{x},n,n-1,s^2)\\), o que implica em:\n\\[\\begin{align}\n\\mu|\\phi,\\boldsymbol{x}&\\sim\\hbox{Normal}\\left(\\bar{x},\\frac{\\phi^{-1}}{n}\\right)\\\\\n\\phi|\\boldsymbol{x}&\\sim\\hbox{Gama}\\left(\\frac{n-1}{2},\\frac{ns^2}{2}\\right)\\\\\n\\mu|\\boldsymbol{x}&\\sim t_{n-1}\\left(\\bar{x},\\frac{s^2}{n-1}\\right)\n\\end{align}\\]\nDisto, teremos que\n\n\n\nParâmetro\nEstimativa\nErro\n\n\n\n\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n-3}}\\)\n\n\n\\(\\phi\\)\n\\(\\frac{n-1}{ns^2}\\)\n\\(\\frac{\\sqrt{2(n-1)}}{s^2n}\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>O modelo normal</span>"
    ]
  },
  {
    "objectID": "normal.html#posteriori-com-a-priori-de-jeffreys",
    "href": "normal.html#posteriori-com-a-priori-de-jeffreys",
    "title": "6  O modelo normal",
    "section": "6.4 Posteriori com a priori de Jeffreys",
    "text": "6.4 Posteriori com a priori de Jeffreys\nO logaritmo da função de verossimilhança é\n\\[l(\\mu,\\phi)=\\frac{n}{2}\\log\\phi -\\frac{n}{2}\\phi\\left[(\\bar{x}-\\mu)^2 + s^2\\right]\\]\nAs derivadas de primeira ordem em \\(\\mu\\) e \\(\\phi\\) são \\[\\begin{align}\n\\frac{\\partial}{\\partial \\mu}l(\\mu,\\phi)&=n\\phi(\\bar{x}-\\mu)\\\\\n\\frac{\\partial}{\\partial \\phi}l(\\mu,\\phi)&=\\frac{n}{2\\phi}-\\frac{n}{2}\\left[(\\bar{x}-\\mu)^2 + s^2\\right]\\\\\n\\end{align}\\]\ne as de segunda ordem são \\[\\begin{align}\n\\frac{\\partial^2}{\\partial \\mu^2}l(\\mu,\\phi)&=-n\\phi\\\\\n\\frac{\\partial^2}{\\partial \\phi^2}l(\\mu,\\phi)&=-\\frac{n}{2\\phi^2}\\\\\n\\frac{\\partial^2}{\\partial \\mu\\partial \\phi}l(\\mu,\\phi)&=0\\\\\n\\end{align}\n\\] logo, a matriz de informação de Fisher é \\[\\mathcal{I}_n(\\mu,\\phi)=n\\left[\\begin{array}{cc}\\phi & 0 \\\\0 & \\frac{1}{2\\phi^2}\\end{array}\\right],\\] e a priori de Jeffreys é \\[\\pi(\\mu,\\phi)\\propto \\sqrt{|\\mathcal{I}_n(\\mu,\\phi)|}=\\phi^{-1/2},\\] que implica na posteriori\n\\[\\pi(\\mu,\\phi|\\boldsymbol{x})\\propto \\phi^{\\frac{n+1}{2}-1}\\left\\{-\\frac{n\\phi}{2}\\left[(\\bar{x}-\\mu)^2 +s^2 \\right]\\right\\}\\] ou seja, \\(\\mu,\\phi|\\boldsymbol{x}\\sim\\hbox{NG}(\\bar{x},n,n,s^2)\\), o que implica em:\n\\[\\begin{align}\n\\mu|\\phi,\\boldsymbol{x}&\\sim\\hbox{Normal}\\left(\\bar{x},\\frac{\\phi^{-1}}{n}\\right)\\\\\n\\phi|\\boldsymbol{x}&\\sim\\hbox{Gama}\\left(\\frac{n}{2},\\frac{ns^2}{2}\\right)\\\\\n\\mu|\\boldsymbol{x}&\\sim t_{n}\\left(\\bar{x},\\frac{s^2}{n}\\right)\n\\end{align}\\]\nDisto, teremos que\n\n\n\nParâmetro\nEstimativa\nErro\n\n\n\n\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n-2}}\\)\n\n\n\\(\\phi\\)\n\\(\\frac{1}{s^{2}}\\)\n\\(\\frac{\\sqrt{2}}{s^2\\sqrt{n}}\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>O modelo normal</span>"
    ]
  },
  {
    "objectID": "normal.html#posteriori-para-a-priori-conjugada",
    "href": "normal.html#posteriori-para-a-priori-conjugada",
    "title": "6  O modelo normal",
    "section": "6.5 Posteriori para a priori conjugada",
    "text": "6.5 Posteriori para a priori conjugada\nConsidere que \\((\\mu,\\phi)\\sim \\hbox{NG}(m_0,n_0,\\nu_0,s^2_0)\\). Esta priori é conjugada para o modelo normal, uma vez que\n\\[\\begin{align}\n\\pi(\\mu,\\phi|\\boldsymbol{x})&\\propto \\phi^{\\frac{n}{2}}\\exp\\left\\{-\\frac{\\phi}{2}n\\left[(\\bar{x}-\\mu)^2+ s^2\\right]\\right\\}\\phi^{\\frac{\\nu_0+1}{2}-1}\\exp\\left\\{-\\frac{\\phi}{2}n_0\\left[(\\mu-m_0)^2 + s_0^2\\right]\\right\\}\\\\\n&\\phi^{\\frac{\\nu_0+n}{2}-1}\\exp\\left\\{-\\frac{\\phi}{2}\\left[n(\\bar{x}-\\mu)^2 + n_0(\\mu-m_0)^2+ns^2 + n_0s^2_0\\right]\\right\\}\\end{align}.\\] Como \\[n(\\bar{x}-\\mu)^2 +n_0(\\mu-m_0)^2 = (n+n_0)(\\mu-m_1)^2+\\frac{n n_0}{n+n_0}(\\bar{x}-m_0)^2\\] onde \\[\\begin{align}\nm_1&=\\frac{n}{n+n_0}\\bar{x}+\\frac{n_0}{n+n_0}m_0\n\\end{align},\\] teremos \\[\\begin{align}\n\\pi(\\mu,\\phi|\\boldsymbol{x})&\\propto \\phi^{\\frac{\\nu_1+1}{2}-1}\\exp\\left\\{-\\frac{\\phi}{2}n_1\\left[(\\mu-m_1)^2 + d_1^2\\right]\\right\\}\\end{align},\\] onde \\[\\begin{align}\n\\nu_1&=\\nu_0+n\\\\\nn_1&=n_0+n\\\\\nm_1&=\\frac{n}{n1}\\bar{x}+\\frac{n_0}{n_1}m_0\\\\\nd_1^2& = \\frac{n_0n}{n_1^2}(\\bar{x}-m_0)^2+\\frac{n}{n_1}s^2 + \\frac{n_0}{n_1}s^2_0\n\\end{align}\\] ou seja, \\(\\mu,\\phi|\\boldsymbol{x}\\sim\\hbox{NG}(m_1,n+n_0,\\nu_0+n,d_1^2)\\), o que implica em:\n\\[\\begin{align}\n\\mu|\\phi,\\boldsymbol{x}&\\sim\\hbox{Normal}\\left(m_1,\\frac{\\phi^{-1}}{n+n_0}\\right)\\\\\n\\phi|\\boldsymbol{x}&\\sim\\hbox{Gama}\\left(\\frac{n+\\nu_0}{2},\\frac{(n+n_0)d_1^2}{2}\\right)\\\\\n\\mu|\\boldsymbol{x}&\\sim t_{n}\\left(m_1,\\frac{d^2_1}{\\nu_0+n}\\right)\n\\end{align}\\]\nDisto, teremos que\n\n\n\n\n\n\n\n\nParâmetro\nEstimativa\nErro\n\n\n\n\n\\(\\mu\\)\n\\(\\frac{n}{n+n_0}\\bar{x}+\\frac{n_0}{n+n_0}m_0\\)\n\\(\\frac{d_1}{\\sqrt{n+\\nu_0-2}}\\)\n\n\n\\(\\phi\\)\n\\(\\frac{n+\\nu_0}{(n+n_0)d_1^2}\\)\n\\(\\frac{\\sqrt{2(n+\\nu_0)}}{d_1^2(n+n_0)}\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>O modelo normal</span>"
    ]
  },
  {
    "objectID": "normal.html#detecção-de-outliers",
    "href": "normal.html#detecção-de-outliers",
    "title": "6  O modelo normal",
    "section": "6.6 Detecção de outliers",
    "text": "6.6 Detecção de outliers",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>O modelo normal</span>"
    ]
  },
  {
    "objectID": "negativeBinomial.html",
    "href": "negativeBinomial.html",
    "title": "7  Binomial negativa",
    "section": "",
    "text": "7.1 O modelo binomial negativo\nA distribuição Poisson é muito comum em problemas de contagem. Como sua esperança e variância são iguais, o termo sobredispersão foi cunhado na literatura como uma variância maior que a média, o que seria indício de que o modelo Poisson não é adequado (de modo análogo, há o conceito de subdispersão, mas não é um fenômeno comum).\nDizemos que \\(X|\\rho,\\phi\\sim\\hbox{Binomial Negativa}\\) se\n\\[p(x|\\rho,\\phi)=\\frac{\\Gamma(\\phi+x)}{x!\\Gamma(\\phi)}\\rho^\\phi(1-\\rho)^x,\\] onde \\(x\\in\\mathbb{N}\\), \\(\\rho\\in(0,1)\\) e \\(\\phi&gt;0\\).\nExistem diversos motivos para considerar o modelo binomial negativo uma alternativa quando o modelo Poisson não parece ser adequado. Primeiro, temos que \\(E(X|\\rho,\\phi)=\\phi(1-\\rho)/\\rho\\) e \\(Var(X|\\rho,\\phi)=E(X|\\rho,\\phi)/\\rho\\), logo, a sobredispersão está presente no modelo. Além disso, se \\(X|\\lambda\\sim\\hbox{Poisson}(\\lambda)\\) e \\(\\lambda\\sim\\hbox{Gama}(\\phi, \\rho/(1-\\rho))\\), então \\(X|\\phi,\\rho\\sim\\hbox{Binomial Negativa}(\\phi,\\rho)\\) logo, este modelo é uma mistura do modelo Poisson. Por último, fazendo \\[\\mu=\\phi\\frac{1-\\rho}{\\rho}\\Rightarrow \\rho(\\phi)=\\frac{\\phi}{\\phi+\\mu},\\] pode-de mostrar que \\[\\lim_{\\phi\\rightarrow\\infty}p(x|\\phi)=\\frac{e^{-\\mu}\\mu^x}{x!}\\] ou seja, o modelo Poisson também pode ser vist como um caso limite do binomial negativo.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binomial negativa</span>"
    ]
  },
  {
    "objectID": "negativeBinomial.html#priori-para-phi-condicionado",
    "href": "negativeBinomial.html#priori-para-phi-condicionado",
    "title": "7  Binomial negativa",
    "section": "7.2 Priori para \\(\\phi\\) condicionado",
    "text": "7.2 Priori para \\(\\phi\\) condicionado\nQuando \\(\\phi\\) é conhecido, a verossimilhança do modelo se torna\n\\[L(\\rho|\\phi)\\propto \\rho^{n\\phi}(1-\\rho)^{\\sum_{i=1}^n x_i},\\] logo, o modelo Beta\\((a,b)\\) é conjugado, com a posteriori dada por \\[\\rho|\\boldsymbol{x},\\phi\\sim\\hbox{Beta}\\left(n\\phi+a,\\sum_{i=1}^n x_i+b\\right).\\]\nA priori de Jeffreys é dada por\n\\[\\pi(\\rho)\\propto \\frac{1}{\\rho(1-\\rho)^{1/2}},\\] o que implica na posteriori \\[\\rho|\\boldsymbol{x},\\phi\\sim\\hbox{Beta}\\left(n\\phi,\\sum_{i=1}^n x_i+\\frac{1}{2}\\right).\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binomial negativa</span>"
    ]
  },
  {
    "objectID": "negativeBinomial.html#priori-para-phi",
    "href": "negativeBinomial.html#priori-para-phi",
    "title": "7  Binomial negativa",
    "section": "7.3 Priori para \\(\\phi\\)",
    "text": "7.3 Priori para \\(\\phi\\)\nSeja \\(\\pi(\\phi)\\pi(\\rho)\\) a priori para \\((\\phi,\\rho)\\). Então, teremos que\n\\[\\pi(\\phi,\\rho|\\boldsymbol{x})\\propto \\frac{\\prod_{i=1}^n\\Gamma(\\phi+x_i)}{\\Gamma(\\phi)^n}\\rho^{n\\phi}(1-\\rho)^{\\sum_{i=1}^n x_i}\\pi(\\phi)\\pi(\\rho).\\]\nAssumindo qualquer uma das prioris da seção anterior, teremos\n\\[\\pi(\\phi,\\rho|\\boldsymbol{x})\\propto \\frac{\\prod_{i=1}^n\\Gamma(\\phi+x_i)}{\\Gamma(\\phi)^n}B\\left(a_0+n\\phi,b_0+\\sum_{i=1}^nx_i\\right)\\pi(\\phi)\\pi(\\rho|\\phi,\\boldsymbol{x}),\\]\nlogo,\n\\[\\pi(\\phi|\\boldsymbol{x})\\propto \\frac{\\prod_{i=1}^n\\Gamma(\\phi+x_i)}{\\Gamma(\\phi)^n}B\\left(a_0+n\\phi,b_0+\\sum_{i=1}^nx_i\\right)\\pi(\\phi)\\]\nComo a posteriori de \\(\\phi\\) não é uma distribuição conhecida, precisamos construir um simulador. O algoritmo Metropolis-Hastings é uma boa escolha, uma vez que a constante de proporcionalidade da densidade é desconhecida.\n\nAlgoritmo Metropolis-Hastings\nO Metropolis-Hastings simula se utiliza de uma distribuição que sabemos simular (denominada proposta) para gerar uma cadeia de Markov cuja distribuição estacionária é a distribuição de interesse.\nNa \\(j\\)-ésima itereção, a simulação do valor proposto \\(\\phi^*\\) é baseada no valor atual da cadeia, \\(\\phi^{(j-1)}\\). Como \\(\\phi&gt;0\\), a proposta \\(\\phi^*\\sim \\hbox{Gamma}(\\tau\\phi^{(j-1)},\\tau)\\) é adequada uma vez que \\[E(\\phi^*)=\\phi^{(j-1)}\\] e \\[\\sqrt{Var(\\phi^*)}=\\frac{\\phi^{(j-1)}}{\\tau}\\] Acima, \\(\\tau\\) é denominado tunning (afinação em tradução livre) e deve ser escolhido para que a cadeia tenha o número de aceites da proposta controlado (algo em torno de 23% ).\nAbaixo, segue o algoritmo\n\nFaça \\(j=0\\) e escolha um valor para \\(\\phi^{(0)}\\) (a estimativa de máxima verossimilhança, por exemplo). Faça um contador de aceites, começando com \\(k=0\\).\nPara o passo \\(j\\):\n\n\nSimule \\(\\phi^*\\sim\\hbox{Gama}(\\tau\\phi^{j-1},\\tau)\\)\nCalcule\n\n\\[prob = \\frac{\\pi(\\phi^*|\\boldsymbol{x})}{\\pi(\\phi^{(j-1)}|\\boldsymbol{x})}\\frac{g(\\phi^{(j-1)}|\\tau\\phi^*,\\tau)}{g(\\phi^*|\\tau\\phi^{(j-1)},\\tau)},\\] onde \\(g(.|a,b)\\) é a função densidade do modelo gama. + Simule \\(u\\sim\\hbox{Uniforme}(0,1)\\). Se \\(u&lt;prob\\), faça \\(\\phi^{(j)}=\\phi^*\\) e \\(k=k+1\\) (houve um aceite). Senão, faça \\(\\phi^{(j)}=\\phi^{(j-1)}\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binomial negativa</span>"
    ]
  },
  {
    "objectID": "aproximacaoNormal.html",
    "href": "aproximacaoNormal.html",
    "title": "8  Aproximação normal e seu uso com o Metropolis-Hastings",
    "section": "",
    "text": "8.1 Aproximação da posteriori pela distribuição normal\nAssuma que \\(\\boldsymbol{\\theta}\\in\\mathbb{R}^q\\). Seja \\(\\ell(\\boldsymbol{\\theta})=\\log L(\\boldsymbol{\\theta})\\) a função log-verossimilhança e \\(\\hat{\\boldsymbol{\\theta}}\\) a estimativa de máxima verossimilhaça para \\(\\boldsymbol{\\theta}\\). Considere a seguinte aproximação de \\(\\ell(\\boldsymbol{\\theta})\\) em séries de Taylor\n\\[\\ell(\\boldsymbol{\\theta})\\approx  \\ell(\\hat{\\boldsymbol{\\theta}})+\\frac{1}{2}(\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}})'\\mathcal{H}(\\hat{\\boldsymbol{\\theta}})(\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}})\\] onde \\(\\boldsymbol{\\theta}\\) é a matriz hessiana (de derivadas segunda) aplicada em \\(\\hat{\\boldsymbol{\\theta}}\\). Deste modo, teremos que \\[\\pi(\\boldsymbol{\\theta}|\\boldsymbol{x})\\propto \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}})'\\left[-\\mathcal{H}(\\hat{\\boldsymbol{\\theta}})\\right](\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}})\\right\\}\\pi(\\boldsymbol{\\theta})\\]\nUtilizando a priori imprópria \\(\\pi(\\boldsymbol{\\theta})\\), temos que \\(\\boldsymbol{\\theta}|\\boldsymbol{x}\\approx \\hbox{Normal}(\\hat{\\boldsymbol{\\theta}},-\\mathcal{H}(\\hat{\\boldsymbol{\\theta}})^{-1})\\).\nNote que as informações necessárias para a aproximação da posteriori acima podem ser obtidas via função optim.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Aproximação normal e seu uso com o Metropolis-Hastings</span>"
    ]
  },
  {
    "objectID": "aproximacaoNormal.html#aproximação-da-posteriori-pela-distribuição-normal",
    "href": "aproximacaoNormal.html#aproximação-da-posteriori-pela-distribuição-normal",
    "title": "8  Aproximação normal e seu uso com o Metropolis-Hastings",
    "section": "",
    "text": "Exemplo  A amostra abaixo foi simulada do modelo Gama\\((\\alpha,\\beta)\\) (o valor dos parâmetros foram omitidos de propósito)\n\nx\n\n [1] 0.2769550 1.1902521 1.1543901 0.6836040 1.2951363 0.8468467 0.7626888\n [8] 0.3830976 0.2270072 0.2785412 0.3853067 0.4818242 0.2021683 0.8914625\n[15] 0.7718524 0.9455476 0.8702839 0.5309044 1.2858882 1.0415047\n\n\nComo \\(\\alpha,\\beta&gt;0\\), considere que \\(\\alpha=\\exp\\{\\theta_1\\}\\) e \\(\\beta=\\exp\\{\\theta_2\\}\\) (deste modo, \\(\\boldsymbol{\\theta}\\in\\mathbb{R}^2\\)).\nA função de log-verossimilhança deste modelo é\n\nlogveross &lt;- function(theta){ sum(dgamma(x, exp(theta[1]), exp(theta[2]), log = T))\n}\n\nPodemos utilizar a função optim para obter as estimativas de máxima verossimilhança e a matriz hessiana. Contudo, primeiro devemos observar que esta função é um minimizador, logo, queremos que \\(\\boldsymbol{\\theta}\\) que minimize \\(-\\ell({\\boldsymbol{\\theta}})\\).\n\nopt &lt;- optim( c(0,0), function(q) -logveross(q), hessian = T)\nopt\n\n$par\n[1] 1.245897 1.567152\n\n$value\n[1] 7.435047\n\n$counts\nfunction gradient \n      65       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n          [,1]      [,2]\n[1,]  80.46195 -69.52104\n[2,] -69.52104  69.52342\n\n\nNo objeto opt, a lista par é o vetor com as estimativas de máxima verossimilhança, enquanto que hessian é o valor de \\(-\\mathcal{H}(\\hat{\\boldsymbol{\\theta}})\\).\nA inversa de opt$hessian vai dar a matriz de covariância entre \\(\\theta_1\\) e \\(\\theta_2\\) a posteriori.\n\nSigma &lt;- solve(opt$hessian)\nSigma\n\n           [,1]       [,2]\n[1,] 0.09138023 0.09137711\n[2,] 0.09137711 0.10575762\n\n\nAgora, podemos simular \\(\\theta_1\\) e \\(\\theta_2\\) a posteriori:\n\nrequire(mvtnorm)\n\nCarregando pacotes exigidos: mvtnorm\n\n\nWarning: pacote 'mvtnorm' foi compilado no R versão 4.4.3\n\ntheta_sim &lt;- rmvnorm(500, opt$par, Sigma)\n\nPor último, podemos fazer inferências sobre \\(\\alpha=\\exp\\{\\theta_1\\}\\) e \\(\\beta=\\exp\\{\\theta_2\\}\\):\n\n# intervalos de credibilidade para alfa\nquantile(exp(theta_sim[,1]), c(.025,.975))\n\n    2.5%    97.5% \n1.978850 6.350455 \n\n# intervalos de credibilidade para beta\nquantile(exp(theta_sim[,2]), c(.025,.975))\n\n    2.5%    97.5% \n2.582863 8.865732",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Aproximação normal e seu uso com o Metropolis-Hastings</span>"
    ]
  },
  {
    "objectID": "aproximacaoNormal.html#metropolis-revisitado",
    "href": "aproximacaoNormal.html#metropolis-revisitado",
    "title": "8  Aproximação normal e seu uso com o Metropolis-Hastings",
    "section": "8.2 Metropolis revisitado",
    "text": "8.2 Metropolis revisitado\nA diferença entre o algoritmo Metropolis e o Metropolis-Hastings está na escolha da distribuição proposta. No primeiro, a proposta é simétrica, \\[g(x|y)=g(y|x).\\] Com isso, teremos que \\[\\frac{f(x)}{f(y)}\\frac{g(y|x)}{g(x|y)}=\\frac{f(x)}{f(y)}\\] e a probabilidade de aceitação da cadeia é baseada somente na distribuição alvo \\(f\\).\nNo algoritmo Metropolis, é comum escolher a distribuição proposta como sendo uma normal. Uma escolha razoável é utilizar como proposta aproximação normal vista na seção anterior.\n\nExemplo  Consideremos novamente a amostra do exemplo anterior. A função de verossimilhança é \\[L(\\theta)=\\prod_{i=1}^n \\frac{\\beta(\\theta_2)^{\\alpha(\\theta_1)}}{\\Gamma(\\alpha(\\theta_1))} x_i^{\\alpha(\\theta_1)-1}e^{-\\beta(\\theta_2)x_i}\\] onde \\(\\alpha(\\theta_1)=e^{\\theta_1}\\), \\(\\beta(\\theta_2)=e^{\\theta_2}\\). Além disso ,considere ad prioris independentes \\(\\theta_i\\sim\\hbox{Normal}(0,100)\\). Então, devemos simular do modelo\n\\[\\pi(\\theta|\\boldsymbol{x})\\propto \\left[\\frac{\\beta(\\theta_2)^{\\alpha(\\theta_1)}}{\\Gamma(\\alpha(\\theta_1))}\\right]^n \\left[\\prod_{i=1}^n x_i\\right]^{\\alpha(\\theta_1)}e^{-\\beta(\\theta_2)\\sum_{i=1}^{n}x_i}e^{-\\frac{1}{200}(\\theta_1^2 + \\theta_2^2)}\\]\nA posteriori aproximada, que encontramos no exemplo anterior é \\[\\boldsymbol{\\theta}|\\boldsymbol{x}\\approx N \\left[ \\left(\\begin{array}{c}1,24\\\\1,56 \\end{array}\\right),\\left(\\begin{array}{cc}0,09 & 0,09\\\\0,09 &0,11\\end{array}\\right)\\right]\\]\nVamos aproveitar a estrutura de covariâncias acima para usar a proposta\n\\[\\boldsymbol{\\theta}^*|\\boldsymbol{x}\\sim N \\left[ \\boldsymbol{\\theta}^{(j-1)},\\tau\\left(\\begin{array}{cc}0,09 & 0,09\\\\0,09 &0,11\\end{array}\\right)\\right]\\] onde \\(\\boldsymbol{\\theta}^*\\) é o candidato gerado e \\(\\boldsymbol{\\theta}^{(j)}\\) é o estado atual da cadeia e \\(\\tau\\) é o tunning da cadeia.\n\nB &lt;- 10000 # número de iterações\ntheta &lt;- array(NA_real_, c(B,2))\n\ntheta[1,] &lt;- opt$par # valor inicial da cadeia é a emv\ntau &lt;- 1             # tunning\ncont &lt;- 0            # contador de aceites\n\nfor(j in 2:B){\n  #simule um candidato\n  theta_cand &lt;- rmvnorm(1, theta[j-1,], tau*Sigma)\n  \n  # calcule a probabilidade do salto\n  lnum &lt;- logveross(theta_cand) +\n    sum(dnorm(theta_cand[1,],0,10, log = T))\n  \n  lden &lt;- logveross(theta[j-1,]) +\n    sum(dnorm(theta[j-1,],0,10, log = T))\n  \n  prob &lt;- exp( lnum - lden)\n  \n  # verifique o salto\n  u &lt;- runif(1)\n  if( u &lt; prob){\n    theta[j, ] &lt;- theta_cand\n    cont &lt;- cont+1\n  } else {\n    theta[j,] &lt;- theta[j-1,]\n  }\n}\n\ncont/B\n\n[1] 0.5628\n\ntheta_sim &lt;- theta[ seq(B/2, B, 15),]\nacf(theta_sim)\n\n\n\n\n\n\n\n\nPor fim, as estimativas intervalares para \\((\\alpha,\\beta)\\) são\n\nquantile(exp(theta_sim[,1]), c(.025,.975))\n\n    2.5%    97.5% \n1.699256 5.644593 \n\n# intervalos de credibilidade para beta\nquantile(exp(theta_sim[,2]), c(.025,.975))\n\n    2.5%    97.5% \n2.087772 7.675778",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Aproximação normal e seu uso com o Metropolis-Hastings</span>"
    ]
  },
  {
    "objectID": "misturas.html",
    "href": "misturas.html",
    "title": "9  Misturas de distribuições",
    "section": "",
    "text": "9.1 Modelos com inflação de zeros\nQuando são observados mais zeros do que o esperado pelo modelo de contagem assumido para a verossimilhança, é usual considerar um modelo com inflação de zeros. Nesse tipo de modelo, assumimos que existe uma variável \\(Z|p\\sim\\hbox{Bernoulli}(\\rho)\\) tal que:\n\\[X=\\left\\{\\begin{array}{ll}0, & \\hbox{se }Z=1\\ \\\\ Y,&\\hbox{se }Z=0\\end{array}\\right.\\] onde \\(Y\\sim h(.|\\theta)\\) é o modelo de contagem. Apenas \\(X\\) é observado e, como\n\\[\\begin{align}P(X=0|\\theta,p)&=P(X=0|Z=0,\\theta)P(Z=0|\\rho)+P(X=0|Z=1,\\theta)P(Z=1|\\rho)\\\\&=(1-\\rho)h(0|\\theta)+\\rho\\end{align}\\] a probabilidade de observar um zero está entre \\(h(0|\\theta)\\) e 1, o que caracteriza a inflação.\nAgora, considere um modelo inflacionado de zeros aumentado:\n\\[f(x,z|\\theta,\\rho)=f(x|z,\\theta)f(z|\\rho)=f(x|z,\\theta)\\rho^z(1-\\rho)^{1-z}.\\] Note que\n\\[f(x|z,\\theta)=\\left\\{\n\\begin{array}{ll}\nh(x|\\theta),&\\hbox{ se }z=0,\\\\\nI(x=0),&\\hbox{ se }z=1\\\\\n\\end{array}\\right.\\] logo, a distribuição conjunta \\(f(x,z|\\theta,\\rho)\\) é dada por\n\\[\\begin{array}{c|cc}\\hline & x=0 & \\hbox{qualquer }x&gt; 0 \\\\ \\hline\nz=0 & h(0|\\theta)(1-\\rho) & h(x|\\theta)(1-\\rho) \\\\\nz=1 & \\rho & 0 \\\\ \\hline\n\\end{array}\n\\] Então,\n\\[\\begin{align}\n\\prod_{i=1}^n f(x_i,z_i|\\theta,\\rho)&=\\prod_{i=1}^n [h(0|\\theta)(1-\\rho)]^{I(x_i=0,z_i=0)}[h(x_i|\\theta)(1-\\rho)]^{I(x_i&gt;0,z_i=0)}\\rho^{I(x_i=0,z_i=1)}\\\\\n&=\\prod_{i=1}^n [h(x_i|\\theta)(1-\\rho)]^{I(z_i=0)}\\rho^{I(x_i=0,z_i=1)}\\\\\n&=\\prod_{i=1}^n(1-\\rho)^{I(z_i=0)}\\rho^{I(x_i=0,z_i=1)}\\prod_{i=1}^n [h(x_i|\\theta)]^{I(z_i=0)}\\end{align}\\] e, notando que \\(I(z_i=0)=1-z_i,\\)\n\\[\\begin{align}\n\\prod_{i=1}^n f(x_i,z_i|\\theta,\\rho)&=\n(1-\\rho)^{n-\\sum_{i=1}^n z_i}\\rho^{\\sum_{i=1}^n z_iI(x_i=0)}\\prod_{i=1}^n [h(x_i|\\theta)]^{1-z_i}\\end{align}\\]\nConsidere, a priori, que \\(\\theta\\) e \\(\\rho\\) são independentes. Seja \\(\\pi(\\theta)\\) a priori para \\(\\theta\\) e considere que \\(\\rho\\sim\\hbox{Beta}(a,b)\\). Então, as condicionais completas para \\(\\theta\\) e \\(\\rho\\) são\n\\[\\begin{align}\n\\pi(\\theta|\\rho,\\boldsymbol{z},\\boldsymbol{x})&\\propto \\prod_{i=1}^n h(x_i|\\theta)^{1-z_i}\\pi(\\theta),\\\\\n\\pi(\\rho|\\theta,\\boldsymbol{z},\\boldsymbol{x})&\\propto \\rho^{\\sum_{i=1}^n z_iI(x_i=0)+a-1}(1-\\rho)^{n-\\sum_{i=1}^n z_i+b-1},\\\\\n\\end{align}\\]\nPara a condicional completa de \\(z_i\\), notemos que \\[P(Z_i=1|x_i&gt;0)=\\frac{P(Z_i=1,X_i&gt;0)}{P(X_i&gt;0)}=0,\\] e que\n\\[P(Z_i=z|x_i=0)= \\left\\{\\begin{array}{ll}\\frac{P(Z_i=0,X_i=0)}{P(X_i=0)}=\\frac{h(0|\\theta)(1-\\rho)}{\\rho+(1-\\rho)h(0|\\theta)},&,z=0\\\\\n\\frac{P(Z_i=1,X_i=0)}{P(X_i=0)}=\\frac{\\rho}{\\rho+(1-\\rho)h(0|\\theta)},&z=1\\end{array}\\right.,\\] logo \\[\\pi(z_i|\\theta,\\rho,\\boldsymbol{x},\\boldsymbol{z}_{(-i)})=\\left\\{\\begin{array}{ll}\\hbox{Bernoulli}\\left( \\frac{\\rho}{\\rho+(1-\\rho)h(0|\\theta)}\\right),&\\hbox{ se }x_i=0\\\\\nI(z_i=0),&\\hbox{ se } x_i&gt;0\\\\ \\end{array}\\right.\\]\nPortanto, um amostrador de Gibbs para um modelo inflacionado de zeros é",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Misturas de distribuições</span>"
    ]
  },
  {
    "objectID": "misturas.html#modelos-com-inflação-de-zeros",
    "href": "misturas.html#modelos-com-inflação-de-zeros",
    "title": "9  Misturas de distribuições",
    "section": "",
    "text": "Amostrador de Gibbs para o modelo inflado de zeros\nFaça \\(j=0\\) e dê os valores iniciais \\(\\theta^{(0)}\\) e \\(\\rho^{(0)}\\).\nNo \\(j\\)-ésimo passo:\n\nPara \\(i\\in\\{1,\\ldots,n\\}\\), se \\(x_i&gt;0\\) faça \\(z_i=0\\). Senão, simule \\[z_i^{(j)}\\sim \\hbox{Bernoulli}\\left(\\frac{\\rho^{(j-1)}}{\\rho^{(j-1)}+(1-\\rho^{(j-1)})h(x_i|\\theta^{(j-1)})}\\right)\\]\nSimule \\(\\rho^{(j)}\\sim\\hbox{Beta}(a+\\sum_{i=1}^n z_i^{(j)}I(x_i=0),b+n-\\sum_{i=1}^n z_i^{(j)})\\)\nSimule \\(\\theta^{(j)}\\) de \\[\\pi(\\theta|\\rho^{(j)},\\boldsymbol{z}^{(j)},\\boldsymbol{x})\\propto \\prod_{i=1}^n h(x_i|\\theta^{(j)})^{1-z_i^{(j)}}\\pi(\\theta^{(j)}).\\]\n\n\n\nExemplo - A Poisson inflada de zeros \nNeste exemplo, vamos considerar que a distribuição da contagem é Poisson(\\(\\theta\\)) e que \\(\\theta\\sim\\hbox{Gama}(r,s)\\). Então,\n\\[\\begin{align}\n\\pi(\\theta|\\rho^{(j)},\\boldsymbol{z}^{(j)},\\boldsymbol{x})&\\propto \\prod_{i=1}^{n} h(x_{i} | \\theta )^{ 1-z_{i}^{(j)} }\\pi(\\theta)=\n\\prod_{i=1}^{n} \\left[\\frac{ e^{-\\theta}\\theta^{x_i} }{x_i!}\\right]^{1-z_{i}^{(j)}}\\frac{s^r}{\\Gamma(r)}\\theta^{r-1} e^{-s\\theta}\\\\&\\propto \\theta^{\\sum_{i=1}^n x_i(1-z_i^{(j)})+r-1}e^{-(n-\\sum_{i=1}^n z_i^{(j)}+s)\\theta}\n\\end{align},\\]\nou seja, \\(\\theta^{(j)}|\\rho^{(j)},\\boldsymbol{z}^{(j)},\\boldsymbol{x}\\sim\\hbox{Gama}(\\sum_{i=1}^n x_i(1-z_i^{(j)})+r,n-\\sum_{i=1}^n z_i^{(j)}+s)\\)\n\n\nOs dados abaixo representam o número anual de furacões atlânticos grandes (categoria 4 ou 5) entre 1987 e 2012, nos Estados Unidos.\n\nfur &lt;-  c(0, 0 ,1,\n0, 0, 1, 0, 0, 1, 0, 0, 2, 2,\n0, 0, 1, 1, 3, 4, 0, 0, 2, 0,\n0, 0, 0)\n\nA frequência relativa de zeros é 0,58. Considerando o modelo Poisson\\((\\theta)\\) com \\(\\pi(\\theta)\\propto \\theta^{-1}\\), temos que\n\nr1 &lt;- sum(fur)\ns1 &lt;- length(fur)\nplot(table(fur)/s1, type= 'p', xlab='No. anual de mortes pod fur', ylab = 'Probabilidade', col = 'cyan3', pch=16)\nlines(0:4,table(fur)/s1, col = 'cyan3')\npoints(0:4, dnbinom(0:4, size = r1, prob = s1/(1+s1)), pch=16, col = 'brown')\nlines(0:4, dnbinom(0:4, size = r1, prob = s1/(1+s1)), col = 'brown')\n\nlegend('bottomleft',c('Freq. relativa','Pred. post. Poisson'), fill=c('cyan3','brown'), bty='n')\n\n\n\n\n\n\n\n\n\n# hiperparâmetros para rho\na = b = 1\n\n# hiperparâmetros para theta\nr=.1\ns=.1\n\n# tamanho da amostra\nn &lt;- length(fur) \n\n# valores iniciais da cadeia\ntheta &lt;- mean(fur)\nrho &lt;- mean(fur == 0)\n\n# amostrador de Gibbs\nB &lt;- 50000\nfor(i in 1:B){\n  # simulando z\n  z &lt;- NULL\n  prob &lt;- rho[i]/ ( (1-rho[i])*dpois(0,theta[i]) + rho[i])\n  for(j in 1:n){\n    if(fur[j] &gt;0){ z[j] &lt;- 0} else{\n      z[j] &lt;- rbinom(1,1,prob)\n    }\n  }\n\n  # simulando rho\n  rho[i+1] &lt;- rbeta( 1, a + sum( z * (fur == 0)) , n- sum(z)+ b )\n  \n  # simulando theta\n  theta[i+1] &lt;- rgamma(1, sum( fur*(1-z) ) + r,  n - sum(z) + s)\n}\n\nVamos descartar a metade das simulações e usar um thinning igual a 15:\n\ntheta_sim &lt;- theta[seq(B/2,B,15)]\nrho_sim &lt;- rho[seq(B/2,B,15)]\n\noo &lt;- par(mfrow=c(2,2))\nts.plot(theta_sim, lwd = 2)\nts.plot(rho_sim, lwd = 2)\nacf(theta_sim)\nacf(rho_sim)\n\n\n\n\n\n\n\n\nVamos estimar as probabilidade de ocorrerem \\(k\\) mortes via preditiva posteriori:\n\n# tamanho do vetor simulado\nBs &lt;- length(theta_sim)\n\nx_til &lt;- array( NA_real_, c(Bs,n))\nfor(j in 1:Bs){\n  z &lt;- rbinom( n, 1, rho_sim[j])\n  x_til[j,] &lt;- (1-z)*rpois(n, theta_sim[j])\n}\n\n# probabilidades estimadas via ZIP\np_zip &lt;- prop.table(table(x_til))\n\np_zip\n\nx_til\n           0            1            2            3            4            5 \n6.152231e-01 1.997600e-01 1.143002e-01 4.764432e-02 1.645056e-02 4.868257e-03 \n           6            7            8            9 \n1.315122e-03 3.691569e-04 4.614462e-05 2.307231e-05 \n\n\nAbaixo mostramos as probabilidades preditas do modelo ZIP, do modelo Poisson a e frequência relativa.\n\nr1 &lt;- sum(fur)\ns1 &lt;- length(fur)\nplot(table(fur)/s1, type= 'p', xlab='No. anual de mortes pod fur', ylab = 'Probabilidade', col = 'cyan3', pch=16)\nlines(0:4,table(fur)/s1, col = 'cyan3')\npoints(0:4, dnbinom(0:4, size = r1, prob = s1/(1+s1)), pch=16, col = 'brown')\nlines(0:4, dnbinom(0:4, size = r1, prob = s1/(1+s1)), col = 'brown')\npoints(names(p_zip),p_zip, pch=16,col = 'magenta')\nlines(names(p_zip),p_zip,col = 'magenta')\n\nlegend('bottomleft',c('Freq. relativa','Pred. post. Poisson', 'Pred. post. ZIP'), fill=c('cyan3','brown', 'magenta'), bty='n')\n\n\n\n\n\n\n\n\n\n\n9.1.1 Exercício\n\nAbaixo, segue o número anual de tornados em Lafayette Parish, Louisiana, entre 1950 e 2012.\n\ntor &lt;- c(0, 0,0, 1, 0, 0, 0, 1, 0, 0,\n1, 0, 0, 0, 1, 1, 0, 0, 0, 2,\n0, 0, 0, 0, 1, 3, 0, 2, 1, 0,\n1, 0, 0, 1, 0, 1, 0, 0, 2, 1,\n0, 1, 2, 0, 0, 1, 0, 1, 2, 0,\n0, 0, 3, 0, 2, 0, 1, 1, 3, 0,\n1, 1, 1)\n\n\nAjuste o modelo Poisson.\nAjuste o modelo Poisson inflado de zeros.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Misturas de distribuições</span>"
    ]
  },
  {
    "objectID": "misturas.html#mistura-escalonada-de-normais",
    "href": "misturas.html#mistura-escalonada-de-normais",
    "title": "9  Misturas de distribuições",
    "section": "9.2 Mistura escalonada de normais",
    "text": "9.2 Mistura escalonada de normais",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Misturas de distribuições</span>"
    ]
  },
  {
    "objectID": "misturas.html#misturas-finitas-com-número",
    "href": "misturas.html#misturas-finitas-com-número",
    "title": "9  Misturas de distribuições",
    "section": "9.3 Misturas finitas com número",
    "text": "9.3 Misturas finitas com número\nDizemos que \\(X|\\boldsymbol{\\theta},\\boldsymbol{p},\\kappa\\) é um modelo de mistura finito se sua função de densidade/probabilidade é dada por\n\\[f(x| \\boldsymbol{\\theta},\\boldsymbol{p} ,\\kappa )=\\sum_{k=1}^\\kappa p_k f_k(x|\\boldsymbol{\\theta}_k).\\]\nCada função \\(f(.|\\boldsymbol{\\theta}_k)\\) é denominada componente da mistura e o número de componentes pode ser desconhecido.\nAssim como o modelo com zeros inflacionados, podemos utilizar uma variável latente \\(\\textbf{z}_i|\\kappa=(z_{i,1},\\ldots,z_{i,\\kappa})\\sim\\hbox{Multinomial}(p_1\\ldots,p_\\kappa|\\sum_{k=1}^\\kappa z_{ik}=1)\\), obtendo o seguinte modelo aumentado\n\\[f(x_i|\\boldsymbol{\\theta},\\textbf{z}_i,\\kappa)=\\prod_{k=1}^\\kappa \\left[f\\left(x_i|\\boldsymbol{\\theta}_k\\right)\\right]^{z_{i,k}}\\]\nA função de verossimilhança aumentada para este modelo é\n\\[\\prod_{i=1}^n f(x_i|\\boldsymbol{\\theta},\\textbf{z}_i,\\kappa)=\\prod_{i=1}^n\\prod_{k=1}^\\kappa \\left[f\\left(x_i|\\boldsymbol{\\theta}_k\\right)\\right]^{z_{i,k}}.\\]\nConsidere as prioris \\(\\pi(\\boldsymbol{\\theta}|\\kappa)=\\prod_{k=1}^\\kappa \\pi(\\boldsymbol{\\theta}_k)\\) e \\(\\textbf{p}|\\kappa\\sim\\hbox{Dirichlet}(a_1,\\ldots,a_\\kappa)\\), onde \\[f(\\textbf{p}|\\kappa)\\propto \\prod_{k=1}^\\kappa p_k^{a_k-1}\\] com \\(\\sum_{k=1}^\\kappa p_k=1\\). As condicionais completas para este problema são\n\n\\(\\begin{align}f(\\boldsymbol{\\theta}_k|resto)\\propto \\prod_{i:z_{i,k}=1}f(x_i|\\boldsymbol{\\theta}_k)\\pi(\\boldsymbol{\\theta}_k)\\end{align}\\)\n\\(\\begin{align}f(\\textbf{z}_i|resto)\\propto \\prod_{k=1}^\\kappa \\left[p_kf(x_i|\\boldsymbol{\\theta}_k)\\right]^{z_{i,k}}\\end{align}\\) ou seja, \\(\\textbf{z}_i|rest\\sim\\hbox{Multinomial}(\\tilde{p}_1,\\ldots,\\tilde{p}_\\kappa)\\), onde\n\n\\[\\tilde{p}_k=\\frac{p_kf(x_i|\\boldsymbol{\\theta}_k)}{\\sum_{k=1}^\\kappa p_kf(x_i|\\boldsymbol{\\theta}_k)}\\] * \\(f(\\textbf{p}|resto)\\propto \\prod_{k=1}^\\kappa p_k^{\\sum_{i=1}^n z_{i,k}+a_k-1}\\), ou seja \\(\\textbf{p}|resto\\sim\\hbox{Dirichlet}(a_1+\\sum_{i=1}^n z_{i,1},\\ldots,a_\\kappa+\\sum_{i=1}^n z_{i,\\kappa})\\)\nSe necessário, podemos atrbuir a priori \\[\\pi(\\kappa)=\\frac{1}{M},\\kappa=1,2,\\ldots,M\\] para obter a condicional completa \\[\\pi(\\kappa|resto)=\\frac{\\prod_{i=1}^n\\prod_{k=1}^\\kappa f(x_i|\\boldsymbol{\\theta}_k)^{z_{i,k}}\\pi(\\boldsymbol{\\theta}_k)\\pi(\\textbf{p}|\\kappa)\\pi(\\textbf{z}_i|\\kappa)}{\\sum_{\\kappa=1}^M \\prod_{i=1}^n\\prod_{k=1}^\\kappa f(x_i|\\boldsymbol{\\theta}_k)^{z_{i,k}}\\pi(\\boldsymbol{\\theta}_k)\\pi(\\textbf{p}|\\kappa)\\pi(\\textbf{z}_i|\\kappa)},\\kappa=1,\\ldots,M.\\]\n\n9.3.1 O velho fiel\nO banco de dados faithful mostra a duração e o tempo até a próxima erupção do geiser Velho Fiel, no parque Yellowstone. Abaixo mostramos o diagrama do tempo de espera entre erupções\n\nhist(faithful$waiting)\n\n\n\n\n\n\n\n\nÉ possível notar classes, uma com tempo e entre erupções menor que 70 com tempo maior. Temos as seguintes estimativas iniciais:\n\n## elementos na classe 1\nx &lt;- faithful$waiting\nz &lt;- x &lt; 70\n# proporção na classe 1\nmean(z)\n\n[1] 0.3786765\n\n# média e desvio padrão na classe 1\nmean( x[z])\n\n[1] 55.15534\n\nsd( x[z])\n\n[1] 6.266558\n\n\n\n## elementos na classe 2\n# proporção na classe 2\nmean(z==F)\n\n[1] 0.6213235\n\n# média e desvio padrão na classe 2\nmean( x[z==F])\n\n[1] 80.49112\n\nsd( x[z==F])\n\n[1] 5.456667\n\n\nVamos considerar que as duas componentes possuem distribuição normal. Para cada componente, teremos as seguintes prioris:\n\\[\\pi(\\mu_i,\\phi_i)=\\frac{\\phi^{1/2}_i}{\\sqrt{2\\pi C}}e^{-\\frac{\\phi_i}{2C}(\\mu_i-m_i)^2}\\frac{b^a}{\\Gamma(a)}\\phi_i^{a-1}e^{b\\phi_i},\\]\n\\[p\\sim\\hbox{Beta}(r,s)\\]\n\\[z_i\\sim\\hbox{Bernoulli}(p)\\]\nO modelo aumentado é \\[f(x_i|\\mu,\\phi,z_{i})=\\left[\\frac{\\phi_1^{1/2}}{\\sqrt{2\\pi}}e^{-\\frac{\\phi_1}{2}(x_i-\\mu_1)}\\right]^{z_i}\\left[\\frac{\\phi_2^{1/2}}{\\sqrt{2\\pi}}e^{-\\frac{\\phi_2}{2}(x_i-\\mu_2)}\\right]^{1-z_i}\\] As condicionais completas são:\n\\[\\begin{align}f(\\mu_1|resto) &\\propto \\exp\\left\\{-\\frac{\\phi_1}{2}\\sum_{i=1}^n z_i(x_i-\\mu_1)^2\\right\\}\\exp\\left\\{-\\frac{\\phi_1}{2C} z_i(\\mu_1-m_1)^2\\right\\}\\\\&\\propto \\exp\\left\\{-\\frac{\\phi_1}{2}\\left(\\sum_{i=1}^n z_i+C^{-1}\\right) \\left(\\mu_1-\\frac{\\sum_{i=1}^{n}x_iz_i+m_1C^{-1}}{\\sum_{i=1}^n z_i+C^{-1}}\\right)^2\\right\\}\\end{align}\\]\n\\[\\begin{align}f(\\mu_2|resto) &\\propto \\exp\\left\\{-\\frac{\\phi_2}{2}\\sum_{i=1}^n (1-z_i)(x_i-\\mu_2)^2\\right\\}\\exp\\left\\{-\\frac{\\phi_2}{2C} (1-z_i)(\\mu_2-m_2)^2\\right\\}\\\\&\\propto \\exp\\left\\{-\\frac{\\phi_2}{2}\\left(\\sum_{i=1}^n (1-z_i)+C^{-1}\\right) \\left(\\mu_2-\\frac{\\sum_{i=1}^{n}x_i(1-z_i)+m_1C^{-1}}{\\sum_{i=1}^n (1-z_i)+C^{-1}}\\right)^2\\right\\}\\end{align}\\]\n\\[\\begin{align}f(\\phi_2|resto)&\\propto \\phi_2^{-\\frac{1}{2}\\sum_{i=1}^{n}z_i}\ne^{-\\frac{\\phi_2}{2}\\sum_{i=1}^n (1-z_i)(x_i-\\mu_2)^2}\\phi^{-1/2}_2e^{-\\frac{\\phi_2}{2}(\\mu_2-m_2)^2}\\phi_2^{a/2-1}e^{-\\phi_2 b/2}\\\\ &\\propto \\phi_2^{\\frac{1}{2}(1+a+\\sum_{i=1}^{n}(1-z_i)-1}e^{-\\frac{\\phi_2}{2}[\\sum_{i=1}^n(1-z_i)(x_i-\\mu_2)^2 +(\\mu_2-m_2)^2 + b]}\\end{align}\\] \\[\\begin{align}f(\\phi_1|resto)&\\propto \\phi^{-\\frac{1}{2}\\sum_{i=1}^{n}z_i}\ne^{-\\frac{\\phi_1}{2}\\sum_{i=1}^n z_i(x_i-\\mu_1)^2}\\phi^{-1/2}e^{-\\frac{\\phi_1}{2}(\\mu_1-m_1)^2}\\phi_1^{a/2-1}e^{-\\phi_1 b/2}\\\\ &\\propto \\phi_1^{\\frac{1}{2}(1+a+\\sum_{i=1}^{n}z_i)-1}e^{-\\frac{\\phi_1}{2}[\\sum_{i=1}^nz_i(x_i-\\mu_1)^2 +(\\mu_1-m_1)^2 + b]}\\end{align}\\]\n\\[\\begin{align}f(p|resto)\\propto \\prod_{i=1}^n p^{z_i}(1-p)^{1-z_i}p^{r-1}(1-p)^{s-1}\\propto p^{r+\\sum_{i=1}^n z_i-1}(1-p)^{s+\\sum_{i=1}^n (1-z_i)-1}\\end{align}\\]\n\\[f(z_i|resto)\\propto\\left[ p\\frac{\\phi_1^{1/2}}{\\sqrt{2\\pi}}e^{-\\frac{\\phi_1}{2}(x_i-\\mu_1)^2}\\right]^{z_i}\\left[ (1-p)\\frac{\\phi_2^{1/2}}{\\sqrt{2\\pi}}e^{-\\frac{\\phi_2}{2}(x_i-\\mu_2)^2}\\right]^{1-z_i}\\]\nAbaixo implementamos o amostrador de Gibbs\n\nB &lt;- 50000\n\n# hiperparmametros\nm1 &lt;- 65\nm2 &lt;- 80\nC &lt;- 1000\nr= 4; s = 6\na = 1; b = .1\n\n# valores iniciais\nz &lt;- x &lt; 70\nphi1 &lt;- 1/36\nphi2 &lt;- 1/25\nmu1 = mu2 =  p = NULL\n\nfor(i in 1:B){\n  # mu dado o resto\n  m1_post &lt;- ( sum(x*z) + m1/C) / ( sum(z) + 1/C )\n  m2_post &lt;- ( sum(x*(1-z)) + m1/C) / ( sum(1-z) + 1/C )\n  s1_post &lt;- 1 / ( ( sum(z) + 1/C )*phi1[i] )\n  s2_post &lt;- 1 / ( ( sum(1-z) + 1/C )*phi2[i] )\n  \n  mu1[i+1] &lt;- rnorm(1, m1_post, sqrt( s1_post) )\n  mu2[i+1] &lt;- rnorm(1, m2_post, sqrt( s2_post) )\n  \n  # phi dado resto\n  phi1[i+1] &lt;- rgamma(1, 1 + a + sum(z), sum( z*(x - mu1[i+1])^2 ) + (mu1[i+1]-m1)^2 + b)\n  phi2[i+1] &lt;- rgamma(1, 1 + a + sum(1-z), sum( (1-z)*(x - mu2[i+1])^2 ) + (mu2[i+1]-m2)^2 + b)\n  \n  # p dado resto\n  p[i+1] &lt;- rbeta(1, r + sum(z), s + sum(1-z) )\n  \n  # z dado resto\n  aux1 &lt;- p[i+1]*dnorm(x,mu1[i+1], 1/sqrt(phi1[i+1]))\n  aux2 &lt;- (1-p[i+1])*dnorm(x,mu2[i+1], 1/sqrt(phi2[i+1]))\n  \n  z &lt;- rbinom(length(x), 1, aux1/( aux1 + aux2))\n}\n# \n\n\nhist(mu1[seq(B/2,B,30)])\n\n\n\n\n\n\n\nhist(mu2[seq(B/2,B,30)])",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Misturas de distribuições</span>"
    ]
  },
  {
    "objectID": "poisson.html",
    "href": "poisson.html",
    "title": "10  O modelo Poisson revisitado",
    "section": "",
    "text": "10.1 Verossimilhança, prioris e posterioris\nDizemos que \\(X|\\theta\\) tem distribuição Poisson se sua função de probabilidade é dada por \\[f(x|\\theta)=\\frac{e^{-\\theta}\\theta^x}{x!},\\] onde \\(x=0,1,\\ldots\\) e \\(\\theta&gt;0\\). O parâmetro \\(\\theta\\) é denominado taxa. Para este modelo \\[E(X|\\theta)=Var(X|\\theta)=\\theta.\\]\nEsta é uma das distribuições para contagens mais importantes. A verossimilhança deste modelo, para uma amostra de vaiid, é dada por \\[L(\\theta)=\\frac{e^{-n\\theta}\\theta^{\\sum_{i=1}^{n}x_i}}{\\prod_{i=1}^{n}x_i!}.\\] O modelo Poisson pertence à família exponencial e sua conjugada é \\(\\theta\\sim\\hbox{Gama}(r,s)\\), onde \\(r\\) e \\(s\\) podem ser interpretados como o total da contagem e o tamanho da amostra .\nNeste caso, a é \\(\\hbox{Gama}(r+\\sum_{i=1}^n x_i+r, s+n)\\).\nA média da é \\[E(\\theta|\\mathbf{x})=\\frac{\\sum_{i=1}^{n}x_i+r}{n+s}=\\frac{n}{n+s}\\bar{x}+\\frac{s}{n+s}E(\\theta),\\] onde fica claro que este estimador é uma média ponderada das informações provenientes das duas fontes de informação (sendo \\(\\bar{x}\\) a estimativa de máxima verossimilhança e \\(E(\\theta)\\) a média ).\nSe \\(n\\gg s\\), então a média a posteriori dará maior peso para a informação dos dados.\nA informação de Fisher é \\[\\mathcal{I}(\\theta)=\\frac{1}{\\theta}\\]\nAssim, a priori de Jeffreys é dada por \\[f(\\theta)\\propto \\theta^{-\\frac{1}{2}},\\] sendo, portanto, uma priori imprópria. Contudo, \\[f(\\theta|\\mathbf{x})\\propto e^{-n\\theta}\\theta^{\\sum_{i=1}^{n}x_i} \\theta^{-\\frac{1}{2}},\\] logo, a posteriori é própria, tendo distribuição \\(Gama(\\sum_{i=1}^{n}x_i+1/2,n)\\).\nConsidere a posteriori \\(\\theta|\\mathbf{x}\\sim\\hbox{Gama}(r_1,s_1)\\). Podemos retirar uma amostra da preditiva do seguinte modo:\n2.Gere \\(\\tilde{\\mathbf{x}}\\sim\\hbox{Poisson}(\\theta_j)\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>O modelo Poisson revisitado</span>"
    ]
  },
  {
    "objectID": "poisson.html#verossimilhança-prioris-e-posterioris",
    "href": "poisson.html#verossimilhança-prioris-e-posterioris",
    "title": "10  O modelo Poisson revisitado",
    "section": "",
    "text": "Gere \\(\\theta_j\\sim\\hbox{Gama}(r_1,s_1)\\)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>O modelo Poisson revisitado</span>"
    ]
  },
  {
    "objectID": "poisson.html#o-modelo-poisson-para-taxas",
    "href": "poisson.html#o-modelo-poisson-para-taxas",
    "title": "10  O modelo Poisson revisitado",
    "section": "10.2 O modelo Poisson para taxas",
    "text": "10.2 O modelo Poisson para taxas\nA taxa é o cociente entre o número de casos de um evento em determinado intervalo de tempo e a população em risco, definida em um espaço e no mesmo intervalo de tempo (``pessoas-tempo’’). Note que, pela definição, a taxa é uma estatística.\nSeja \\(n\\) o tamanho da população no espaço/tempo e seja \\(y\\) o número de casos do evento de interesse. Então,\n\\[\\hbox{taxa} = \\frac{y}{n}\\]\nContudo, como \\(n\\) tende a ser muito maior que \\(y\\), é comum reportar a taxa vezes \\(10^k\\), para algum \\(k&gt;0\\).\n\nExemplo: Segundo o Anuário de Segurança Pública 2022, em 2021 houveram 68.885 casos de estupro. Considerando uma população de 212,7 milhões de habitantes, a taxa de estupro para aquele ano foi de \\[\\frac{68.885}{212.700.000}=3,23\\times 10^{-4}\\] casos por pessoa-ano. Como \\(n\\) tende a ser maior que \\(y\\), é comum considerar.\nMultiplicando a taxa por \\(10^5\\), temos uma taxa de 32,3 casos para cada 100.000 habitantes.\n\nAgora,considere que \\(\\theta\\) é o parâmetro taxa. Então,\n\\[\\hat{\\theta}=\\frac{y}{n}\\] é a estimativa para \\(\\theta\\). Como \\(y\\) é uma contagem, é razoável supor que \\[\\theta =\\frac{1}{n}E(Y|\\theta).\\] e um modelo possível seria \\(y|\\theta\\sim\\hbox{Poisson}(\\theta n)\\).\nAgora, considere que uma população está particionada em \\(m\\) localidades. Para um dado intervalo de tempo, sejam \\(n_i\\) e \\(y_i\\) a população da localidade \\(i\\) e seu respectivo número de casos observados. Suponha ainda que a taxa \\(\\theta\\) é comum para a pooulação e que \\(y_i\\) é condicionalmente independente de \\(y_j\\) dado \\(\\theta\\). Assumindo a distribuição Poisson, teremos\n\\[L(\\theta)=\\prod_{i=1}^m\\frac{e^{-\\theta n_i}(\\theta n_i)^{y_i}}{y_i!}\\varpropto \\theta^{\\sum_{i=1}^m y_i}e^{-\\theta \\sum_{i=1}^m n_i}=\\theta^{\\sum_{i=1}^n y_i}e^{-\\theta N},\\] onde \\(N=\\sum_{i=1}^m n_i\\) é o tamanho da população. Como a verossimilhança pertence à família exponencial, temos que o modelo Gama\\((a,b)\\) é conjugado gerando a posteriori\n\\[\\theta|\\mathbf{y}\\sim\\hbox{Gama}\\left(\\sum_{i=1}^{m}y_i+a,N+b\\right).\\]\nA prioris impróprias \\(\\pi(\\theta)\\varpropto \\theta^{-1}\\) e \\(\\pi(\\theta)\\varpropto \\theta^{-1/2}\\) geram, respectivamente, as posterioris \\(\\hbox{Gama}(\\sum_{i=1}^m y_i,N)\\) e \\(\\hbox{Gama}(\\sum_{i=1}^m y_i+1/2,N)\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>O modelo Poisson revisitado</span>"
    ]
  },
  {
    "objectID": "poisson.html#exemplo-1-crime-de-estupro-de-vulnerável-no-interior-do-amazonas",
    "href": "poisson.html#exemplo-1-crime-de-estupro-de-vulnerável-no-interior-do-amazonas",
    "title": "10  O modelo Poisson revisitado",
    "section": "10.3 Exemplo 1: crime de estupro de vulnerável no interior do Amazonas",
    "text": "10.3 Exemplo 1: crime de estupro de vulnerável no interior do Amazonas\nOs dados a seguir foram cedidos pelo Observatório de Violência de Gênero no Amazonas e compreendem os anos entre 2010 e 2012.\n\n\n\nCidade\nvitimas\nPopulacao feminina\n\n\n\n\nAmatura\n3\n639\n\n\nAtalaia do Norte\n6\n905\n\n\nBarreirinha\n12\n1899\n\n\nBenjamin Constant\n2\n2036\n\n\nBoa Vista do Ramos\n6\n1060\n\n\nFonte Boa\n0\n1438\n\n\nJutai\n1\n1143\n\n\nMaues\n13\n3421\n\n\nNhamunda\n9\n1168\n\n\nParintins\n20\n6700\n\n\nSanto Antonio do Ica\n7\n1608\n\n\nSao Paulo de Olivenca\n5\n2033\n\n\nTabatinga\n8\n3095\n\n\nTonantins\n1\n1186\n\n\n\nConsiderando a priori \\(\\pi(\\theta)\\varpropto \\theta^{-1/2}\\) teremos:\n\n# banco de dados\ncasos &lt;- c( 3, 6, 12, 2, 6, 0, 1, 13, 9, 20, 7, 5, 8, 1 )   \n\npop &lt;- c(639, 905, 1899, 2036, 1060, 1438, 1143, 3421, 1168, 6700, 1608, 2033, 3095, 1186)\npop &lt;- pop/10^5\n\nmunicipios &lt;- c( 'Amatura', 'Atl.Norte', 'Barr', 'BC','BV Ramos', 'Fonte B', 'Jutai', 'Maues', 'Nhamunda', 'Parintins', 'StoIca', 'SP Olivenca', 'Tbt','Tonantins')\n\n\n# posteriori\na_post &lt;- sum(casos) + .5\nb_post &lt;- sum(pop)\n\ncurve(dgamma(x,a_post,b_post),230,450 ,lwd = 2, xlab = expression(theta), ylab = 'densidade a posteriori' )\n\n\n\n\n\n\n\n\nAbaixo, simulamos 50.000 amostras da preditiva a posteriori\n\nB &lt;- 50000 # número de simulações da preditiva a posteriori\nm &lt;- length(casos)\n\npred_mun &lt;- NULL\nfor(i in 1:5000){\n  theta &lt;- rgamma(1, sum(casos) + .5, sum(pop))\n  pred_mun &lt;- rbind(pred_mun, rpois(m , theta * pop))\n}\n\npred_mun &lt;- data.frame(pred_mun)\nnames(pred_mun) &lt;- municipios\nboxplot(pred_mun)\npoints(1:14,casos, pch=16,cex = 1.2, col ='tomato')\n\n\n\n\n\n\n\n\n\noo &lt;- par()\nmat &lt;- matrix(1:m,ncol=2)\nmat &lt;- rbind(mat, c(15,16))\nlayout(mat, heights = rep(1,14,.5,.5))\n\nfor(i in 1:m){\n  freq &lt;- prop.table( table(pred_mun[,i]) )\n  par(mar = c(1,5,1,1), cex = .8)\n  plot.new()\n  plot.window(xlim=c(0,46), ylim = c(0,.3))\n  points(as.numeric(names(freq)), freq, type='h', lwd = 2)\n  title(ylab=municipios[i])\n  points(casos[i],0,pch=16,col='tomato',cex= 1.2)\n  \n}\n  par(mar = c(2,5,0,1))\n  plot.new()\n  plot.window(xlim=c(0,46), ylim = c(0,.1))\n  segments(0,.05,46,.05,lwd=2)\n for(j in seq(0,45,5)){\n    segments(j,.05,j,.03)\n    text(j,.01,j)\n  }\n    par(mar = c(2,5,0,1))\n  plot.new()\n  plot.window(xlim=c(0,46), ylim = c(0,.1))\n  segments(0,.05,46,.05,lwd=2)\n  for(j in seq(0,45,5)){\n    segments(j,.05,j,.03)\n    text(j,.01,j)\n  }\n\n\n\n\n\n\n\npar(oo)\n\nWarning in par(oo): parâmetro gráfico \"cin\" não pode ser especificado\n\n\nWarning in par(oo): parâmetro gráfico \"cra\" não pode ser especificado\n\n\nWarning in par(oo): parâmetro gráfico \"csi\" não pode ser especificado\n\n\nWarning in par(oo): parâmetro gráfico \"cxy\" não pode ser especificado\n\n\nWarning in par(oo): parâmetro gráfico \"din\" não pode ser especificado\n\n\nWarning in par(oo): parâmetro gráfico \"page\" não pode ser especificado\n\n\n\np &lt;- NULL\nfor(i in 1:m){\np[i] &lt;- 2*min(mean(pred_mun[,i] &gt; casos[i]),\nmean(pred_mun[,i] &lt; casos[i]))\n}\n\ndata.frame(municipios,p)\n\n    municipios      p\n1      Amatura 0.3304\n2    Atl.Norte 0.0668\n3         Barr 0.0344\n4           BC 0.0200\n5     BV Ramos 0.1384\n6      Fonte B 0.0000\n7        Jutai 0.0588\n8        Maues 0.5032\n9     Nhamunda 0.0108\n10   Parintins 0.6520\n11      StoIca 0.3416\n12 SP Olivenca 0.4220\n13         Tbt 0.4020\n14   Tonantins 0.0456",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>O modelo Poisson revisitado</span>"
    ]
  }
]